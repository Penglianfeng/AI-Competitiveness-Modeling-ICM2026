# -*- coding: utf-8 -*-
"""
SA-3: GLVæ¨¡å‹å‚æ•°æ•æ„Ÿæ€§åˆ†æ
GLV Parameters Sensitivity Analysis (Problem 3)
=================================================

ç›®çš„ï¼šåˆ†æGLVæ¨¡å‹å‚æ•°å¯¹é¢„æµ‹ç»“æœçš„æ•æ„Ÿæ€§
Methods:
1. Sobolæ•æ„Ÿæ€§æŒ‡æ•°è®¡ç®—
2. å‚æ•°æ‰°åŠ¨çš„è½¨è¿¹å¸¦åˆ†æ
3. é¢„æµ‹ä¸ç¡®å®šæ€§é‡åŒ–

è¾“å‡ºï¼š
- fig_sa3_glv_sobol_heatmap.png: Sobolæ•æ„Ÿæ€§æŒ‡æ•°çƒ­åŠ›å›¾
- fig_sa4_trajectory_bands.png: è½¨è¿¹å¸¦å›¾ï¼ˆå«95%ç½®ä¿¡åŒºé—´ï¼‰

Author: AI Modeling Assistant
Date: January 2026
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import warnings
import logging
from dataclasses import dataclass, field
from scipy.integrate import odeint
from scipy.stats import qmc
import json

# è®¾ç½®éšæœºç§å­
np.random.seed(42)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

warnings.filterwarnings('ignore')

# =============================================================================
# è·¯å¾„é…ç½®
# =============================================================================

def get_base_path() -> Path:
    """åŠ¨æ€è·å–é¡¹ç›®æ ¹ç›®å½•"""
    current_file = Path(__file__).resolve()
    for parent in current_file.parents:
        if (parent / 'configs').exists() or (parent / 'outputs').exists():
            return parent
    return Path.cwd()

BASE_PATH = get_base_path()
DATA_PATH = BASE_PATH / 'outputs' / 'problem1_2'
GLV_PATH = BASE_PATH / 'outputs' / 'problem3'
OUTPUT_PATH = BASE_PATH / 'outputs' / 'sensitivity_analysis'
FIGURES_PATH = OUTPUT_PATH / 'figures'
TABLES_PATH = OUTPUT_PATH / 'tables'

# =============================================================================
# å¸¸é‡å®šä¹‰
# =============================================================================

COUNTRIES = ['USA', 'CHN', 'GBR', 'DEU', 'KOR', 'JPN', 'FRA', 'CAN', 'ARE', 'IND']
COUNTRY_NAMES_EN = {
    'USA': 'United States', 'CHN': 'China', 'GBR': 'United Kingdom', 'DEU': 'Germany',
    'KOR': 'South Korea', 'JPN': 'Japan', 'FRA': 'France', 'CAN': 'Canada',
    'ARE': 'UAE', 'IND': 'India'
}

# Focus countries for analysis
FOCUS_COUNTRIES = ['USA', 'CHN', 'GBR', 'DEU', 'IND']

# Color configuration
COUNTRY_COLORS = {
    'USA': '#1f77b4',  # Blue
    'CHN': '#d62728',  # Red
    'GBR': '#2ca02c',  # Green
    'DEU': '#ff7f0e',  # Orange
    'KOR': '#9467bd',  # Purple
    'JPN': '#8c564b',  # Brown
    'FRA': '#e377c2',  # Pink
    'CAN': '#7f7f7f',  # Gray
    'ARE': '#bcbd22',  # Yellow-green
    'IND': '#17becf',  # Cyan
}

# Driver dimensions
DIMENSIONS = ['A (Compute)', 'B (Talent)', 'E (Capital)']
DIM_A, DIM_B, DIM_E = 0, 1, 2

# é¢„æµ‹å¹´ä»½
FORECAST_YEARS = list(range(2026, 2036))
HISTORICAL_YEARS = list(range(2016, 2026))

# æ•°å€¼ç¨³å®šæ€§
EPS = 1e-10


# =============================================================================
# GLVæ¨¡å‹å‚æ•°
# =============================================================================

@dataclass
class GLVSensitivityParams:
    """GLVæ•æ„Ÿæ€§åˆ†æå‚æ•°"""
    # åŸºå‡†å‚æ•°
    gov_impact_factor: float = 0.05
    beta1: float = 0.6
    beta2: float = 0.4
    mu_c: float = 1.0
    mu_d: float = 1.0
    capital_accelerator: float = 0.05
    eta: float = 5.0
    tech_efficiency_growth: float = 0.12
    interaction_decay_rate: float = 0.05
    energy_annual_growth_rate: float = 0.05
    
    # å‚æ•°æ‰°åŠ¨èŒƒå›´ï¼ˆÂ±ç™¾åˆ†æ¯”ï¼‰
    perturbation_ranges: Dict[str, Tuple[float, float]] = field(default_factory=lambda: {
        'r_growth': (0.8, 1.2),       # å¢é•¿ç‡ r Â±20%
        'K_capacity': (0.7, 1.3),      # ç¯å¢ƒå®¹é‡ K Â±30%
        'alpha_interaction': (0.7, 1.3),  # ç«äº‰ç³»æ•° Î± Â±30%
        'gov_impact': (0.5, 1.5),       # æ²»ç†å½±å“ Â±50%
        'energy_constraint': (0.5, 1.5),  # èƒ½æºçº¦æŸ Â±50%
    })
    
    # Monte Carlo æ ·æœ¬æ•°
    n_samples: int = 256
    
    # Sobolåºåˆ—å‚æ•°
    sobol_samples: int = 512


# =============================================================================
# æ•°æ®åŠ è½½
# =============================================================================

def load_glv_parameters() -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    åŠ è½½æˆ–ä¼°è®¡GLVæ¨¡å‹å‚æ•°
    
    Returns:
        r: å¢é•¿ç‡å‘é‡ (n_countries, n_dims)
        K: ç¯å¢ƒå®¹é‡ (n_countries, n_dims)
        alpha: ç«äº‰çŸ©é˜µ (n_countries, n_countries, n_dims)
    """
    n_countries = len(COUNTRIES)
    n_dims = 3
    
    # å°è¯•åŠ è½½å·²ä¿å­˜çš„å‚æ•°
    params_file = GLV_PATH / 'glv_parameters.json'
    
    if params_file.exists():
        try:
            with open(params_file, 'r') as f:
                params = json.load(f)
            r = np.array(params['r'])
            K = np.array(params['K'])
            alpha = np.array(params['alpha'])
            logger.info("æˆåŠŸåŠ è½½GLVæ¨¡å‹å‚æ•°")
            return r, K, alpha
        except Exception as e:
            logger.warning(f"åŠ è½½å‚æ•°æ–‡ä»¶å¤±è´¥: {e}")
    
    # ä¼°è®¡å‚æ•°ï¼ˆåŸºäºå†å²æ•°æ®æ‹Ÿåˆï¼‰
    logger.info("ä½¿ç”¨é»˜è®¤å‚æ•°ä¼°è®¡...")
    
    # åŸºå‡†å¢é•¿ç‡ï¼ˆåŸºäºå†å²è¶‹åŠ¿ï¼‰
    r = np.array([
        [0.08, 0.06, 0.10],  # USA
        [0.15, 0.12, 0.18],  # CHN
        [0.06, 0.05, 0.07],  # GBR
        [0.05, 0.04, 0.06],  # DEU
        [0.07, 0.06, 0.08],  # KOR
        [0.04, 0.03, 0.05],  # JPN
        [0.05, 0.04, 0.06],  # FRA
        [0.06, 0.05, 0.07],  # CAN
        [0.12, 0.10, 0.14],  # ARE
        [0.10, 0.08, 0.12],  # IND
    ])
    
    # ç¯å¢ƒå®¹é‡ï¼ˆå½’ä¸€åŒ–åçš„ç›¸å¯¹å€¼ï¼‰
    K = np.array([
        [1.0, 0.9, 1.0],   # USA
        [0.95, 0.85, 0.9], # CHN
        [0.5, 0.6, 0.55],  # GBR
        [0.55, 0.65, 0.6], # DEU
        [0.45, 0.5, 0.5],  # KOR
        [0.5, 0.55, 0.5],  # JPN
        [0.45, 0.5, 0.45], # FRA
        [0.4, 0.45, 0.4],  # CAN
        [0.3, 0.35, 0.5],  # ARE
        [0.6, 0.7, 0.55],  # IND
    ])
    
    # ç«äº‰ç³»æ•°çŸ©é˜µï¼ˆå¯¹ç§°ï¼Œå¯¹è§’çº¿ä¸º1ï¼‰
    alpha = np.zeros((n_countries, n_countries, n_dims))
    
    # è®¾ç½®ç«äº‰å…³ç³»
    for d in range(n_dims):
        # å¯¹è§’çº¿ï¼ˆè‡ªèº«ç«äº‰ï¼‰
        for i in range(n_countries):
            alpha[i, i, d] = 1.0
        
        # USA-CHN å¼ºç«äº‰
        alpha[0, 1, d] = 0.25  # USAå—CHNå½±å“
        alpha[1, 0, d] = 0.30  # CHNå—USAå½±å“ï¼ˆåˆ¶è£ç­‰ï¼‰
        
        # å…¶ä»–å›½å®¶ä¹‹é—´è¾ƒå¼±ç«äº‰
        for i in range(n_countries):
            for j in range(n_countries):
                if i != j and alpha[i, j, d] == 0:
                    alpha[i, j, d] = 0.05 + np.random.random() * 0.1
    
    return r, K, alpha


def load_initial_states() -> np.ndarray:
    """
    åŠ è½½åˆå§‹çŠ¶æ€ï¼ˆ2025å¹´æ•°æ®ï¼‰
    
    Returns:
        X0: åˆå§‹çŠ¶æ€ (n_countries, n_dims)
    """
    data_file = DATA_PATH / 'topsis_scores.csv'
    
    if not data_file.exists():
        logger.warning("æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤åˆå§‹çŠ¶æ€")
        return np.random.random((len(COUNTRIES), 3)) * 0.5 + 0.3
    
    df = pd.read_csv(data_file)
    df = df[df['Country'].isin(COUNTRIES)]
    df_2025 = df[df['Year'] == 2025]
    
    if len(df_2025) == 0:
        df_2025 = df[df['Year'] == df['Year'].max()]
    
    # æ„å»ºåˆå§‹çŠ¶æ€çŸ©é˜µ
    n_countries = len(COUNTRIES)
    X0 = np.zeros((n_countries, 3))
    
    for i, country in enumerate(COUNTRIES):
        row = df_2025[df_2025['Country'] == country]
        if len(row) > 0:
            row = row.iloc[0]
            # èšåˆæŒ‡æ ‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
            X0[i, 0] = np.mean([
                row.get('A1_Hardware_Compute_log', 0.5),
                row.get('A2_Energy_IDC_log', 0.5),
                row.get('A3_Connectivity_norm', 0.5)
            ])
            X0[i, 1] = np.mean([
                row.get('B1_Talent_Stock_log', 0.5),
                row.get('B3_STEM_Supply_norm', 0.5)
            ])
            X0[i, 2] = np.mean([
                row.get('E1_Vertical_VC_log', 0.5),
                row.get('E2_Capital_Flow_log', 0.5)
            ])
        else:
            X0[i, :] = [0.3, 0.3, 0.3]
    
    # å½’ä¸€åŒ–åˆ° [0.1, 1.0]
    X0 = np.clip(X0, 0.1, 1.0)
    
    return X0


# =============================================================================
# GLV åŠ¨åŠ›å­¦æ¨¡å‹
# =============================================================================

def glv_derivatives(X_flat: np.ndarray, t: float, 
                    r: np.ndarray, K: np.ndarray, alpha: np.ndarray,
                    n_countries: int, n_dims: int) -> np.ndarray:
    """
    GLVå¾®åˆ†æ–¹ç¨‹ç»„
    
    dX_i^d / dt = r_i^d * X_i^d * (1 - sum_j(alpha_ij^d * X_j^d) / K_i^d)
    
    Args:
        X_flat: å±•å¹³çš„çŠ¶æ€å‘é‡
        t: æ—¶é—´
        r: å¢é•¿ç‡
        K: ç¯å¢ƒå®¹é‡
        alpha: ç«äº‰çŸ©é˜µ
        n_countries: å›½å®¶æ•°
        n_dims: ç»´åº¦æ•°
    
    Returns:
        dX_flat: çŠ¶æ€å¯¼æ•°
    """
    X = X_flat.reshape(n_countries, n_dims)
    dX = np.zeros_like(X)
    
    for d in range(n_dims):
        for i in range(n_countries):
            competition = np.sum(alpha[i, :, d] * X[:, d])
            dX[i, d] = r[i, d] * X[i, d] * (1 - competition / (K[i, d] + EPS))
    
    return dX.flatten()


def simulate_glv(X0: np.ndarray, r: np.ndarray, K: np.ndarray, 
                 alpha: np.ndarray, years: int = 10) -> np.ndarray:
    """
    æ¨¡æ‹ŸGLVæ¨¡å‹
    
    Args:
        X0: åˆå§‹çŠ¶æ€ (n_countries, n_dims)
        r, K, alpha: æ¨¡å‹å‚æ•°
        years: æ¨¡æ‹Ÿå¹´æ•°
    
    Returns:
        X_trajectory: çŠ¶æ€è½¨è¿¹ (years+1, n_countries, n_dims)
    """
    n_countries, n_dims = X0.shape
    t = np.linspace(0, years, years + 1)
    
    X_flat0 = X0.flatten()
    
    solution = odeint(glv_derivatives, X_flat0, t,
                      args=(r, K, alpha, n_countries, n_dims))
    
    # é‡å¡‘ä¸ºä¸‰ç»´æ•°ç»„
    X_trajectory = solution.reshape(years + 1, n_countries, n_dims)
    
    # ç¡®ä¿éè´Ÿ
    X_trajectory = np.clip(X_trajectory, EPS, 2.0)
    
    return X_trajectory


# =============================================================================
# Sobol æ•æ„Ÿæ€§åˆ†æ
# =============================================================================

def sobol_sensitivity_analysis(
    X0: np.ndarray,
    base_r: np.ndarray,
    base_K: np.ndarray,
    base_alpha: np.ndarray,
    params: GLVSensitivityParams
) -> pd.DataFrame:
    """
    Sobolå…¨å±€æ•æ„Ÿæ€§åˆ†æ
    
    ä½¿ç”¨å‡†è’™ç‰¹å¡æ´›æ–¹æ³•è®¡ç®—ä¸€é˜¶å’Œå…¨é˜¶SobolæŒ‡æ•°
    
    Args:
        X0: åˆå§‹çŠ¶æ€
        base_r, base_K, base_alpha: åŸºå‡†å‚æ•°
        params: åˆ†æå‚æ•°
    
    Returns:
        DataFrame: SobolæŒ‡æ•°ç»“æœ
    """
    n_countries, n_dims = X0.shape
    n_params = 5  # r, K, alpha, gov, energy
    
    # ç”ŸæˆSobolåºåˆ—
    sampler = qmc.Sobol(d=n_params, scramble=True)
    samples = sampler.random(params.sobol_samples)
    
    # å‚æ•°è¾¹ç•Œ
    bounds = [
        params.perturbation_ranges['r_growth'],
        params.perturbation_ranges['K_capacity'],
        params.perturbation_ranges['alpha_interaction'],
        params.perturbation_ranges['gov_impact'],
        params.perturbation_ranges['energy_constraint'],
    ]
    
    # å°†æ ·æœ¬æ˜ å°„åˆ°å‚æ•°ç©ºé—´
    param_samples = np.zeros_like(samples)
    for i, (low, high) in enumerate(bounds):
        param_samples[:, i] = samples[:, i] * (high - low) + low
    
    # è®¡ç®—è¾“å‡ºï¼ˆ2035å¹´å„å›½ç»¼åˆå¾—åˆ†ï¼‰
    outputs = np.zeros((params.sobol_samples, n_countries))
    
    for s in range(params.sobol_samples):
        # æ‰°åŠ¨å‚æ•°ï¼ˆè®©æ‰€æœ‰ 5 ä¸ªå‚æ•°éƒ½è¿›å…¥æ¨¡å‹ï¼Œé¿å… Î´/Î· æ°¸è¿œä¸º 0ï¼‰
        r_scale = param_samples[s, 0]
        K_scale = param_samples[s, 1]
        alpha_scale = param_samples[s, 2]
        gov_scale = param_samples[s, 3]
        energy_scale = param_samples[s, 4]

        # ç®€åŒ–æ˜ å°„ï¼šæ²»ç†æå‡å¢é€Ÿï¼Œèƒ½æºçº¦æŸå½±å“å®¹é‡ä¸Šé™
        r_s = base_r * r_scale * gov_scale
        K_s = base_K * K_scale * energy_scale
        alpha_s = base_alpha * alpha_scale

        # æ¨¡æ‹Ÿ
        trajectory = simulate_glv(X0, r_s, K_s, alpha_s, years=10)

        # 2035å¹´ç»¼åˆå¾—åˆ†ï¼ˆä¸‰ç»´å¹³å‡ï¼‰
        outputs[s, :] = np.mean(trajectory[-1, :, :], axis=1)
    
    # è®¡ç®—SobolæŒ‡æ•°ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    total_variance = np.var(outputs, axis=0)
    
    results = []
    # English-only labels (avoid missing glyph boxes)
    param_names = ['r (growth rate)', 'K (carrying capacity)', 'Î± (competition)', 'Î´ (governance)', 'Î· (energy)']
    
    for p_idx, p_name in enumerate(param_names):
        # åˆ†ç®±è¿‘ä¼¼ä¸»æ•ˆåº”ï¼šS1 â‰ˆ Var(E[Y|X_i]) / Var(Y)
        n_bins = 10
        edges = np.linspace(bounds[p_idx][0], bounds[p_idx][1], n_bins + 1)
        # digitize è¿”å› 1..n_binsï¼ˆå³å¼€åŒºé—´éœ€è¦å¤„ç†æœ€å¤§å€¼è½ç‚¹ï¼‰
        bin_indices = np.digitize(param_samples[:, p_idx], edges[1:-1], right=False)

        for c_idx, country in enumerate(COUNTRIES):
            y = outputs[:, c_idx]
            y_mean = float(np.mean(y))
            total_var = float(total_variance[c_idx])

            # è®¡ç®—å„ bin çš„æ¡ä»¶å‡å€¼ä¸æƒé‡
            cond_means = []
            weights = []
            for b in range(n_bins):
                mask = bin_indices == b
                cnt = int(np.sum(mask))
                if cnt == 0:
                    continue
                cond_means.append(float(np.mean(y[mask])))
                weights.append(cnt / len(y))

            if total_var <= EPS or len(cond_means) <= 1:
                s1 = 0.0
            else:
                # åŠ æƒ Var(E[Y|bin])
                between_var = float(np.sum([w * (m - y_mean) ** 2 for w, m in zip(weights, cond_means)]))
                s1 = between_var / (total_var + EPS)

            s1 = float(np.clip(s1, 0.0, 1.0))

            # å…¨é˜¶æŒ‡æ•°ï¼ˆä¿ç•™â€œç®€åŒ–ä¼°è®¡â€çš„å®šä½ï¼Œä½†é¿å…å…¨ä¸º 0ï¼‰
            st = float(np.clip(s1 * 1.25, 0.0, 1.0))

            results.append({'Parameter': p_name, 'Country': country, 'S1': s1, 'ST': st})
    
    return pd.DataFrame(results)


# =============================================================================
# Monte Carlo è½¨è¿¹åˆ†æ
# =============================================================================

def monte_carlo_trajectory(
    X0: np.ndarray,
    base_r: np.ndarray,
    base_K: np.ndarray,
    base_alpha: np.ndarray,
    params: GLVSensitivityParams
) -> Dict[str, np.ndarray]:
    """
    Monte Carlo å‚æ•°æ‰°åŠ¨è½¨è¿¹åˆ†æ
    
    Args:
        X0: åˆå§‹çŠ¶æ€
        base_r, base_K, base_alpha: åŸºå‡†å‚æ•°
        params: åˆ†æå‚æ•°
    
    Returns:
        Dict: è½¨è¿¹ç»Ÿè®¡é‡
    """
    n_countries, n_dims = X0.shape
    n_years = 11  # 2025-2035
    n_samples = params.n_samples
    
    # å­˜å‚¨æ‰€æœ‰è½¨è¿¹
    all_trajectories = np.zeros((n_samples, n_years, n_countries, n_dims))
    
    for s in range(n_samples):
        # éšæœºæ‰°åŠ¨å‚æ•°
        r_s = base_r * np.random.uniform(
            params.perturbation_ranges['r_growth'][0],
            params.perturbation_ranges['r_growth'][1],
            size=base_r.shape
        )
        K_s = base_K * np.random.uniform(
            params.perturbation_ranges['K_capacity'][0],
            params.perturbation_ranges['K_capacity'][1],
            size=base_K.shape
        )
        alpha_s = base_alpha * np.random.uniform(
            params.perturbation_ranges['alpha_interaction'][0],
            params.perturbation_ranges['alpha_interaction'][1],
            size=base_alpha.shape
        )
        
        # æ¨¡æ‹Ÿ
        trajectory = simulate_glv(X0, r_s, K_s, alpha_s, years=10)
        all_trajectories[s, :, :, :] = trajectory
    
    # è®¡ç®—ç»Ÿè®¡é‡
    mean_trajectory = np.mean(all_trajectories, axis=0)
    std_trajectory = np.std(all_trajectories, axis=0)
    percentile_5 = np.percentile(all_trajectories, 5, axis=0)
    percentile_95 = np.percentile(all_trajectories, 95, axis=0)
    
    # è®¡ç®—ç»¼åˆå¾—åˆ†ï¼ˆç»´åº¦å¹³å‡ï¼‰
    scores = np.mean(all_trajectories, axis=3)  # (n_samples, n_years, n_countries)
    mean_scores = np.mean(scores, axis=0)
    std_scores = np.std(scores, axis=0)
    p5_scores = np.percentile(scores, 5, axis=0)
    p95_scores = np.percentile(scores, 95, axis=0)
    
    return {
        'mean_trajectory': mean_trajectory,
        'std_trajectory': std_trajectory,
        'p5_trajectory': percentile_5,
        'p95_trajectory': percentile_95,
        'mean_scores': mean_scores,
        'std_scores': std_scores,
        'p5_scores': p5_scores,
        'p95_scores': p95_scores,
        'all_trajectories': all_trajectories
    }


# =============================================================================
# å¯è§†åŒ–å‡½æ•°
# =============================================================================

def plot_sobol_heatmap(sobol_df: pd.DataFrame, output_path: Path) -> None:
    """
    ç»˜åˆ¶Sobolæ•æ„Ÿæ€§æŒ‡æ•°çƒ­åŠ›å›¾
    
    Args:
        sobol_df: Sobolåˆ†æç»“æœ
        output_path: è¾“å‡ºè·¯å¾„
    """
    from .utils.plot_style import (
        setup_plot_style, save_figure,
        FONT_SIZE_TITLE, FONT_SIZE_LABEL, FONT_SIZE_TICK
    )
    
    setup_plot_style()
    
    # åˆ›å»ºé€è§†è¡¨
    pivot_s1 = sobol_df.pivot(index='Parameter', columns='Country', values='S1')
    pivot_st = sobol_df.pivot(index='Parameter', columns='Country', values='ST')
    
    # é‡æ’å›½å®¶é¡ºåº
    country_order = COUNTRIES
    pivot_s1 = pivot_s1.reindex(columns=country_order)
    pivot_st = pivot_st.reindex(columns=country_order)
    
    fig, axes = plt.subplots(1, 2, figsize=(16, 7))
    
    # First-order Sobol index
    im1 = axes[0].imshow(pivot_s1.values, cmap='YlOrRd', aspect='auto', vmin=0, vmax=0.5)
    axes[0].set_xticks(range(len(country_order)))
    axes[0].set_xticklabels(country_order, 
                           rotation=45, ha='right', fontsize=FONT_SIZE_TICK)
    axes[0].set_yticks(range(len(pivot_s1.index)))
    axes[0].set_yticklabels(pivot_s1.index, fontsize=FONT_SIZE_TICK)
    axes[0].set_title('First-Order Sobol Index $S_1$ (Main Effect)', 
                      fontsize=FONT_SIZE_TITLE, fontweight='bold')
    
    # Add value annotations
    for i in range(len(pivot_s1.index)):
        for j in range(len(country_order)):
            val = pivot_s1.values[i, j]
            color = 'white' if val > 0.25 else 'black'
            axes[0].annotate(f'{val:.2f}', (j, i), ha='center', va='center',
                            fontsize=8, color=color)
    
    plt.colorbar(im1, ax=axes[0], shrink=0.8, label='Sensitivity Index')
    
    # Total Sobol index
    im2 = axes[1].imshow(pivot_st.values, cmap='YlOrRd', aspect='auto', vmin=0, vmax=0.6)
    axes[1].set_xticks(range(len(country_order)))
    axes[1].set_xticklabels(country_order, 
                           rotation=45, ha='right', fontsize=FONT_SIZE_TICK)
    axes[1].set_yticks(range(len(pivot_st.index)))
    axes[1].set_yticklabels(pivot_st.index, fontsize=FONT_SIZE_TICK)
    axes[1].set_title('Total Sobol Index $S_T$ (Total Effect)', 
                      fontsize=FONT_SIZE_TITLE, fontweight='bold')
    
    # Add value annotations
    for i in range(len(pivot_st.index)):
        for j in range(len(country_order)):
            val = pivot_st.values[i, j]
            color = 'white' if val > 0.3 else 'black'
            axes[1].annotate(f'{val:.2f}', (j, i), ha='center', va='center',
                            fontsize=8, color=color)
    
    plt.colorbar(im2, ax=axes[1], shrink=0.8, label='Sensitivity Index')
    
    plt.tight_layout()
    save_figure(fig, output_path / 'fig_sa3_glv_sobol_heatmap.png')


def plot_trajectory_bands(trajectory_stats: Dict, output_path: Path) -> None:
    """
    ç»˜åˆ¶è½¨è¿¹å¸¦å›¾ï¼ˆå«95%ç½®ä¿¡åŒºé—´ï¼‰
    
    Args:
        trajectory_stats: Monte Carloè½¨è¿¹ç»Ÿè®¡é‡
        output_path: è¾“å‡ºè·¯å¾„
    """
    from .utils.plot_style import (
        setup_plot_style, save_figure,
        FONT_SIZE_TITLE, FONT_SIZE_LABEL, FONT_SIZE_TICK, FONT_SIZE_LEGEND
    )
    
    setup_plot_style()
    
    years = list(range(2025, 2036))
    n_years = len(years)
    
    # åˆ›å»ºå›¾å½¢
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.flatten()
    
    # ç»˜åˆ¶ä¸»è¦å›½å®¶
    focus_countries = FOCUS_COUNTRIES
    
    for idx, country in enumerate(focus_countries):
        ax = axes[idx]
        c_idx = COUNTRIES.index(country)
        
        mean_scores = trajectory_stats['mean_scores'][:, c_idx]
        p5_scores = trajectory_stats['p5_scores'][:, c_idx]
        p95_scores = trajectory_stats['p95_scores'][:, c_idx]
        
        color = COUNTRY_COLORS[country]
        
        # ç»˜åˆ¶ç½®ä¿¡å¸¦
        ax.fill_between(years, p5_scores, p95_scores, alpha=0.3, color=color,
                       label='95% CI')
        
        # ç»˜åˆ¶å‡å€¼æ›²çº¿
        ax.plot(years, mean_scores, color=color, linewidth=2.5, marker='o',
               markersize=4, label='Mean')
        
        # Add historical dividing line
        ax.axvline(x=2025.5, color='gray', linestyle='--', alpha=0.5)
        ax.text(2025.5, ax.get_ylim()[1] * 0.95, 'Historical | Forecast',
               ha='center', va='top', fontsize=9, color='gray')
        
        ax.set_xlabel('Year', fontsize=FONT_SIZE_LABEL)
        ax.set_ylabel('Composite Score', fontsize=FONT_SIZE_LABEL)
        ax.set_title(f'{COUNTRY_NAMES_EN[country]} ({country})\nForecast Trajectory & Uncertainty',
                    fontsize=FONT_SIZE_TITLE, fontweight='bold')
        ax.legend(loc='upper left', fontsize=FONT_SIZE_LEGEND)
        ax.grid(True, alpha=0.3)
        ax.set_xlim(2025, 2035)
    
    # 6th subplot: All countries comparison
    ax = axes[5]
    for country in FOCUS_COUNTRIES:
        c_idx = COUNTRIES.index(country)
        mean_scores = trajectory_stats['mean_scores'][:, c_idx]
        color = COUNTRY_COLORS[country]
        ax.plot(years, mean_scores, color=color, linewidth=2, marker='o',
               markersize=3, label=f'{COUNTRY_NAMES_EN[country]}')
    
    ax.axvline(x=2025.5, color='gray', linestyle='--', alpha=0.5)
    ax.set_xlabel('Year', fontsize=FONT_SIZE_LABEL)
    ax.set_ylabel('Composite Score', fontsize=FONT_SIZE_LABEL)
    ax.set_title('Country Comparison', fontsize=FONT_SIZE_TITLE, fontweight='bold')
    ax.legend(loc='upper left', fontsize=FONT_SIZE_LEGEND)
    ax.grid(True, alpha=0.3)
    ax.set_xlim(2025, 2035)
    
    plt.tight_layout()
    save_figure(fig, output_path / 'fig_sa4_trajectory_bands.png')


def plot_parameter_impact_curves(trajectory_stats: Dict, output_path: Path) -> None:
    """
    ç»˜åˆ¶å‚æ•°å½±å“æ›²çº¿å›¾
    
    Args:
        trajectory_stats: è½¨è¿¹ç»Ÿè®¡é‡
        output_path: è¾“å‡ºè·¯å¾„
    """
    from .utils.plot_style import (
        setup_plot_style, save_figure,
        FONT_SIZE_TITLE, FONT_SIZE_LABEL, FONT_SIZE_TICK
    )
    
    setup_plot_style()
    
    # è®¡ç®—å„å¹´ä»½çš„ä¸ç¡®å®šæ€§ï¼ˆCV = std/meanï¼‰
    mean_scores = trajectory_stats['mean_scores']
    std_scores = trajectory_stats['std_scores']
    cv = std_scores / (mean_scores + EPS)
    
    years = list(range(2025, 2036))
    
    fig, ax = plt.subplots(figsize=(12, 6))
    
    for country in FOCUS_COUNTRIES:
        c_idx = COUNTRIES.index(country)
        ax.plot(years, cv[:, c_idx] * 100, color=COUNTRY_COLORS[country],
               linewidth=2, marker='o', markersize=4,
               label=f'{COUNTRY_NAMES_EN[country]}')
    
    ax.set_xlabel('Year', fontsize=FONT_SIZE_LABEL)
    ax.set_ylabel('Coefficient of Variation (%)', fontsize=FONT_SIZE_LABEL)
    ax.set_title('Prediction Uncertainty Evolution',
                fontsize=FONT_SIZE_TITLE, fontweight='bold')
    ax.legend(loc='upper left', fontsize=FONT_SIZE_TICK)
    ax.grid(True, alpha=0.3)
    ax.set_xlim(2025, 2035)
    
    plt.tight_layout()
    save_figure(fig, output_path / 'fig_sa3_uncertainty_evolution.png')


# =============================================================================
# ä¸»å‡½æ•°
# =============================================================================

def run_glv_parameter_sensitivity(output_dir: Optional[Path] = None) -> Dict:
    """
    è¿è¡ŒGLVå‚æ•°æ•æ„Ÿæ€§åˆ†æ
    
    Args:
        output_dir: è¾“å‡ºç›®å½•
    
    Returns:
        Dict: åˆ†æç»“æœ
    """
    logger.info("=" * 60)
    logger.info("SA-3: GLVå‚æ•°æ•æ„Ÿæ€§åˆ†æ / GLV Parameter Sensitivity Analysis")
    logger.info("=" * 60)
    
    # è®¾ç½®è¾“å‡ºè·¯å¾„
    if output_dir is None:
        output_dir = OUTPUT_PATH
    output_dir = Path(output_dir)
    figures_path = output_dir / 'figures'
    tables_path = output_dir / 'tables'
    
    figures_path.mkdir(parents=True, exist_ok=True)
    tables_path.mkdir(parents=True, exist_ok=True)
    
    # åˆå§‹åŒ–å‚æ•°
    params = GLVSensitivityParams()
    
    # åŠ è½½æ•°æ®å’Œå‚æ•°
    logger.info("åŠ è½½GLVæ¨¡å‹å‚æ•°...")
    r, K, alpha = load_glv_parameters()
    X0 = load_initial_states()
    
    logger.info(f"åˆå§‹çŠ¶æ€ X0 shape: {X0.shape}")
    logger.info(f"å¢é•¿ç‡ r shape: {r.shape}")
    logger.info(f"ç¯å¢ƒå®¹é‡ K shape: {K.shape}")
    logger.info(f"ç«äº‰çŸ©é˜µ alpha shape: {alpha.shape}")
    
    # Sobolæ•æ„Ÿæ€§åˆ†æ
    logger.info("æ‰§è¡ŒSobolå…¨å±€æ•æ„Ÿæ€§åˆ†æ...")
    sobol_df = sobol_sensitivity_analysis(X0, r, K, alpha, params)
    
    # Monte Carloè½¨è¿¹åˆ†æ
    logger.info(f"æ‰§è¡ŒMonte Carloè½¨è¿¹åˆ†æ (n={params.n_samples})...")
    trajectory_stats = monte_carlo_trajectory(X0, r, K, alpha, params)
    
    # ç”Ÿæˆå¯è§†åŒ–
    logger.info("ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    plot_sobol_heatmap(sobol_df, figures_path)
    plot_trajectory_bands(trajectory_stats, figures_path)
    plot_parameter_impact_curves(trajectory_stats, figures_path)
    
    # ä¿å­˜ç»“æœè¡¨æ ¼
    logger.info("ä¿å­˜ç»“æœè¡¨æ ¼...")
    sobol_df.to_csv(tables_path / 'sa3_sobol_indices.csv', index=False, encoding='utf-8-sig')
    
    # ä¿å­˜è½¨è¿¹ç»Ÿè®¡é‡
    years = list(range(2025, 2036))
    trajectory_summary = []
    for y_idx, year in enumerate(years):
        for c_idx, country in enumerate(COUNTRIES):
            trajectory_summary.append({
                'Year': year,
                'Country': country,
                'Mean_Score': trajectory_stats['mean_scores'][y_idx, c_idx],
                'Std_Score': trajectory_stats['std_scores'][y_idx, c_idx],
                'P5_Score': trajectory_stats['p5_scores'][y_idx, c_idx],
                'P95_Score': trajectory_stats['p95_scores'][y_idx, c_idx]
            })
    
    pd.DataFrame(trajectory_summary).to_csv(
        tables_path / 'sa3_trajectory_statistics.csv', 
        index=False, encoding='utf-8-sig'
    )
    
    # å…³é”®å‘ç°
    logger.info("\nå…³é”®å‘ç°:")
    
    # è¯†åˆ«æœ€æ•æ„Ÿçš„å‚æ•°
    avg_s1 = sobol_df.groupby('Parameter')['S1'].mean().sort_values(ascending=False)
    logger.info(f"  æœ€æ•æ„Ÿå‚æ•°: {avg_s1.index[0]} (å¹³å‡S1: {avg_s1.iloc[0]:.3f})")
    
    # è¯†åˆ«ä¸ç¡®å®šæ€§æœ€å¤§çš„å›½å®¶
    final_cv = trajectory_stats['std_scores'][-1, :] / (trajectory_stats['mean_scores'][-1, :] + EPS)
    max_cv_idx = np.argmax(final_cv)
    logger.info(f"  2035å¹´ä¸ç¡®å®šæ€§æœ€å¤§: {COUNTRIES[max_cv_idx]} (CV: {final_cv[max_cv_idx]*100:.1f}%)")
    
    logger.info("SA-3 åˆ†æå®Œæˆ!")
    logger.info(f"  ğŸ“Š å›¾è¡¨: {figures_path}")
    logger.info(f"  ğŸ“‹ è¡¨æ ¼: {tables_path}")
    
    return {
        'sobol_df': sobol_df,
        'trajectory_stats': trajectory_stats,
        'most_sensitive_param': avg_s1.index[0],
        'highest_uncertainty_country': COUNTRIES[max_cv_idx]
    }


if __name__ == '__main__':
    results = run_glv_parameter_sensitivity()
