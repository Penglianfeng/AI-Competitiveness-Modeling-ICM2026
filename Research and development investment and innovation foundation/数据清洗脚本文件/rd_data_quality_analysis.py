# -*- coding: utf-8 -*-
"""
åæ•°æ¯ Bé¢˜ - R&Dä¸åˆ›æ–°åŸºç¡€æ•°æ®è´¨é‡åˆ†æè„šæœ¬
============================================
é’ˆå¯¹ Research and development investment and innovation foundation æ–‡ä»¶å¤¹
çš„æ•°æ®è¿›è¡Œæ·±åº¦è´¨é‡åˆ†æ

æ•°æ®ç‰¹ç‚¹ï¼š
- R&Dæ”¯å‡ºã€ç ”ç©¶äººå‘˜ã€ä¸“åˆ©ã€é«˜ç­‰æ•™è‚²ç­‰åˆ›æ–°åŸºç¡€æŒ‡æ ‡
- æ¥æºï¼šUNESCO UISã€World Bank
- ä¸»è¦ä¸ºæ¯”ä¾‹æŒ‡æ ‡å’Œå­˜é‡æŒ‡æ ‡ï¼ˆä¸AIæŠ•èµ„ç­‰æµé‡æŒ‡æ ‡ä¸åŒï¼‰

ä½œè€…: åæ•°æ¯å‚èµ›é˜Ÿ
æ—¥æœŸ: 2026-01-17
"""

import pandas as pd
import numpy as np
import os
from pathlib import Path
from datetime import datetime
import warnings
import json
warnings.filterwarnings('ignore')

# ===================== é…ç½® =====================
BASE_DIR = Path(r"d:\åæ•°æ¯\Research and development investment and innovation foundation")
UIS_DATA_DIR = BASE_DIR / "uis_rd_data"
WB_DATA_DIR = BASE_DIR / "World Bank Data"
MERGED_DATA_DIR = BASE_DIR / "merged_data"
OUTPUT_DIR = BASE_DIR / "preprocessed"
OUTPUT_DIR.mkdir(exist_ok=True)

# ç›®æ ‡å›½å®¶ï¼ˆä¸ä¸»æ•°æ®é›†ä¿æŒä¸€è‡´ï¼‰
TARGET_COUNTRIES = {
    'USA': 'ç¾å›½', 'CHN': 'ä¸­å›½', 'GBR': 'è‹±å›½', 'DEU': 'å¾·å›½', 
    'FRA': 'æ³•å›½', 'CAN': 'åŠ æ‹¿å¤§', 'JPN': 'æ—¥æœ¬', 'KOR': 'éŸ©å›½', 
    'ARE': 'é˜¿è”é…‹', 'IND': 'å°åº¦'
}

# ç›®æ ‡å¹´ä»½èŒƒå›´ï¼ˆæ¯”AIæ•°æ®èŒƒå›´æ›´å®½ï¼Œç”¨äºè¶‹åŠ¿åˆ†æï¼‰
TARGET_YEARS = list(range(2010, 2026))
FOCUS_YEARS = list(range(2016, 2026))  # é‡ç‚¹åˆ†æå¹´ä»½ï¼ˆä¸AIæ•°æ®å¯¹é½ï¼‰

# æŒ‡æ ‡åˆ†ç±»ï¼ˆç”¨äºç¡®å®šå¤„ç†æ–¹å¼ï¼‰
INDICATOR_CATEGORIES = {
    'ratio_pct': [  # æ¯”ä¾‹/ç™¾åˆ†æ¯”æŒ‡æ ‡ï¼Œæ— éœ€å¯¹æ•°å˜æ¢
        'rd_expenditure_pct_gdp',
        'bachelor_degree_pct',
        'master_degree_pct', 
        'phd_degree_pct',
        'internet_users_pct',
        'ict_service_exports_pct',
        'high_tech_exports_pct',
        'govt_edu_expenditure_pct_gdp',
        'higher_edu_expenditure_pct',
        'labor_force_higher_edu_pct'
    ],
    'count_intensive': [  # å¯†åº¦å‹å­˜é‡æŒ‡æ ‡
        'researchers_per_million',
        'fixed_broadband_per_100',
        'mobile_subscriptions_per_100',
        'secure_internet_servers_per_million'
    ],
    'count_absolute': [  # ç»å¯¹æ•°é‡æŒ‡æ ‡ï¼Œå¯èƒ½éœ€è¦å¯¹æ•°å˜æ¢
        'patent_applications_resident',
        'patent_applications_nonresident',
        'high_tech_exports_usd'
    ],
    'enrollment': [  # å…¥å­¦ç‡æŒ‡æ ‡
        'higher_edu_enrollment_rate'
    ]
}


class RDDataQualityAnalyzer:
    """R&Dä¸åˆ›æ–°åŸºç¡€æ•°æ®è´¨é‡åˆ†æå™¨"""
    
    def __init__(self):
        self.issues = []
        self.warnings = []
        self.recommendations = []
        self.analysis_results = {}
        
    def run_full_analysis(self):
        """æ‰§è¡Œå®Œæ•´åˆ†æ"""
        print("=" * 100)
        print("ğŸ”¬ åæ•°æ¯ Bé¢˜ - R&Dä¸åˆ›æ–°åŸºç¡€æ•°æ®è´¨é‡åˆ†ææŠ¥å‘Š")
        print("=" * 100)
        print(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"æ•°æ®ç›®å½•: {BASE_DIR}")
        print()
        
        # 1. æ•°æ®æºæ¦‚è§ˆ
        self.analyze_data_sources()
        
        # 2. åˆå¹¶æ•°æ®é›†åˆ†æ
        self.analyze_merged_dataset()
        
        # 3. æ—¶é—´è¦†ç›–åº¦åˆ†æ
        self.analyze_temporal_coverage()
        
        # 4. å›½å®¶è¦†ç›–åº¦åˆ†æ  
        self.analyze_country_coverage()
        
        # 5. ç¼ºå¤±å€¼æ¨¡å¼åˆ†æ
        self.analyze_missing_patterns()
        
        # 6. æ•°å€¼åˆ†å¸ƒåˆ†æ
        self.analyze_value_distributions()
        
        # 7. 2024-2025å¹´æ•°æ®å¯ç”¨æ€§
        self.analyze_recent_data_availability()
        
        # 8. ä¸ä¸»æ•°æ®é›†æ—¶é—´å¯¹é½åˆ†æ
        self.analyze_alignment_with_ai_data()
        
        # 9. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        self.generate_report()
        
        return self.analysis_results
    
    def analyze_data_sources(self):
        """åˆ†ææ•°æ®æºç»“æ„"""
        print("\n" + "=" * 80)
        print("ğŸ“‚ 1. æ•°æ®æºæ¦‚è§ˆ")
        print("=" * 80)
        
        sources = {}
        
        # UISæ•°æ®
        if UIS_DATA_DIR.exists():
            raw_files = list((UIS_DATA_DIR / "raw").glob("*.csv")) if (UIS_DATA_DIR / "raw").exists() else []
            processed_files = list((UIS_DATA_DIR / "processed").glob("*.csv")) if (UIS_DATA_DIR / "processed").exists() else []
            sources['UIS (UNESCO Institute for Statistics)'] = {
                'raw_files': [f.name for f in raw_files],
                'processed_files': [f.name for f in processed_files]
            }
            print(f"\nğŸ“ UISæ•°æ®:")
            print(f"   - åŸå§‹æ–‡ä»¶: {len(raw_files)} ä¸ª")
            for f in raw_files:
                print(f"      â€¢ {f.name}")
            print(f"   - å¤„ç†åæ–‡ä»¶: {len(processed_files)} ä¸ª")
        
        # World Bankæ•°æ®
        if WB_DATA_DIR.exists():
            wb_files = list(WB_DATA_DIR.glob("*.csv"))
            sources['World Bank'] = {
                'files': [f.name for f in wb_files]
            }
            print(f"\nğŸ“ World Bankæ•°æ®:")
            print(f"   - æ•°æ®æ–‡ä»¶: {len(wb_files)} ä¸ª")
            for f in wb_files:
                df = pd.read_csv(f, nrows=1)
                print(f"      â€¢ {f.name} ({len(df.columns)} åˆ—)")
        
        # åˆå¹¶æ•°æ®
        if MERGED_DATA_DIR.exists():
            merged_files = list(MERGED_DATA_DIR.glob("*.csv"))
            sources['Merged Data'] = {
                'files': [f.name for f in merged_files]
            }
            print(f"\nğŸ“ åˆå¹¶æ•°æ®:")
            for f in merged_files:
                df = pd.read_csv(f)
                print(f"      â€¢ {f.name} ({len(df)} è¡Œ, {len(df.columns)} åˆ—)")
        
        self.analysis_results['data_sources'] = sources
    
    def analyze_merged_dataset(self):
        """åˆ†æåˆå¹¶åçš„å®½è¡¨æ•°æ®é›†"""
        print("\n" + "=" * 80)
        print("ğŸ“Š 2. åˆå¹¶æ•°æ®é›†åˆ†æ (rd_innovation_wide.csv)")
        print("=" * 80)
        
        merged_file = MERGED_DATA_DIR / "rd_innovation_wide.csv"
        if not merged_file.exists():
            print("âš ï¸ åˆå¹¶æ•°æ®é›†ä¸å­˜åœ¨!")
            return
        
        df = pd.read_csv(merged_file)
        
        print(f"\nåŸºæœ¬ä¿¡æ¯:")
        print(f"   - æ€»è¡Œæ•°: {len(df)}")
        print(f"   - æ€»åˆ—æ•°: {len(df.columns)}")
        print(f"   - å›½å®¶æ•°: {df['country_code'].nunique()}")
        print(f"   - å¹´ä»½èŒƒå›´: {df['year'].min()} - {df['year'].max()}")
        
        # æŒ‡æ ‡åˆ—åˆ†æ
        indicator_cols = [c for c in df.columns if c not in ['country_code', 'year', 'country_cn', 'country_en']]
        print(f"\nğŸ“ˆ æŒ‡æ ‡åˆ— ({len(indicator_cols)} ä¸ª):")
        
        for col in indicator_cols:
            non_null = df[col].notna().sum()
            non_null_pct = non_null / len(df) * 100
            if non_null > 0:
                mean_val = df[col].mean()
                std_val = df[col].std()
                print(f"   â€¢ {col}: {non_null_pct:.1f}% éç©º, å‡å€¼={mean_val:.2f}, æ ‡å‡†å·®={std_val:.2f}")
        
        self.analysis_results['merged_dataset'] = {
            'rows': len(df),
            'columns': len(df.columns),
            'countries': df['country_code'].nunique(),
            'year_range': (int(df['year'].min()), int(df['year'].max())),
            'indicators': indicator_cols
        }
    
    def analyze_temporal_coverage(self):
        """åˆ†ææ—¶é—´è¦†ç›–åº¦"""
        print("\n" + "=" * 80)
        print("ğŸ“… 3. æ—¶é—´è¦†ç›–åº¦åˆ†æ")
        print("=" * 80)
        
        merged_file = MERGED_DATA_DIR / "rd_innovation_wide.csv"
        if not merged_file.exists():
            return
            
        df = pd.read_csv(merged_file)
        
        # å„æŒ‡æ ‡çš„æ—¶é—´è¦†ç›–
        indicator_cols = [c for c in df.columns if c not in ['country_code', 'year', 'country_cn', 'country_en']]
        
        coverage = {}
        print(f"\nå„æŒ‡æ ‡æ—¶é—´è¦†ç›–:")
        print("-" * 70)
        
        for col in indicator_cols[:15]:  # æ˜¾ç¤ºä¸»è¦æŒ‡æ ‡
            valid_years = df[df[col].notna()]['year'].unique()
            if len(valid_years) > 0:
                min_year = int(min(valid_years))
                max_year = int(max(valid_years))
                coverage[col] = {
                    'min_year': min_year,
                    'max_year': max_year,
                    'years_count': len(valid_years)
                }
                # æ£€æŸ¥2016-2025è¦†ç›–
                focus_coverage = len([y for y in valid_years if y in FOCUS_YEARS])
                print(f"   {col[:40]:40s}: {min_year}-{max_year}, 2016-2025è¦†ç›–: {focus_coverage}/10")
                
                if max_year < 2024:
                    self.warnings.append(f"âš ï¸ {col}: æœ€æ–°æ•°æ®ä»…åˆ°{max_year}å¹´ï¼Œç¼ºå°‘è¿‘æœŸæ•°æ®")
        
        self.analysis_results['temporal_coverage'] = coverage
    
    def analyze_country_coverage(self):
        """åˆ†æå„å›½æ•°æ®è¦†ç›–æƒ…å†µ"""
        print("\n" + "=" * 80)
        print("ğŸŒ 4. å„å›½æ•°æ®è¦†ç›–åº¦åˆ†æ")
        print("=" * 80)
        
        merged_file = MERGED_DATA_DIR / "rd_innovation_wide.csv"
        if not merged_file.exists():
            return
            
        df = pd.read_csv(merged_file)
        indicator_cols = [c for c in df.columns if c not in ['country_code', 'year', 'country_cn', 'country_en']]
        
        print(f"\nå„å›½æŒ‡æ ‡éç©ºç‡:")
        print("-" * 70)
        
        country_coverage = {}
        for country in TARGET_COUNTRIES.keys():
            country_data = df[df['country_code'] == country]
            if len(country_data) == 0:
                print(f"   âŒ {country} ({TARGET_COUNTRIES[country]}): æ— æ•°æ®")
                self.issues.append(f"ğŸš¨ {country} ({TARGET_COUNTRIES[country]}) æ— æ•°æ®")
                continue
            
            # è®¡ç®—éç©ºç‡
            non_null_rates = {}
            for col in indicator_cols:
                non_null = country_data[col].notna().sum()
                non_null_pct = non_null / len(country_data) * 100
                non_null_rates[col] = non_null_pct
            
            avg_rate = np.mean(list(non_null_rates.values()))
            country_coverage[country] = {
                'avg_coverage': avg_rate,
                'years': len(country_data),
                'detail': non_null_rates
            }
            
            status = "âœ…" if avg_rate > 70 else "âš ï¸" if avg_rate > 40 else "âŒ"
            print(f"   {status} {country} ({TARGET_COUNTRIES[country]:4s}): å¹³å‡è¦†ç›–ç‡ {avg_rate:.1f}%, {len(country_data)}å¹´æ•°æ®")
            
            if avg_rate < 40:
                self.warnings.append(f"âš ï¸ {country}: æ•°æ®è¦†ç›–ç‡ä»…{avg_rate:.1f}%ï¼Œéœ€ç‰¹æ®Šå¤„ç†")
        
        # ç‰¹åˆ«å…³æ³¨é˜¿è”é…‹å’Œå°åº¦
        print(f"\nğŸ” æ–°å…´å›½å®¶æ•°æ®è¯¦æƒ…:")
        for country in ['ARE', 'IND']:
            if country in country_coverage:
                detail = country_coverage[country]['detail']
                low_coverage = [k for k, v in detail.items() if v < 30]
                if low_coverage:
                    print(f"   {country} ä½è¦†ç›–æŒ‡æ ‡ (<30%): {low_coverage[:5]}")
        
        self.analysis_results['country_coverage'] = country_coverage
    
    def analyze_missing_patterns(self):
        """åˆ†æç¼ºå¤±å€¼æ¨¡å¼"""
        print("\n" + "=" * 80)
        print("ğŸ” 5. ç¼ºå¤±å€¼æ¨¡å¼åˆ†æ")
        print("=" * 80)
        
        merged_file = MERGED_DATA_DIR / "rd_innovation_wide.csv"
        if not merged_file.exists():
            return
            
        df = pd.read_csv(merged_file)
        indicator_cols = [c for c in df.columns if c not in ['country_code', 'year', 'country_cn', 'country_en']]
        
        print(f"\nç¼ºå¤±å€¼ç»Ÿè®¡:")
        print("-" * 70)
        
        missing_summary = []
        for col in indicator_cols:
            missing = df[col].isna().sum()
            missing_pct = missing / len(df) * 100
            if missing > 0:
                missing_summary.append({
                    'indicator': col,
                    'missing_count': missing,
                    'missing_pct': missing_pct
                })
        
        # æŒ‰ç¼ºå¤±ç‡æ’åº
        missing_summary.sort(key=lambda x: x['missing_pct'], reverse=True)
        
        print(f"{'æŒ‡æ ‡':<45} {'ç¼ºå¤±æ•°':>8} {'ç¼ºå¤±ç‡':>8}")
        print("-" * 65)
        for item in missing_summary[:15]:
            print(f"{item['indicator'][:44]:<45} {item['missing_count']:>8} {item['missing_pct']:>7.1f}%")
        
        # åˆ†æç¼ºå¤±æ¨¡å¼ç±»å‹
        print(f"\nç¼ºå¤±æ¨¡å¼åˆ†ç±»:")
        
        # 1. å°¾éƒ¨ç¼ºå¤±ï¼ˆæœ€æ–°å¹´ä»½ç¼ºå¤±ï¼‰
        tail_missing = []
        for col in indicator_cols:
            recent_data = df[df['year'] >= 2022][col]
            if recent_data.isna().all():
                tail_missing.append(col)
        if tail_missing:
            print(f"   ğŸ“ å°¾éƒ¨ç¼ºå¤± (2022+æ— æ•°æ®): {len(tail_missing)} ä¸ªæŒ‡æ ‡")
            self.recommendations.append(f"ğŸ’¡ {len(tail_missing)}ä¸ªæŒ‡æ ‡ç¼ºå°‘2022å¹´åæ•°æ®ï¼Œå»ºè®®Holt-Winterså¤–æ¨")
        
        # 2. å¤´éƒ¨ç¼ºå¤±ï¼ˆæ—©æœŸå¹´ä»½ç¼ºå¤±ï¼‰
        head_missing = []
        for col in indicator_cols:
            early_data = df[df['year'] <= 2012][col]
            if early_data.isna().all():
                head_missing.append(col)
        if head_missing:
            print(f"   ğŸ“ å¤´éƒ¨ç¼ºå¤± (2012å‰æ— æ•°æ®): {len(head_missing)} ä¸ªæŒ‡æ ‡")
        
        # 3. éšæœºç¼ºå¤±
        random_missing = [col for col in indicator_cols 
                         if col not in tail_missing and col not in head_missing 
                         and df[col].isna().sum() > 0]
        if random_missing:
            print(f"   ğŸ“ éšæœº/ç»“æ„æ€§ç¼ºå¤±: {len(random_missing)} ä¸ªæŒ‡æ ‡")
            self.recommendations.append(f"ğŸ’¡ {len(random_missing)}ä¸ªæŒ‡æ ‡æœ‰ä¸­é—´ç¼ºå¤±ï¼Œå»ºè®®ä¸‰æ¬¡æ ·æ¡æ’å€¼")
        
        self.analysis_results['missing_patterns'] = {
            'tail_missing': tail_missing,
            'head_missing': head_missing,
            'random_missing': random_missing
        }
    
    def analyze_value_distributions(self):
        """åˆ†ææ•°å€¼åˆ†å¸ƒç‰¹å¾"""
        print("\n" + "=" * 80)
        print("ğŸ“ˆ 6. æ•°å€¼åˆ†å¸ƒåˆ†æ")
        print("=" * 80)
        
        merged_file = MERGED_DATA_DIR / "rd_innovation_wide.csv"
        if not merged_file.exists():
            return
            
        df = pd.read_csv(merged_file)
        indicator_cols = [c for c in df.columns if c not in ['country_code', 'year', 'country_cn', 'country_en']]
        
        print(f"\nåˆ†å¸ƒç‰¹å¾åˆ†æ:")
        print("-" * 85)
        print(f"{'æŒ‡æ ‡':<35} {'ååº¦':>8} {'å³°åº¦':>8} {'å˜å¼‚ç³»æ•°':>10} {'å»ºè®®å¯¹æ•°':>10}")
        print("-" * 85)
        
        distribution_analysis = []
        for col in indicator_cols:
            values = df[col].dropna()
            if len(values) < 10:
                continue
            
            skewness = values.skew()
            kurtosis = values.kurtosis()
            cv = values.std() / values.mean() if values.mean() != 0 else np.nan
            
            # åˆ¤æ–­æ˜¯å¦éœ€è¦å¯¹æ•°å˜æ¢
            # å¯¹äºR&Dæ•°æ®ï¼Œä¸»è¦æ˜¯æ¯”ä¾‹æŒ‡æ ‡ï¼Œä¸€èˆ¬ä¸éœ€è¦å¯¹æ•°å˜æ¢
            # åªæœ‰ç»å¯¹æ•°é‡æŒ‡æ ‡ï¼ˆå¦‚ä¸“åˆ©æ•°ï¼‰å¯èƒ½éœ€è¦
            need_log = False
            if col in INDICATOR_CATEGORIES.get('count_absolute', []):
                if abs(skewness) > 2 or cv > 2:
                    need_log = True
            
            distribution_analysis.append({
                'indicator': col,
                'skewness': skewness,
                'kurtosis': kurtosis,
                'cv': cv,
                'need_log': need_log
            })
            
            log_mark = "âœ…" if need_log else "âŒ"
            print(f"{col[:34]:<35} {skewness:>8.2f} {kurtosis:>8.2f} {cv:>10.2f} {log_mark:>10}")
        
        # ç»Ÿè®¡éœ€è¦å¯¹æ•°å˜æ¢çš„æŒ‡æ ‡
        need_log_count = sum(1 for d in distribution_analysis if d['need_log'])
        print(f"\nğŸ“ ç»“è®º: {need_log_count} ä¸ªæŒ‡æ ‡å»ºè®®å¯¹æ•°å˜æ¢ï¼ˆä¸»è¦æ˜¯ç»å¯¹æ•°é‡æŒ‡æ ‡ï¼‰")
        print("   ğŸ’¡ æ³¨æ„: R&Dæ•°æ®å¤šä¸ºæ¯”ä¾‹æŒ‡æ ‡ï¼Œä¸AIæŠ•èµ„æ•°æ®ä¸åŒï¼Œå¤§éƒ¨åˆ†ä¸éœ€è¦å¯¹æ•°å˜æ¢")
        
        self.analysis_results['distribution_analysis'] = distribution_analysis
    
    def analyze_recent_data_availability(self):
        """åˆ†æ2024-2025å¹´æ•°æ®å¯ç”¨æ€§"""
        print("\n" + "=" * 80)
        print("ğŸ¯ 7. 2024-2025å¹´æ•°æ®å¯ç”¨æ€§åˆ†æï¼ˆå…³é”®ï¼ï¼‰")
        print("=" * 80)
        
        merged_file = MERGED_DATA_DIR / "rd_innovation_wide.csv"
        if not merged_file.exists():
            return
            
        df = pd.read_csv(merged_file)
        indicator_cols = [c for c in df.columns if c not in ['country_code', 'year', 'country_cn', 'country_en']]
        
        # 2024å¹´æ•°æ®
        data_2024 = df[df['year'] == 2024]
        has_2024 = []
        missing_2024 = []
        
        for col in indicator_cols:
            if data_2024[col].notna().any():
                has_2024.append(col)
            else:
                missing_2024.append(col)
        
        print(f"\n2024å¹´æ•°æ®:")
        print(f"   âœ… æœ‰æ•°æ®: {len(has_2024)} ä¸ªæŒ‡æ ‡")
        print(f"   âŒ ç¼ºå¤±: {len(missing_2024)} ä¸ªæŒ‡æ ‡")
        
        if missing_2024:
            print(f"   ç¼ºå¤±æŒ‡æ ‡: {missing_2024[:5]}{'...' if len(missing_2024) > 5 else ''}")
        
        # 2025å¹´æ•°æ®
        data_2025 = df[df['year'] == 2025]
        if len(data_2025) == 0:
            print(f"\n2025å¹´æ•°æ®:")
            print(f"   âŒ æ— 2025å¹´æ•°æ®è®°å½•")
            self.issues.append("ğŸš¨ R&Dæ•°æ®é›†æ— 2025å¹´æ•°æ®ï¼Œéœ€è¦å¤–æ¨é¢„æµ‹")
        else:
            has_2025 = [col for col in indicator_cols if data_2025[col].notna().any()]
            print(f"\n2025å¹´æ•°æ®:")
            print(f"   æœ‰æ•°æ®: {len(has_2025)} ä¸ªæŒ‡æ ‡")
        
        self.analysis_results['recent_availability'] = {
            'has_2024': has_2024,
            'missing_2024': missing_2024,
            'has_2025': len(data_2025) > 0
        }
        
        if len(missing_2024) > 0:
            self.recommendations.append(f"ğŸ’¡ {len(missing_2024)}ä¸ªæŒ‡æ ‡ç¼ºå°‘2024å¹´æ•°æ®ï¼Œå»ºè®®æ—¶é—´åºåˆ—å¤–æ¨")
    
    def analyze_alignment_with_ai_data(self):
        """åˆ†æä¸AIæ•°æ®é›†çš„æ—¶é—´å¯¹é½æƒ…å†µ"""
        print("\n" + "=" * 80)
        print("ğŸ”— 8. ä¸ä¸»AIæ•°æ®é›†æ—¶é—´å¯¹é½åˆ†æ")
        print("=" * 80)
        
        merged_file = MERGED_DATA_DIR / "rd_innovation_wide.csv"
        if not merged_file.exists():
            return
        
        df = pd.read_csv(merged_file)
        
        # AIæ•°æ®é›†ç›®æ ‡å¹´ä»½æ˜¯2016-2025
        print(f"\nAIæ•°æ®é›†ç›®æ ‡èŒƒå›´: 2016-2025")
        print(f"R&Dæ•°æ®é›†èŒƒå›´: {df['year'].min()}-{df['year'].max()}")
        
        # æ£€æŸ¥2016-2025çš„è¦†ç›–
        indicator_cols = [c for c in df.columns if c not in ['country_code', 'year', 'country_cn', 'country_en']]
        
        alignment_issues = []
        for col in indicator_cols:
            col_data = df[df[col].notna()]
            covered_focus = [y for y in FOCUS_YEARS if y in col_data['year'].values]
            if len(covered_focus) < len(FOCUS_YEARS):
                missing_years = [y for y in FOCUS_YEARS if y not in covered_focus]
                alignment_issues.append({
                    'indicator': col,
                    'missing_years': missing_years
                })
        
        if alignment_issues:
            print(f"\nâš ï¸ {len(alignment_issues)} ä¸ªæŒ‡æ ‡åœ¨2016-2025æœŸé—´æœ‰ç¼ºå¤±:")
            for issue in alignment_issues[:10]:
                print(f"   â€¢ {issue['indicator']}: ç¼ºå¤± {issue['missing_years']}")
            
            # æŒ‰ç¼ºå¤±çš„å¹´ä»½åˆ†ç±»
            missing_2025_count = sum(1 for i in alignment_issues if 2025 in i['missing_years'])
            missing_2024_count = sum(1 for i in alignment_issues if 2024 in i['missing_years'])
            
            print(f"\n   ç¼ºå¤±2025å¹´: {missing_2025_count} ä¸ªæŒ‡æ ‡")
            print(f"   ç¼ºå¤±2024å¹´: {missing_2024_count} ä¸ªæŒ‡æ ‡")
        else:
            print(f"\nâœ… æ‰€æœ‰æŒ‡æ ‡åœ¨2016-2025æœŸé—´æ•°æ®å®Œæ•´")
        
        self.analysis_results['alignment_issues'] = alignment_issues
    
    def generate_report(self):
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        print("\n" + "=" * 100)
        print("ğŸ“‹ ç»¼åˆåˆ†ææŠ¥å‘Šä¸å»ºè®®")
        print("=" * 100)
        
        # é—®é¢˜æ±‡æ€»
        print("\nğŸš¨ å…³é”®é—®é¢˜:")
        if self.issues:
            for issue in self.issues:
                print(f"   {issue}")
        else:
            print("   âœ… æ— å…³é”®é—®é¢˜")
        
        # è­¦å‘Šæ±‡æ€»
        print(f"\nâš ï¸ è­¦å‘Š ({len(self.warnings)} æ¡):")
        for w in list(set(self.warnings))[:10]:
            print(f"   {w}")
        
        # å»ºè®®æ±‡æ€»
        print(f"\nğŸ’¡ é¢„å¤„ç†å»ºè®®:")
        for rec in list(set(self.recommendations)):
            print(f"   {rec}")
        
        # ä¸bé¢˜æ•°æ®æºè„šæœ¬çš„å·®å¼‚
        print(f"\nğŸ“ ä¸AIæ•°æ®é¢„å¤„ç†è„šæœ¬çš„å…³é”®å·®å¼‚:")
        print("   1. æ•°æ®ç±»å‹: R&Dæ•°æ®å¤šä¸ºæ¯”ä¾‹æŒ‡æ ‡ï¼Œæ— éœ€å¤§è§„æ¨¡å¯¹æ•°å˜æ¢")
        print("   2. è´§å¸å¤„ç†: æ— éœ€CPIé€šèƒ€è°ƒæ•´ï¼ˆæ— å¤§é‡ç¾å…ƒæŠ•èµ„æ•°æ®ï¼‰")
        print("   3. æ—¶é—´èŒƒå›´: æ•°æ®ä»2010å¹´å¼€å§‹ï¼Œæ—¶é—´åºåˆ—æ›´é•¿ï¼Œå¯ç”¨äºè¶‹åŠ¿åˆ†æ")
        print("   4. ç¼ºå¤±å¤„ç†: å°¾éƒ¨ç¼ºå¤±ï¼ˆ2024-2025ï¼‰éœ€å¤–æ¨ï¼Œä¸­é—´ç¼ºå¤±å¯æ’å€¼")
        print("   5. å›½å®¶å¼‚è´¨æ€§: é˜¿è”é…‹/å°åº¦æ•°æ®ä»éœ€ç‰¹æ®Šå…³æ³¨")
        
        # ä¿å­˜æŠ¥å‘Š
        report_content = f"""# R&Dä¸åˆ›æ–°åŸºç¡€æ•°æ®è´¨é‡åˆ†ææŠ¥å‘Š

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## æ•°æ®æ¦‚è§ˆ

- æ•°æ®æ¥æº: UNESCO UIS, World Bank
- æ—¶é—´èŒƒå›´: 2010-2024
- ç›®æ ‡å›½å®¶: {list(TARGET_COUNTRIES.keys())}
- é‡ç‚¹å¹´ä»½: 2016-2025ï¼ˆä¸AIæ•°æ®å¯¹é½ï¼‰

## å…³é”®é—®é¢˜

{chr(10).join(['- ' + i for i in self.issues]) if self.issues else 'æ— å…³é”®é—®é¢˜'}

## è­¦å‘Š

{chr(10).join(['- ' + w for w in list(set(self.warnings))])}

## é¢„å¤„ç†å»ºè®®

{chr(10).join(['- ' + r for r in list(set(self.recommendations))])}

## ä¸AIæ•°æ®é¢„å¤„ç†çš„å·®å¼‚

| æ–¹é¢ | AIæ•°æ®(bé¢˜æ•°æ®æº) | R&Dåˆ›æ–°æ•°æ® |
|------|------------------|-------------|
| æ•°æ®ç±»å‹ | æµé‡(æŠ•èµ„ã€å‘è¡¨) | å­˜é‡/æ¯”ä¾‹(R&Då æ¯”) |
| å¯¹æ•°å˜æ¢ | å¿…é¡»ï¼ˆååº¦>5ï¼‰ | ä»…ç»å¯¹æ•°é‡æŒ‡æ ‡ |
| é€šèƒ€è°ƒæ•´ | å¿…é¡»ï¼ˆå¤šå¹´ç¾å…ƒæ•°æ®ï¼‰ | ä¸éœ€è¦ |
| PPPè°ƒæ•´ | æŠ•èµ„ç±»éœ€è¦ | å·²æœ‰PPPç‰ˆæœ¬ |
| 2025å¤–æ¨ | éƒ¨åˆ†æŒ‡æ ‡éœ€è¦ | å¤šæ•°æŒ‡æ ‡éœ€è¦ |

## é¢„å¤„ç†æ¸…å•

### 1. æ—¶é—´ç»´åº¦å¤„ç†
- [ ] 2024-2025å¹´ç¼ºå¤±æ•°æ®å¤–æ¨ï¼ˆHolt-Wintersï¼‰
- [ ] ä¸­é—´å¹´ä»½ç¼ºå¤±æ’å€¼ï¼ˆä¸‰æ¬¡æ ·æ¡ï¼‰
- [ ] ä¸AIæ•°æ®2016-2025å¯¹é½

### 2. ç¼ºå¤±å€¼å¤„ç†
- [ ] å°¾éƒ¨ç¼ºå¤±ï¼šæ—¶é—´åºåˆ—å¤–æ¨
- [ ] éšæœºç¼ºå¤±ï¼šä¸‰æ¬¡æ ·æ¡æ’å€¼
- [ ] ç»“æ„æ€§ç¼ºå¤±ï¼ˆé˜¿è”é…‹ç­‰ï¼‰ï¼šæ ‡è®°å¹¶è€ƒè™‘é™æƒ

### 3. ç‰¹å¾å·¥ç¨‹
- [ ] åˆ›å»ºå¹´å¢é•¿ç‡ç‰¹å¾
- [ ] åˆ›å»º3å¹´ç§»åŠ¨å¹³å‡
- [ ] åˆ›å»ºä¸AIæ•°æ®çš„æ»åå…³è”ç‰¹å¾

### 4. æ•°æ®å¯¹é½
- [ ] å›½å®¶ä»£ç æ ‡å‡†åŒ–ï¼ˆISO 3166-1 alpha-3ï¼‰
- [ ] å¹´ä»½èŒƒå›´ç»Ÿä¸€ä¸º2016-2025
- [ ] è¾“å‡ºæ ¼å¼ä¸ä¸»è¡¨ä¸€è‡´
"""
        
        report_path = OUTPUT_DIR / "rd_data_quality_report.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"\nâœ… è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
        
        # ä¿å­˜åˆ†æç»“æœJSON
        results_path = OUTPUT_DIR / "rd_analysis_results.json"
        with open(results_path, 'w', encoding='utf-8') as f:
            # è½¬æ¢numpyç±»å‹
            def convert_types(obj):
                if isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, dict):
                    return {k: convert_types(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_types(i) for i in obj]
                return obj
            
            json.dump(convert_types(self.analysis_results), f, ensure_ascii=False, indent=2)
        
        print(f"âœ… åˆ†æç»“æœJSONå·²ä¿å­˜è‡³: {results_path}")


def main():
    """ä¸»å‡½æ•°"""
    analyzer = RDDataQualityAnalyzer()
    analyzer.run_full_analysis()


if __name__ == "__main__":
    main()
