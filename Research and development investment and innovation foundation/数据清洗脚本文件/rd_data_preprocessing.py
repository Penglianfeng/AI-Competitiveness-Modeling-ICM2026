# -*- coding: utf-8 -*-
"""
åæ•°æ¯ Bé¢˜ - R&Dä¸åˆ›æ–°åŸºç¡€æ•°æ®é¢„å¤„ç†è„šæœ¬
============================================
é’ˆå¯¹ Research and development investment and innovation foundation æ–‡ä»¶å¤¹
çš„æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œç”Ÿæˆå¯ä¸AIä¸»è¡¨åˆå¹¶çš„æ ‡å‡†åŒ–æ•°æ®

æ•°æ®ç‰¹ç‚¹ï¼š
- R&Dæ”¯å‡ºå æ¯”ã€ç ”ç©¶äººå‘˜å¯†åº¦ã€ä¸“åˆ©ç”³è¯·ã€é«˜ç­‰æ•™è‚²æŒ‡æ ‡
- æ¥æºï¼šUNESCO UISã€World Bank
- ä¸»è¦ä¸ºæ¯”ä¾‹æŒ‡æ ‡å’Œå­˜é‡æŒ‡æ ‡ï¼ˆæ— éœ€å¤§è§„æ¨¡å¯¹æ•°å˜æ¢å’Œé€šèƒ€è°ƒæ•´ï¼‰

è¾“å‡ºï¼š
- rd_innovation_preprocessed.csv: é¢„å¤„ç†åçš„å®½è¡¨
- å¯ç›´æ¥ä¸ä¸»AIæ•°æ®è¡¨æŒ‰Country+Yearåˆå¹¶

ä½œè€…: åæ•°æ¯å‚èµ›é˜Ÿ
æ—¥æœŸ: 2026-01-17
"""

import pandas as pd
import numpy as np
from pathlib import Path
from scipy import stats
from scipy.interpolate import CubicSpline
from datetime import datetime
import warnings
import json

warnings.filterwarnings('ignore')

# ============================================================================
# å…¨å±€é…ç½®
# ============================================================================

BASE_DIR = Path(r"d:\åæ•°æ¯\Research and development investment and innovation foundation")
MERGED_DATA_DIR = BASE_DIR / "merged_data"
OUTPUT_DIR = BASE_DIR / "preprocessed"
OUTPUT_DIR.mkdir(exist_ok=True)

# ç›®æ ‡å›½å®¶ï¼ˆä¸ä¸»æ•°æ®é›†ä¸€è‡´ï¼‰
TARGET_COUNTRIES = ['USA', 'CHN', 'GBR', 'DEU', 'FRA', 'CAN', 'JPN', 'KOR', 'ARE', 'IND']

# å›½å®¶ä¸­æ–‡å
COUNTRY_CN = {
    'USA': 'ç¾å›½', 'CHN': 'ä¸­å›½', 'GBR': 'è‹±å›½', 'DEU': 'å¾·å›½',
    'FRA': 'æ³•å›½', 'CAN': 'åŠ æ‹¿å¤§', 'JPN': 'æ—¥æœ¬', 'KOR': 'éŸ©å›½',
    'ARE': 'é˜¿è”é…‹', 'IND': 'å°åº¦'
}

# ç›®æ ‡å¹´ä»½ï¼ˆä¸AIæ•°æ®å¯¹é½ï¼‰
TARGET_YEARS = list(range(2016, 2026))

# æŒ‡æ ‡åˆ†ç±»åŠå¤„ç†ç­–ç•¥
INDICATOR_CONFIG = {
    # æ¯”ä¾‹æŒ‡æ ‡ï¼ˆç™¾åˆ†æ¯”ï¼‰ï¼Œæ— éœ€å¯¹æ•°å˜æ¢
    'rd_expenditure_pct_gdp': {
        'type': 'ratio', 'unit': '%', 'log_transform': False,
        'description': 'R&Dæ”¯å‡ºå GDPæ¯”ä¾‹'
    },
    'rd_expenditure_pct_gdp_wb': {
        'type': 'ratio', 'unit': '%', 'log_transform': False,
        'description': 'R&Dæ”¯å‡ºå GDPæ¯”ä¾‹(World Bank)'
    },
    'higher_edu_enrollment_rate': {
        'type': 'ratio', 'unit': '%', 'log_transform': False,
        'description': 'é«˜ç­‰æ•™è‚²æ¯›å…¥å­¦ç‡'
    },
    'bachelor_degree_pct': {
        'type': 'ratio', 'unit': '%', 'log_transform': False,
        'description': 'å­¦å£«å­¦ä½äººå£æ¯”ä¾‹'
    },
    'master_degree_pct': {
        'type': 'ratio', 'unit': '%', 'log_transform': False,
        'description': 'ç¡•å£«å­¦ä½äººå£æ¯”ä¾‹'
    },
    'phd_degree_pct': {
        'type': 'ratio', 'unit': '%', 'log_transform': False,
        'description': 'åšå£«å­¦ä½äººå£æ¯”ä¾‹'
    },
    'internet_users_pct': {
        'type': 'ratio', 'unit': '%', 'log_transform': False,
        'description': 'äº’è”ç½‘ç”¨æˆ·æ¯”ä¾‹'
    },
    'ict_service_exports_pct': {
        'type': 'ratio', 'unit': '%', 'log_transform': False,
        'description': 'ICTæœåŠ¡å‡ºå£å æ¯”'
    },
    'high_tech_exports_pct': {
        'type': 'ratio', 'unit': '%', 'log_transform': False,
        'description': 'é«˜ç§‘æŠ€å‡ºå£å åˆ¶æˆå“å‡ºå£æ¯”ä¾‹'
    },
    'govt_edu_expenditure_pct_gdp': {
        'type': 'ratio', 'unit': '%', 'log_transform': False,
        'description': 'æ”¿åºœæ•™è‚²æ”¯å‡ºå GDPæ¯”ä¾‹'
    },
    'higher_edu_expenditure_pct': {
        'type': 'ratio', 'unit': '%', 'log_transform': False,
        'description': 'é«˜ç­‰æ•™è‚²æ”¯å‡ºå æ•™è‚²æ”¯å‡ºæ¯”ä¾‹'
    },
    'labor_force_higher_edu_pct': {
        'type': 'ratio', 'unit': '%', 'log_transform': False,
        'description': 'é«˜ç­‰æ•™è‚²åŠ³åŠ¨åŠ›æ¯”ä¾‹'
    },
    
    # å¯†åº¦æŒ‡æ ‡ï¼ˆæ¯ç™¾ä¸‡äºº/æ¯ç™¾äººï¼‰
    'researchers_per_million': {
        'type': 'density', 'unit': 'per million', 'log_transform': False,
        'description': 'æ¯ç™¾ä¸‡äººç ”ç©¶äººå‘˜æ•°'
    },
    'researchers_per_million_wb': {
        'type': 'density', 'unit': 'per million', 'log_transform': False,
        'description': 'æ¯ç™¾ä¸‡äººç ”ç©¶äººå‘˜æ•°(World Bank)'
    },
    'RESEARCHERS_PER_MILLION': {
        'type': 'density', 'unit': 'per million', 'log_transform': False,
        'description': 'æ¯ç™¾ä¸‡äººç ”ç©¶äººå‘˜æ•°(UIS)'
    },
    'fixed_broadband_per_100': {
        'type': 'density', 'unit': 'per 100', 'log_transform': False,
        'description': 'æ¯ç™¾äººå›ºå®šå®½å¸¦è®¢é˜…æ•°'
    },
    'mobile_subscriptions_per_100': {
        'type': 'density', 'unit': 'per 100', 'log_transform': False,
        'description': 'æ¯ç™¾äººç§»åŠ¨ç”µè¯è®¢é˜…æ•°'
    },
    'secure_internet_servers_per_million': {
        'type': 'density', 'unit': 'per million', 'log_transform': False,
        'description': 'æ¯ç™¾ä¸‡äººå®‰å…¨äº’è”ç½‘æœåŠ¡å™¨æ•°'
    },
    
    # ç»å¯¹æ•°é‡æŒ‡æ ‡ï¼ˆå¯èƒ½éœ€è¦å¯¹æ•°å˜æ¢ï¼‰
    'patent_applications_resident': {
        'type': 'count', 'unit': 'count', 'log_transform': True,
        'description': 'å±…æ°‘ä¸“åˆ©ç”³è¯·æ•°'
    },
    'patent_applications_nonresident': {
        'type': 'count', 'unit': 'count', 'log_transform': True,
        'description': 'éå±…æ°‘ä¸“åˆ©ç”³è¯·æ•°'
    },
    'high_tech_exports_usd': {
        'type': 'monetary', 'unit': 'USD', 'log_transform': True,
        'description': 'é«˜ç§‘æŠ€äº§å“å‡ºå£ï¼ˆç¾å…ƒï¼‰'
    }
}


# ============================================================================
# å·¥å…·å‡½æ•°
# ============================================================================

def holt_winters_forecast(series: pd.Series, periods: int = 2) -> np.ndarray:
    """
    Holt-WintersæŒ‡æ•°å¹³æ»‘å¤–æ¨
    ç”¨äºå°¾éƒ¨ç¼ºå¤±ï¼ˆå¦‚é¢„æµ‹2024-2025å¹´æ•°æ®ï¼‰
    """
    try:
        from statsmodels.tsa.holtwinters import ExponentialSmoothing
        
        clean_series = series.dropna()
        if len(clean_series) < 4:
            return linear_extrapolate(clean_series, periods)
        
        try:
            # R&Dæ¯”ä¾‹æ•°æ®é€šå¸¸å¹³ç¨³å¢é•¿ï¼Œä½¿ç”¨åŠ æ³•è¶‹åŠ¿
            model = ExponentialSmoothing(
                clean_series.values,
                trend='add',
                seasonal=None,
                damped_trend=True
            )
            fitted = model.fit(optimized=True)
            forecast = fitted.forecast(periods)
            
            # å¯¹äºæ¯”ä¾‹æŒ‡æ ‡ï¼Œç¡®ä¿åœ¨åˆç†èŒƒå›´å†…
            return np.clip(forecast, 0, 100)
        except:
            return linear_extrapolate(clean_series, periods)
    except ImportError:
        return linear_extrapolate(series.dropna(), periods)


def linear_extrapolate(series: pd.Series, periods: int) -> np.ndarray:
    """ç®€å•çº¿æ€§å¤–æ¨"""
    if len(series) < 2:
        return np.array([series.iloc[-1]] * periods) if len(series) > 0 else np.array([np.nan] * periods)
    
    x = np.arange(len(series))
    y = series.values
    slope, intercept, _, _, _ = stats.linregress(x, y)
    
    future_x = np.arange(len(series), len(series) + periods)
    forecast = slope * future_x + intercept
    return forecast


def cubic_spline_interpolate(df: pd.DataFrame, country: str, 
                             year_col: str, value_col: str) -> pd.DataFrame:
    """
    ä¸‰æ¬¡æ ·æ¡æ’å€¼å¡«è¡¥ä¸­é—´ç¼ºå¤±å€¼
    """
    country_data = df[df['country_code'] == country].copy()
    country_data = country_data.sort_values(year_col)
    
    if len(country_data) < 4:
        return country_data
    
    valid_mask = country_data[value_col].notna()
    if valid_mask.sum() < 4:
        return country_data
    
    years_valid = country_data.loc[valid_mask, year_col].values
    values_valid = country_data.loc[valid_mask, value_col].values
    
    try:
        cs = CubicSpline(years_valid, values_valid)
        
        missing_mask = country_data[value_col].isna()
        if missing_mask.any():
            missing_years = country_data.loc[missing_mask, year_col].values
            # åªæ’å€¼èŒƒå›´å†…çš„å¹´ä»½
            for year in missing_years:
                if years_valid.min() <= year <= years_valid.max():
                    idx = country_data[country_data[year_col] == year].index[0]
                    interpolated_value = cs(year)
                    df.loc[idx, value_col] = interpolated_value
    except Exception as e:
        pass
    
    return df


def log_transform_column(series: pd.Series, check_skewness: bool = True) -> tuple:
    """
    å¯¹æ•°å˜æ¢ï¼ˆlog1pï¼‰
    è¿”å›: (å˜æ¢åçš„series, æ˜¯å¦å˜æ¢, åŸå§‹ååº¦)
    """
    clean = series.dropna()
    if len(clean) < 10:
        return series, False, np.nan
    
    skewness = clean.skew()
    
    if check_skewness and abs(skewness) > 2:
        transformed = np.log1p(series.clip(lower=0))
        return transformed, True, skewness
    
    return series, False, skewness


# ============================================================================
# ä¸»é¢„å¤„ç†ç±»
# ============================================================================

class RDDataPreprocessor:
    """R&Dä¸åˆ›æ–°åŸºç¡€æ•°æ®é¢„å¤„ç†å™¨"""
    
    def __init__(self):
        self.processing_log = []
        self.master_df = None
        
    def log(self, message: str):
        """è®°å½•å¤„ç†æ—¥å¿—"""
        self.processing_log.append({
            'timestamp': datetime.now().isoformat(),
            'message': message
        })
        print(f"  ğŸ“ {message}")
    
    def load_merged_data(self) -> pd.DataFrame:
        """åŠ è½½åˆå¹¶åçš„å®½è¡¨æ•°æ®"""
        print("\n" + "=" * 80)
        print("ğŸ“‚ 1. åŠ è½½æ•°æ®")
        print("=" * 80)
        
        merged_file = MERGED_DATA_DIR / "rd_innovation_wide.csv"
        if not merged_file.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°åˆå¹¶æ•°æ®æ–‡ä»¶: {merged_file}")
        
        df = pd.read_csv(merged_file)
        self.log(f"å·²åŠ è½½ {len(df)} è¡Œ, {len(df.columns)} åˆ—")
        
        # ç­›é€‰ç›®æ ‡å›½å®¶
        df = df[df['country_code'].isin(TARGET_COUNTRIES)]
        self.log(f"ç­›é€‰ç›®æ ‡å›½å®¶å: {len(df)} è¡Œ")
        
        return df
    
    def create_base_framework(self) -> pd.DataFrame:
        """åˆ›å»ºä¸»è¡¨åŸºç¡€æ¡†æ¶ï¼ˆç¡®ä¿æ‰€æœ‰å›½å®¶-å¹´ä»½ç»„åˆéƒ½å­˜åœ¨ï¼‰"""
        print("\n" + "=" * 80)
        print("ğŸ—ï¸ 2. åˆ›å»ºåŸºç¡€æ¡†æ¶")
        print("=" * 80)
        
        rows = []
        for country in TARGET_COUNTRIES:
            for year in TARGET_YEARS:
                rows.append({
                    'country_code': country,
                    'year': year,
                    'country_cn': COUNTRY_CN.get(country, country)
                })
        
        framework = pd.DataFrame(rows)
        self.log(f"åˆ›å»º {len(framework)} è¡ŒåŸºç¡€æ¡†æ¶ ({len(TARGET_COUNTRIES)}å›½ Ã— {len(TARGET_YEARS)}å¹´)")
        
        return framework
    
    def merge_with_framework(self, framework: pd.DataFrame, 
                             source_df: pd.DataFrame) -> pd.DataFrame:
        """å°†æºæ•°æ®åˆå¹¶åˆ°åŸºç¡€æ¡†æ¶"""
        # è·å–æŒ‡æ ‡åˆ—
        indicator_cols = [c for c in source_df.columns 
                         if c not in ['country_code', 'year', 'country_cn', 'country_en']]
        
        # ç­›é€‰ç›®æ ‡å¹´ä»½
        source_df = source_df[source_df['year'].isin(TARGET_YEARS)]
        
        # åˆå¹¶
        result = framework.merge(
            source_df[['country_code', 'year'] + indicator_cols],
            on=['country_code', 'year'],
            how='left'
        )
        
        self.log(f"å·²åˆå¹¶ {len(indicator_cols)} ä¸ªæŒ‡æ ‡åˆ—")
        
        return result
    
    def interpolate_missing(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ’å€¼å¤„ç†ä¸­é—´ç¼ºå¤±å€¼"""
        print("\n" + "=" * 80)
        print("ğŸ”§ 3. ç¼ºå¤±å€¼æ’å€¼ï¼ˆä¸‰æ¬¡æ ·æ¡ï¼‰")
        print("=" * 80)
        
        indicator_cols = [c for c in df.columns 
                         if c not in ['country_code', 'year', 'country_cn']]
        
        interpolated_count = 0
        
        for col in indicator_cols:
            for country in TARGET_COUNTRIES:
                country_mask = df['country_code'] == country
                country_data = df[country_mask].sort_values('year')
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ä¸­é—´ç¼ºå¤±ï¼ˆä¸¤ç«¯æœ‰å€¼ï¼Œä¸­é—´ç¼ºå¤±ï¼‰
                values = country_data[col].values
                valid_indices = np.where(~np.isnan(values.astype(float)))[0]
                
                if len(valid_indices) < 4:
                    continue
                
                # æ£€æµ‹ä¸­é—´ç¼ºå¤±
                first_valid = valid_indices[0]
                last_valid = valid_indices[-1]
                
                for i in range(first_valid + 1, last_valid):
                    if np.isnan(float(values[i])):
                        # æœ‰ä¸­é—´ç¼ºå¤±ï¼Œè¿›è¡Œæ’å€¼
                        df = cubic_spline_interpolate(df, country, 'year', col)
                        interpolated_count += 1
                        break
        
        self.log(f"å®Œæˆ {interpolated_count} æ¬¡ä¸‰æ¬¡æ ·æ¡æ’å€¼")
        
        return df
    
    def extrapolate_tail_missing(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¤–æ¨å°¾éƒ¨ç¼ºå¤±å€¼ï¼ˆ2024-2025ï¼‰"""
        print("\n" + "=" * 80)
        print("ğŸ“ˆ 4. å°¾éƒ¨ç¼ºå¤±å¤–æ¨ï¼ˆHolt-Wintersï¼‰")
        print("=" * 80)
        
        indicator_cols = [c for c in df.columns 
                         if c not in ['country_code', 'year', 'country_cn']]
        
        extrapolated_count = 0
        
        for col in indicator_cols:
            for country in TARGET_COUNTRIES:
                country_mask = df['country_code'] == country
                country_data = df[country_mask].sort_values('year')
                
                # æ£€æŸ¥2024-2025æ˜¯å¦ç¼ºå¤±
                val_2024 = country_data[country_data['year'] == 2024][col].values
                val_2025 = country_data[country_data['year'] == 2025][col].values
                
                missing_years = []
                if len(val_2024) == 0 or pd.isna(val_2024[0]):
                    missing_years.append(2024)
                if len(val_2025) == 0 or pd.isna(val_2025[0]):
                    missing_years.append(2025)
                
                if not missing_years:
                    continue
                
                # è·å–å†å²æ•°æ®è¿›è¡Œå¤–æ¨
                historical = country_data[country_data['year'] < min(missing_years)][col].dropna()
                if len(historical) < 3:
                    continue
                
                # å¤–æ¨
                periods = len(missing_years)
                try:
                    forecast = holt_winters_forecast(historical, periods)
                    
                    for i, year in enumerate(missing_years):
                        idx = df[(df['country_code'] == country) & (df['year'] == year)].index
                        if len(idx) > 0:
                            df.loc[idx[0], col] = forecast[i]
                            extrapolated_count += 1
                except Exception as e:
                    continue
        
        self.log(f"å®Œæˆ {extrapolated_count} ä¸ªå€¼çš„Holt-Winterså¤–æ¨")
        
        return df
    
    def apply_log_transform(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¯¹ç»å¯¹æ•°é‡æŒ‡æ ‡åº”ç”¨å¯¹æ•°å˜æ¢"""
        print("\n" + "=" * 80)
        print("ğŸ“Š 5. å¯¹æ•°å˜æ¢ï¼ˆä»…ç»å¯¹æ•°é‡æŒ‡æ ‡ï¼‰")
        print("=" * 80)
        
        transform_cols = [col for col, config in INDICATOR_CONFIG.items() 
                         if config.get('log_transform', False) and col in df.columns]
        
        for col in transform_cols:
            transformed, did_transform, skewness = log_transform_column(df[col])
            if did_transform:
                df[f'{col}_log'] = transformed
                self.log(f"{col}: ååº¦={skewness:.2f}, å·²æ·»åŠ å¯¹æ•°å˜æ¢åˆ—")
            else:
                self.log(f"{col}: ååº¦={skewness:.2f}, æ— éœ€å¯¹æ•°å˜æ¢")
        
        return df
    
    def add_derived_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ·»åŠ è¡ç”Ÿç‰¹å¾"""
        print("\n" + "=" * 80)
        print("ğŸ”¨ 6. æ„å»ºè¡ç”Ÿç‰¹å¾")
        print("=" * 80)
        
        # æŒ‰å›½å®¶æ’åºä»¥è®¡ç®—æ»åå’Œå¢é•¿ç‡
        df = df.sort_values(['country_code', 'year'])
        
        # 1. å¹´å¢é•¿ç‡
        growth_cols = ['rd_expenditure_pct_gdp', 'researchers_per_million', 'higher_edu_enrollment_rate']
        for col in growth_cols:
            if col in df.columns:
                growth_col = f'{col}_YoY_Growth'
                df[growth_col] = df.groupby('country_code')[col].pct_change() * 100
                self.log(f"å·²è®¡ç®— {col} å¹´å¢é•¿ç‡")
        
        # 2. 3å¹´ç§»åŠ¨å¹³å‡ï¼ˆå¹³æ»‘æ³¢åŠ¨ï¼‰
        ma_cols = ['rd_expenditure_pct_gdp', 'researchers_per_million']
        for col in ma_cols:
            if col in df.columns:
                ma_col = f'{col}_MA3'
                df[ma_col] = df.groupby('country_code')[col].transform(
                    lambda x: x.rolling(window=3, min_periods=2).mean()
                )
                self.log(f"å·²è®¡ç®— {col} 3å¹´ç§»åŠ¨å¹³å‡")
        
        # 3. æ»åç‰¹å¾ï¼ˆç”¨äºä¸AIäº§å‡ºçš„å› æœåˆ†æï¼‰
        lag_cols = ['rd_expenditure_pct_gdp', 'researchers_per_million', 
                    'higher_edu_enrollment_rate', 'patent_applications_resident']
        for col in lag_cols:
            if col in df.columns:
                for lag in [1, 2, 3]:
                    lag_col = f'{col}_lag{lag}'
                    df[lag_col] = df.groupby('country_code')[col].shift(lag)
                self.log(f"å·²ä¸º {col} æ·»åŠ 1-3å¹´æ»åç‰¹å¾")
        
        # 4. ç»¼åˆåˆ›æ–°æŒ‡æ•°ï¼ˆå¯é€‰ï¼‰
        # æ ‡å‡†åŒ–ååŠ æƒå¹³å‡
        innovation_components = ['rd_expenditure_pct_gdp', 'researchers_per_million', 
                                 'higher_edu_enrollment_rate', 'internet_users_pct']
        available_components = [c for c in innovation_components if c in df.columns]
        
        if len(available_components) >= 3:
            # Min-Maxæ ‡å‡†åŒ–
            for col in available_components:
                min_val = df[col].min()
                max_val = df[col].max()
                if max_val > min_val:
                    df[f'{col}_normalized'] = (df[col] - min_val) / (max_val - min_val)
            
            # åŠ æƒå¹³å‡
            norm_cols = [f'{c}_normalized' for c in available_components]
            df['Innovation_Foundation_Index'] = df[norm_cols].mean(axis=1)
            self.log(f"å·²è®¡ç®—åˆ›æ–°åŸºç¡€ç»¼åˆæŒ‡æ•°ï¼ˆåŸºäº{len(available_components)}ä¸ªæŒ‡æ ‡ï¼‰")
            
            # åˆ é™¤ä¸­é—´æ ‡å‡†åŒ–åˆ—
            df = df.drop(columns=norm_cols)
        
        # 5. ä¸“åˆ©å¼ºåº¦ï¼ˆå±…æ°‘/éå±…æ°‘æ¯”ï¼‰
        if 'patent_applications_resident' in df.columns and 'patent_applications_nonresident' in df.columns:
            df['patent_intensity_ratio'] = (
                df['patent_applications_resident'] / 
                df['patent_applications_nonresident'].replace(0, np.nan)
            )
            self.log("å·²è®¡ç®—ä¸“åˆ©å¼ºåº¦æ¯”ï¼ˆå±…æ°‘/éå±…æ°‘ï¼‰")
        
        return df
    
    def handle_structural_missing(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¤„ç†ç»“æ„æ€§ç¼ºå¤±ï¼ˆç‰¹åˆ«æ˜¯æ–°å…´å›½å®¶ï¼‰"""
        print("\n" + "=" * 80)
        print("ğŸŒ 7. ç»“æ„æ€§ç¼ºå¤±å¤„ç†")
        print("=" * 80)
        
        # æ£€æŸ¥é˜¿è”é…‹å’Œå°åº¦çš„è¦†ç›–æƒ…å†µ
        for country in ['ARE', 'IND']:
            country_data = df[df['country_code'] == country]
            indicator_cols = [c for c in df.columns 
                             if c not in ['country_code', 'year', 'country_cn']]
            
            missing_counts = {}
            for col in indicator_cols:
                missing = country_data[col].isna().sum()
                if missing > 0:
                    missing_counts[col] = missing
            
            if missing_counts:
                self.log(f"{country} ({COUNTRY_CN[country]}): {len(missing_counts)} ä¸ªæŒ‡æ ‡æœ‰ç¼ºå¤±")
                # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„å¤„ç†é€»è¾‘
                # ä¾‹å¦‚ä½¿ç”¨ç›¸ä¼¼å›½å®¶æ•°æ®ä¼°ç®—
        
        return df
    
    def validate_output(self, df: pd.DataFrame) -> dict:
        """éªŒè¯è¾“å‡ºæ•°æ®è´¨é‡"""
        print("\n" + "=" * 80)
        print("âœ… 8. è¾“å‡ºéªŒè¯")
        print("=" * 80)
        
        validation = {
            'total_rows': len(df),
            'total_columns': len(df.columns),
            'countries': df['country_code'].nunique(),
            'years': df['year'].nunique(),
            'year_range': (int(df['year'].min()), int(df['year'].max())),
            'missing_summary': {}
        }
        
        # æ£€æŸ¥æ¯ä¸ªæŒ‡æ ‡çš„ç¼ºå¤±ç‡
        indicator_cols = [c for c in df.columns 
                         if c not in ['country_code', 'year', 'country_cn']]
        
        for col in indicator_cols:
            missing_pct = df[col].isna().sum() / len(df) * 100
            validation['missing_summary'][col] = missing_pct
        
        # æ‰“å°éªŒè¯ç»“æœ
        print(f"   æ€»è¡Œæ•°: {validation['total_rows']}")
        print(f"   æ€»åˆ—æ•°: {validation['total_columns']}")
        print(f"   å›½å®¶æ•°: {validation['countries']}")
        print(f"   å¹´ä»½èŒƒå›´: {validation['year_range'][0]}-{validation['year_range'][1]}")
        
        # æ£€æŸ¥é«˜ç¼ºå¤±ç‡æŒ‡æ ‡
        high_missing = {k: v for k, v in validation['missing_summary'].items() if v > 30}
        if high_missing:
            print(f"\n   âš ï¸ é«˜ç¼ºå¤±ç‡æŒ‡æ ‡ (>30%):")
            for col, pct in sorted(high_missing.items(), key=lambda x: -x[1])[:5]:
                print(f"      â€¢ {col}: {pct:.1f}%")
        
        return validation
    
    def save_output(self, df: pd.DataFrame, validation: dict):
        """ä¿å­˜é¢„å¤„ç†ç»“æœ"""
        print("\n" + "=" * 80)
        print("ğŸ’¾ 9. ä¿å­˜è¾“å‡º")
        print("=" * 80)
        
        # ä¿å­˜CSV
        csv_path = OUTPUT_DIR / "rd_innovation_preprocessed.csv"
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        self.log(f"CSVå·²ä¿å­˜: {csv_path}")
        
        # ä¿å­˜Excel
        excel_path = OUTPUT_DIR / "rd_innovation_preprocessed.xlsx"
        df.to_excel(excel_path, index=False)
        self.log(f"Excelå·²ä¿å­˜: {excel_path}")
        
        # ä¿å­˜åˆ—è¯´æ˜
        column_desc = {
            'country_code': 'å›½å®¶ä»£ç ï¼ˆISO 3166-1 alpha-3ï¼‰',
            'year': 'å¹´ä»½',
            'country_cn': 'å›½å®¶ä¸­æ–‡å',
            'rd_expenditure_pct_gdp': 'R&Dæ”¯å‡ºå GDPæ¯”ä¾‹(%)',
            'researchers_per_million': 'æ¯ç™¾ä¸‡äººç ”ç©¶äººå‘˜æ•°',
            'higher_edu_enrollment_rate': 'é«˜ç­‰æ•™è‚²æ¯›å…¥å­¦ç‡(%)',
            'patent_applications_resident': 'å±…æ°‘ä¸“åˆ©ç”³è¯·æ•°',
            'patent_applications_nonresident': 'éå±…æ°‘ä¸“åˆ©ç”³è¯·æ•°',
            'high_tech_exports_pct': 'é«˜ç§‘æŠ€å‡ºå£å åˆ¶æˆå“å‡ºå£æ¯”ä¾‹(%)',
            'internet_users_pct': 'äº’è”ç½‘ç”¨æˆ·æ¯”ä¾‹(%)',
            '*_YoY_Growth': 'å¹´åŒæ¯”å¢é•¿ç‡(%)',
            '*_MA3': '3å¹´ç§»åŠ¨å¹³å‡',
            '*_lag1/2/3': 'æ»å1/2/3å¹´ç‰¹å¾',
            '*_log': 'å¯¹æ•°å˜æ¢å€¼(log1p)',
            'Innovation_Foundation_Index': 'åˆ›æ–°åŸºç¡€ç»¼åˆæŒ‡æ•°(0-1æ ‡å‡†åŒ–)'
        }
        
        desc_path = OUTPUT_DIR / "column_descriptions.json"
        with open(desc_path, 'w', encoding='utf-8') as f:
            json.dump(column_desc, f, ensure_ascii=False, indent=2)
        self.log(f"åˆ—è¯´æ˜å·²ä¿å­˜: {desc_path}")
        
        # ä¿å­˜å¤„ç†æ—¥å¿—
        log_path = OUTPUT_DIR / "preprocessing_log.json"
        with open(log_path, 'w', encoding='utf-8') as f:
            json.dump(self.processing_log, f, ensure_ascii=False, indent=2)
        self.log(f"å¤„ç†æ—¥å¿—å·²ä¿å­˜: {log_path}")
        
        # ä¿å­˜éªŒè¯æŠ¥å‘Š
        validation_path = OUTPUT_DIR / "validation_report.json"
        # è½¬æ¢numpyç±»å‹
        def convert(obj):
            if isinstance(obj, (np.integer, np.int64)):
                return int(obj)
            elif isinstance(obj, (np.floating, np.float64)):
                return float(obj)
            return obj
        
        validation_converted = {k: convert(v) if not isinstance(v, dict) 
                               else {kk: convert(vv) for kk, vv in v.items()} 
                               for k, v in validation.items()}
        
        with open(validation_path, 'w', encoding='utf-8') as f:
            json.dump(validation_converted, f, ensure_ascii=False, indent=2)
        self.log(f"éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {validation_path}")
    
    def run(self):
        """æ‰§è¡Œå®Œæ•´é¢„å¤„ç†æµç¨‹"""
        print("=" * 100)
        print("ğŸ”¬ åæ•°æ¯ Bé¢˜ - R&Dä¸åˆ›æ–°åŸºç¡€æ•°æ®é¢„å¤„ç†")
        print("=" * 100)
        print(f"æ‰§è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # 1. åŠ è½½æ•°æ®
        source_df = self.load_merged_data()
        
        # 2. åˆ›å»ºåŸºç¡€æ¡†æ¶
        framework = self.create_base_framework()
        
        # 3. åˆå¹¶æ•°æ®
        df = self.merge_with_framework(framework, source_df)
        
        # 4. æ’å€¼ä¸­é—´ç¼ºå¤±
        df = self.interpolate_missing(df)
        
        # 5. å¤–æ¨å°¾éƒ¨ç¼ºå¤±
        df = self.extrapolate_tail_missing(df)
        
        # 6. å¯¹æ•°å˜æ¢
        df = self.apply_log_transform(df)
        
        # 7. è¡ç”Ÿç‰¹å¾
        df = self.add_derived_features(df)
        
        # 8. å¤„ç†ç»“æ„æ€§ç¼ºå¤±
        df = self.handle_structural_missing(df)
        
        # 9. éªŒè¯
        validation = self.validate_output(df)
        
        # 10. ä¿å­˜
        self.save_output(df, validation)
        
        self.master_df = df
        
        print("\n" + "=" * 100)
        print("âœ… R&Dä¸åˆ›æ–°åŸºç¡€æ•°æ®é¢„å¤„ç†å®Œæˆ!")
        print("=" * 100)
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        print(f"\nğŸ“Š è¾“å‡ºç»Ÿè®¡:")
        print(f"   - ç»´åº¦: {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")
        print(f"   - å›½å®¶: {', '.join(TARGET_COUNTRIES)}")
        print(f"   - å¹´ä»½: {TARGET_YEARS[0]}-{TARGET_YEARS[-1]}")
        print(f"   - è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
        
        return df


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================

def main():
    """ä¸»å‡½æ•°"""
    preprocessor = RDDataPreprocessor()
    df = preprocessor.run()
    
    # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
    print("\nğŸ“‹ è¾“å‡ºæ•°æ®é¢„è§ˆï¼ˆå‰5è¡Œï¼‰:")
    print(df.head().to_string())


if __name__ == "__main__":
    main()
