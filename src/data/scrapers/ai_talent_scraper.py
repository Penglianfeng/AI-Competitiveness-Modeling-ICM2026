#!/usr/bin/env python3
"""
AIäººæ‰æ•°æ®çˆ¬å–å™¨ V2 (ä¿®å¤ç‰ˆ)
============================
ä¿®å¤äº†UNESCOå’ŒOECDçš„APIè°ƒç”¨é—®é¢˜

æ•°æ®æ¥æºï¼š
1. World Bank Open Data API - æ•™è‚²ä¸äººåŠ›èµ„æœ¬æŒ‡æ ‡
2. UNESCO UIS Data Browser - ç›´æ¥ä¸‹è½½CSV
3. OECD. Stat - ä½¿ç”¨æ­£ç¡®çš„SDMXç«¯ç‚¹
"""

import requests
import pandas as pd
import json
import time
import os
import io
from datetime import datetime
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
import logging
import urllib.parse

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ============================================================================
# é…ç½®
# ============================================================================

@dataclass
class CountryConfig:
    """å›½å®¶é…ç½®"""
    iso3: str
    iso2: str
    name_en: str
    name_cn: str
    oecd_member: bool


COUNTRIES = {
    "USA": CountryConfig("USA", "US", "United States", "ç¾å›½", True),
    "CHN": CountryConfig("CHN", "CN", "China", "ä¸­å›½", False),
    "GBR": CountryConfig("GBR", "GB", "United Kingdom", "è‹±å›½", True),
    "DEU": CountryConfig("DEU", "DE", "Germany", "å¾·å›½", True),
    "KOR": CountryConfig("KOR", "KR", "South Korea", "éŸ©å›½", True),
    "JPN": CountryConfig("JPN", "JP", "Japan", "æ—¥æœ¬", True),
    "FRA": CountryConfig("FRA", "FR", "France", "æ³•å›½", True),
    "CAN": CountryConfig("CAN", "CA", "Canada", "åŠ æ‹¿å¤§", True),
    "ARE": CountryConfig("ARE", "AE", "United Arab Emirates", "é˜¿è”é…‹", False),
    "IND": CountryConfig("IND", "IN", "India", "å°åº¦", False),
}


# ============================================================================
# World Bank çˆ¬å–å™¨ (å·²éªŒè¯å¯ç”¨)
# ============================================================================

class WorldBankScraper:
    """World Bankæ•°æ®çˆ¬å–å™¨"""
    
    BASE_URL = "https://api.worldbank.org/v2"
    
    # å·²éªŒè¯å¯ç”¨çš„æŒ‡æ ‡
    INDICATORS = {
        # ç§‘ç ”äººå‘˜
        "SP.POP.SCIE.RD.P6": ("æ¯ç™¾ä¸‡äººç ”ç©¶äººå‘˜æ•°", "researchers"),
        "SP.POP.TECH.RD.P6": ("æ¯ç™¾ä¸‡äººç ”å‘æŠ€æœ¯äººå‘˜æ•°", "researchers"),
        
        # é«˜ç­‰æ•™è‚²
        "SE.TER.ENRR": ("é«˜ç­‰æ•™è‚²æ¯›å…¥å­¦ç‡(%)", "education"),
        "SE.TER.ENRL": ("é«˜ç­‰æ•™è‚²åœ¨æ ¡ç”Ÿæ€»æ•°", "education"),
        "SE.TER.ENRL.FE.ZS": ("é«˜ç­‰æ•™è‚²å¥³æ€§å æ¯”(%)", "education"),
        
        # æ•™è‚²æ”¯å‡º
        "SE.XPD.TOTL.GD.ZS": ("æ•™è‚²æ”¯å‡ºå GDPæ¯”ä¾‹(%)", "investment"),
        "SE.XPD.TERT.PC.ZS": ("é«˜ç­‰æ•™è‚²ç”Ÿå‡æ”¯å‡ºå äººå‡GDPæ¯”ä¾‹(%)", "investment"),
        "GB.XPD.RSDV.GD.ZS": ("R&Dæ”¯å‡ºå GDPæ¯”ä¾‹(%)", "investment"),
        
        # äººå£
        "SP.POP.TOTL": ("æ€»äººå£", "demographic"),
        "SP.POP.1564.TO.ZS": ("15-64å²äººå£å æ¯”(%)", "demographic"),
    }
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({"User-Agent": "AI-Talent-Research/2.0"})
    
    def fetch_indicator(self, indicator:  str, countries: List[str],
                       start_year: int, end_year: int) -> pd.DataFrame:
        """è·å–å•ä¸ªæŒ‡æ ‡"""
        country_str = ";".join(countries)
        url = f"{self.BASE_URL}/country/{country_str}/indicator/{indicator}"
        
        params = {
            "format": "json",
            "per_page": 1000,
            "date":  f"{start_year}:{end_year}"
        }
        
        try:
            resp = self.session.get(url, params=params, timeout=30)
            resp.raise_for_status()
            data = resp.json()
            
            if isinstance(data, list) and len(data) > 1 and data[1]: 
                records = []
                for item in data[1]:
                    records.append({
                        "country_code": item.get("countryiso3code"),
                        "country_name": item.get("country", {}).get("value"),
                        "year": int(item.get("date")) if item.get("date") else None,
                        "value":  item.get("value"),
                        "indicator_code": indicator
                    })
                return pd.DataFrame(records)
        except Exception as e:
            logger.debug(f"World Bankè¯·æ±‚å¤±è´¥ [{indicator}]: {e}")
        
        return pd.DataFrame()
    
    def fetch_all(self, countries: List[str], start_year: int, end_year: int) -> pd.DataFrame:
        """è·å–æ‰€æœ‰æŒ‡æ ‡"""
        logger.info(f"[World Bank] å¼€å§‹çˆ¬å– {len(self.INDICATORS)} ä¸ªæŒ‡æ ‡...")
        
        all_data = []
        for code, (name_cn, category) in self.INDICATORS.items():
            logger.info(f"  - {name_cn}")
            df = self.fetch_indicator(code, countries, start_year, end_year)
            
            if not df.empty:
                df["indicator_name_cn"] = name_cn
                df["category"] = category
                df["source"] = "World Bank"
                all_data.append(df)
                valid_count = df["value"].notna().sum()
                logger.info(f"    âœ“ {len(df)} æ¡è®°å½• ({valid_count} æ¡æœ‰æ•ˆ)")
            else:
                logger.warning(f"    âœ— æ— æ•°æ®")
            
            time.sleep(0.3)
        
        if all_data:
            return pd. concat(all_data, ignore_index=True)
        return pd.DataFrame()


# ============================================================================
# UNESCO UIS çˆ¬å–å™¨ (ä¿®å¤ç‰ˆ - ä½¿ç”¨Bulk Download)
# ============================================================================

class UNESCOScraper: 
    """UNESCO UISæ•°æ®çˆ¬å–å™¨ - ä½¿ç”¨Bulk Download Service"""
    
    # UNESCO UIS Bulk Data Download URLs
    BULK_URLS = {
        "SDG": "https://uis.unesco.org/sites/default/files/documents/SDG.zip",
        "STI": "https://uis.unesco.org/sites/default/files/documents/UIS_STI.zip",
        "EDU": "https://uis.unesco.org/sites/default/files/documents/UIS_Education.zip",
    }
    
    # å¤‡ç”¨ï¼šä½¿ç”¨SDMX API
    SDMX_BASE = "https://api.uis.unesco.org/sdmx/data"
    
    # å…³é”®æŒ‡æ ‡ (SDMX dataflowå’Œkey)
    SDMX_QUERIES = {
        "researchers_per_million": {
            "dataflow": "UNESCO,STI,1.0",
            "key": "....RD_P....",  # Researchers per million
            "name_cn": "æ¯ç™¾ä¸‡äººç ”ç©¶äººå‘˜æ•°(FTE)"
        },
        "gerd_gdp": {
            "dataflow": "UNESCO,STI,1.0",
            "key": "....XPD_GERD_GDP....",
            "name_cn": "R&Dæ”¯å‡ºå GDPæ¯”ä¾‹"
        },
    }
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "AI-Talent-Research/2.0",
            "Accept":  "application/vnd.sdmx.data+csv;version=1.0"
        })
    
    def fetch_via_sdmx(self, countries: List[str], start_year: int, end_year: int) -> pd.DataFrame:
        """é€šè¿‡SDMX APIè·å–æ•°æ®"""
        
        all_data = []
        country_filter = "+".join(countries)
        
        # å°è¯•è·å–STIæ•°æ®
        dataflows = [
            ("UNESCO,STI,1.0", "ç§‘æŠ€åˆ›æ–°æ•°æ®"),
            ("UNESCO,SDG,3.0", "SDGæŒ‡æ ‡æ•°æ®"),
        ]
        
        for dataflow, desc in dataflows:
            logger.info(f"  å°è¯•è·å– {desc}...")
            
            # æ„å»ºSDMX URL
            url = f"{self. SDMX_BASE}/{dataflow}/{country_filter}"
            
            params = {
                "startPeriod": str(start_year),
                "endPeriod": str(end_year),
                "format":  "csv"
            }
            
            try:
                resp = self.session.get(url, params=params, timeout=60)
                
                if resp.status_code == 200:
                    # è§£æCSVå“åº”
                    df = pd.read_csv(io.StringIO(resp.text))
                    if not df.empty:
                        df["source"] = "UNESCO"
                        df["dataflow"] = dataflow
                        all_data.append(df)
                        logger.info(f"    âœ“ è·å– {len(df)} æ¡è®°å½•")
                else:
                    logger.debug(f"    çŠ¶æ€ç : {resp.status_code}")
                    
            except Exception as e:
                logger.debug(f"    è¯·æ±‚å¤±è´¥: {e}")
            
            time.sleep(0.5)
        
        if all_data:
            return pd.concat(all_data, ignore_index=True)
        return pd.DataFrame()
    
    def fetch_via_data_explorer(self, countries: List[str], start_year: int, end_year: int) -> pd.DataFrame:
        """é€šè¿‡Data Explorer APIè·å–æ•°æ® (å¤‡ç”¨æ–¹æ³•)"""
        
        # UIS Data Explorer REST API
        base_url = "http://data.uis.unesco.org/RestSDMX/sdmx.ashx/GetData"
        
        datasets = [
            "EDULIT_DS",  # Education and Literacy
            "STI_DS",     # Science, Technology and Innovation
        ]
        
        all_data = []
        
        for dataset in datasets:
            logger.info(f"  å°è¯• Data Explorer:  {dataset}")
            
            url = f"{base_url}/{dataset}/{'+'.join(countries)}"
            params = {"startTime": start_year, "endTime": end_year}
            
            try:
                resp = self.session.get(url, params=params, timeout=60)
                if resp.status_code == 200:
                    # å°è¯•è§£æXMLæˆ–JSON
                    if "xml" in resp.headers.get("Content-Type", ""):
                        # XMLè§£æé€»è¾‘
                        pass
                    else:
                        data = resp.json() if resp.text. startswith("{") else None
                        if data:
                            all_data.append(pd.DataFrame(data))
            except Exception as e:
                logger.debug(f"    å¤±è´¥: {e}")
            
            time.sleep(0.5)
        
        if all_data:
            return pd.concat(all_data, ignore_index=True)
        return pd.DataFrame()
    
    def fetch_predefined_data(self) -> pd.DataFrame:
        """è·å–é¢„å®šä¹‰çš„UNESCOå…³é”®æ•°æ® (åŸºäºå·²çŸ¥å¯ç”¨æ•°æ®)"""
        
        logger.info("  ä½¿ç”¨ç›´æ¥APIæŸ¥è¯¢è·å–æ•°æ®...")
        
        # ç›´æ¥æ„é€ å·²çŸ¥å¯ç”¨çš„æ•°æ®æŸ¥è¯¢
        queries = [
            # SDG 9.5. 1 - R&D expenditure as % of GDP
            {
                "url": "https://api.uis.unesco.org/sdmx/data/UNESCO,SDG4,1.0/.",
                "indicator": "SDG 9.5.1",
                "name_cn": "R&Dæ”¯å‡ºå GDPæ¯”ä¾‹(SDG)"
            },
            # SDG 9.5.2 - Researchers per million
            {
                "url": "https://api.uis.unesco.org/sdmx/data/UNESCO,SDG4,1.0/.",
                "indicator": "SDG 9.5.2", 
                "name_cn":  "æ¯ç™¾ä¸‡äººç ”ç©¶äººå‘˜(SDG)"
            },
        ]
        
        # ç”±äºUNESCO APIç»“æ„å¤æ‚ï¼Œè¿™é‡Œæä¾›å¤‡ç”¨çš„æ‰‹åŠ¨æ•°æ®è·å–å»ºè®®
        logger.info("  âš ï¸ UNESCO APIéœ€è¦ç‰¹å®šçš„è®¤è¯æˆ–æ ¼å¼")
        logger.info("  ğŸ’¡ å»ºè®®æ‰‹åŠ¨ä¸‹è½½:  https://data.uis.unesco.org/")
        
        return pd.DataFrame()
    
    def fetch_all(self, countries: List[str], start_year: int, end_year: int) -> pd.DataFrame:
        """è·å–æ‰€æœ‰UNESCOæ•°æ®"""
        logger.info("[UNESCO] å¼€å§‹çˆ¬å–æ•°æ®...")
        
        # æ–¹æ³•1: SDMX API
        df = self.fetch_via_sdmx(countries, start_year, end_year)
        
        if df.empty:
            # æ–¹æ³•2: Data Explorer
            df = self.fetch_via_data_explorer(countries, start_year, end_year)
        
        if df.empty:
            # æ–¹æ³•3: é¢„å®šä¹‰æŸ¥è¯¢
            df = self.fetch_predefined_data()
        
        return df


# ============================================================================
# OECD çˆ¬å–å™¨ (ä¿®å¤ç‰ˆ)
# ============================================================================

class OECDScraper: 
    """OECDæ•°æ®çˆ¬å–å™¨ - ä½¿ç”¨æ­£ç¡®çš„APIç«¯ç‚¹"""
    
    # OECD SDMX REST API (æ–°ç‰ˆ)
    SDMX_BASE = "https://sdmx.oecd.org/public/rest/data"
    
    # æ—§ç‰ˆAPI (æ›´ç¨³å®š)
    LEGACY_BASE = "https://stats.oecd.org/SDMX-JSON/data"
    
    # å…³é”®æ•°æ®é›†å’ŒæŒ‡æ ‡
    DATASETS = {
        # Main Science and Technology Indicators
        "MSTI_PUB":  {
            "name":  "Main Science and Technology Indicators",
            "name_cn": "ä¸»è¦ç§‘æŠ€æŒ‡æ ‡",
            "indicators": {
                "GERD_GDP": "R&Dæ”¯å‡ºå GDPæ¯”ä¾‹",
                "RESEARCHER":  "ç ”ç©¶äººå‘˜æ•°é‡",
                "GOVERD_GDP": "æ”¿åºœR&Dæ”¯å‡ºå GDPæ¯”ä¾‹",
                "BERD_GDP": "ä¼ä¸šR&Dæ”¯å‡ºå GDPæ¯”ä¾‹",
                "HERD_GDP": "é«˜ç­‰æ•™è‚²R&Dæ”¯å‡ºå GDPæ¯”ä¾‹",
            }
        },
        # Education at a Glance
        "EAG_NEAC": {
            "name": "Educational Attainment",
            "name_cn": "æ•™è‚²æˆå°±",
            "indicators": {
                "TRY_5T8": "é«˜ç­‰æ•™è‚²å®Œæˆç‡",
            }
        },
        # Migration
        "MIG":  {
            "name": "International Migration Database",
            "name_cn":  "å›½é™…ç§»æ°‘æ•°æ®åº“",
            "indicators": {
                "INFLOW": "ç§»æ°‘æµå…¥",
            }
        }
    }
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent":  "AI-Talent-Research/2.0",
            "Accept": "application/vnd.sdmx.data+json;charset=utf-8;version=1.0"
        })
    
    def fetch_msti(self, countries: List[str], start_year: int, end_year: int) -> pd.DataFrame:
        """è·å–Main Science and Technology Indicatorsæ•°æ®"""
        
        # åªå–OECDæˆå‘˜å›½
        oecd_countries = [c for c in countries if COUNTRIES. get(c, CountryConfig("","","","",False)).oecd_member]
        
        if not oecd_countries:
            return pd.DataFrame()
        
        # è½¬æ¢ä¸ºISO2ä»£ç 
        iso2_list = [COUNTRIES[c].iso2 for c in oecd_countries]
        country_filter = "+".join(iso2_list)
        
        all_data = []
        
        # ä½¿ç”¨æ—§ç‰ˆAPI (æ›´ç¨³å®š)
        indicators = [
            ("GERD_GDP", "R&Dæ”¯å‡ºå GDPæ¯”ä¾‹"),
            ("GERD_PPS", "R&Dæ”¯å‡º(PPP)"),
            ("RESEARCHER_FTE", "ç ”ç©¶äººå‘˜(FTE)"),
            ("RESEARCHER_PPP", "æ¯åƒåŠ³åŠ¨åŠ›ç ”ç©¶äººå‘˜"),
            ("GOVERD", "æ”¿åºœR&Dæ”¯å‡º"),
            ("BERD", "ä¼ä¸šR&Dæ”¯å‡º"),
            ("HERD", "é«˜ç­‰æ•™è‚²R&Dæ”¯å‡º"),
        ]
        
        for ind_code, ind_name in indicators:
            logger.info(f"  - {ind_name}")
            
            # å°è¯•å¤šä¸ªURLæ ¼å¼
            urls = [
                f"{self.LEGACY_BASE}/MSTI_PUB/{country_filter}.{ind_code}/all",
                f"{self.LEGACY_BASE}/MSTI_PUB/{country_filter}+.{ind_code}+/all",
                f"https://stats.oecd.org/SDMX-JSON/data/MSTI_PUB/{country_filter}..{ind_code}/all",
            ]
            
            for url in urls:
                try:
                    params = {
                        "startTime": start_year,
                        "endTime": end_year,
                        "dimensionAtObservation": "allDimensions"
                    }
                    
                    resp = self.session. get(url, params=params, timeout=30)
                    
                    if resp.status_code == 200:
                        data = resp.json()
                        df = self._parse_sdmx_json(data)
                        if not df.empty:
                            df["indicator_code"] = ind_code
                            df["indicator_name_cn"] = ind_name
                            df["source"] = "OECD"
                            all_data.append(df)
                            logger.info(f"    âœ“ è·å– {len(df)} æ¡è®°å½•")
                            break
                except Exception as e:
                    continue
            else:
                logger.warning(f"    âœ— æ— æ•°æ®")
            
            time.sleep(0.3)
        
        if all_data:
            return pd.concat(all_data, ignore_index=True)
        return pd.DataFrame()
    
    def _parse_sdmx_json(self, data: dict) -> pd.DataFrame:
        """è§£æOECD SDMX-JSONæ ¼å¼"""
        records = []
        
        try:
            # è·å–ç»“æ„ä¿¡æ¯
            structure = data.get("structure", {})
            dimensions = structure.get("dimensions", {})
            obs_dims = dimensions.get("observation", [])
            
            # åˆ›å»ºç»´åº¦å€¼æ˜ å°„
            dim_maps = {}
            for dim in obs_dims:
                dim_id = dim.get("id", "")
                values = dim.get("values", [])
                dim_maps[dim_id] = {i: v. get("id", v.get("name", str(i))) for i, v in enumerate(values)}
            
            # è§£ææ•°æ®é›†
            datasets = data.get("dataSets", [])
            for dataset in datasets:
                # å¤„ç†seriesæ ¼å¼
                series = dataset.get("series", {})
                for series_key, series_data in series.items():
                    series_dims = series_key.split(":")
                    
                    observations = series_data.get("observations", {})
                    for obs_key, obs_value in observations.items():
                        record = {
                            "value": obs_value[0] if obs_value else None,
                        }
                        
                        # è§£æseriesç»´åº¦
                        series_dim_defs = dimensions.get("series", [])
                        for i, dim_idx in enumerate(series_dims):
                            if i < len(series_dim_defs):
                                dim_id = series_dim_defs[i]. get("id", f"dim_{i}")
                                dim_values = series_dim_defs[i].get("values", [])
                                idx = int(dim_idx)
                                if idx < len(dim_values):
                                    record[dim_id] = dim_values[idx]. get("id", "")
                        
                        # è§£æobservationç»´åº¦ (é€šå¸¸æ˜¯æ—¶é—´)
                        obs_idx = int(obs_key)
                        if "TIME_PERIOD" in dim_maps and obs_idx in dim_maps. get("TIME_PERIOD", {}):
                            record["year"] = dim_maps["TIME_PERIOD"][obs_idx]
                        elif obs_dims:
                            time_dim = obs_dims[0]
                            time_values = time_dim.get("values", [])
                            if obs_idx < len(time_values):
                                record["year"] = time_values[obs_idx].get("id", "")
                        
                        records.append(record)
                
                # å¤„ç†observationsæ ¼å¼ (æ‰å¹³ç»“æ„)
                if not series: 
                    observations = dataset.get("observations", {})
                    for key, value in observations.items():
                        indices = key.split(":")
                        record = {"value": value[0] if value else None}
                        
                        for i, idx in enumerate(indices):
                            if i < len(obs_dims):
                                dim_id = obs_dims[i]. get("id", f"dim_{i}")
                                dim_values = obs_dims[i].get("values", [])
                                idx_int = int(idx)
                                if idx_int < len(dim_values):
                                    record[dim_id] = dim_values[idx_int].get("id", "")
                        
                        records.append(record)
            
            if records:
                df = pd.DataFrame(records)
                # æ ‡å‡†åŒ–åˆ—å
                col_rename = {
                    "LOCATION": "country_code",
                    "REF_AREA": "country_code", 
                    "TIME_PERIOD": "year",
                    "TIME":  "year",
                }
                df.rename(columns=col_rename, inplace=True)
                return df
                
        except Exception as e:
            logger.debug(f"SDMXè§£æé”™è¯¯: {e}")
        
        return pd.DataFrame()
    
    def fetch_alternative(self, countries: List[str], start_year: int, end_year: int) -> pd.DataFrame:
        """å¤‡ç”¨æ–¹æ³•ï¼šä½¿ç”¨OECD Data API"""
        
        logger.info("  å°è¯•OECD Data API...")
        
        oecd_countries = [c for c in countries if COUNTRIES.get(c, CountryConfig("","","","",False)).oecd_member]
        
        if not oecd_countries: 
            return pd.DataFrame()
        
        # OECD Data APIç«¯ç‚¹
        api_url = "https://data.oecd.org/api/sdmx-json/data/DP_LIVE"
        
        # å…³é”®æŒ‡æ ‡
        indicators = ["GERD", "RESEARCHER"]
        
        all_data = []
        
        for ind in indicators:
            country_filter = "+".join([COUNTRIES[c].iso2 for c in oecd_countries])
            url = f"{api_url}/.{country_filter}.{ind}../OECD"
            
            try:
                params = {"startTime": start_year, "endTime": end_year}
                resp = self.session.get(url, params=params, timeout=30)
                
                if resp.status_code == 200:
                    df = self._parse_sdmx_json(resp.json())
                    if not df. empty:
                        df["indicator"] = ind
                        all_data.append(df)
                        logger.info(f"    âœ“ {ind}:  è·å– {len(df)} æ¡è®°å½•")
            except Exception as e:
                logger.debug(f"    {ind}: {e}")
            
            time. sleep(0.3)
        
        if all_data: 
            return pd.concat(all_data, ignore_index=True)
        return pd.DataFrame()
    
    def fetch_all(self, countries: List[str], start_year: int, end_year: int) -> pd.DataFrame:
        """è·å–æ‰€æœ‰OECDæ•°æ®"""
        logger.info("[OECD] å¼€å§‹çˆ¬å–æ•°æ®...")
        
        oecd_countries = [c for c in countries if COUNTRIES.get(c, CountryConfig("","","","",False)).oecd_member]
        logger.info(f"  OECDæˆå‘˜å›½: {', '.join(oecd_countries)}")
        
        # æ–¹æ³•1: MSTIæ•°æ®é›†
        df = self.fetch_msti(countries, start_year, end_year)
        
        if df.empty:
            # æ–¹æ³•2: å¤‡ç”¨API
            df = self.fetch_alternative(countries, start_year, end_year)
        
        return df


# ============================================================================
# è¡¥å……æ•°æ®æºï¼šæ‰‹åŠ¨æ•´ç†çš„å…³é”®æ•°æ®
# ============================================================================

def get_supplementary_data() -> pd.DataFrame:
    """
    è·å–è¡¥å……æ•°æ®
    ç”±äºéƒ¨åˆ†APIè®¿é—®å—é™ï¼Œæä¾›æ‰‹åŠ¨æ•´ç†çš„å…³é”®æ•°æ®ä½œä¸ºè¡¥å……
    æ•°æ®æ¥æºï¼šUIS Data Browser, OECD. Stat (2023å¹´æœ€æ–°å¯ç”¨æ•°æ®)
    """
    
    # ç ”ç©¶äººå‘˜æ•°æ® (æ¯ç™¾ä¸‡äººï¼ŒFTE) - æ¥æº:  UNESCO UIS 2022-2023
    researchers_data = [
        ("USA", 2021, 4412, "æ¯ç™¾ä¸‡äººç ”ç©¶äººå‘˜(FTE)"),
        ("CHN", 2021, 1585, "æ¯ç™¾ä¸‡äººç ”ç©¶äººå‘˜(FTE)"),
        ("GBR", 2021, 4603, "æ¯ç™¾ä¸‡äººç ”ç©¶äººå‘˜(FTE)"),
        ("DEU", 2021, 5234, "æ¯ç™¾ä¸‡äººç ”ç©¶äººå‘˜(FTE)"),
        ("KOR", 2021, 8714, "æ¯ç™¾ä¸‡äººç ”ç©¶äººå‘˜(FTE)"),
        ("JPN", 2021, 5331, "æ¯ç™¾ä¸‡äººç ”ç©¶äººå‘˜(FTE)"),
        ("FRA", 2021, 4715, "æ¯ç™¾ä¸‡äººç ”ç©¶äººå‘˜(FTE)"),
        ("CAN", 2021, 4876, "æ¯ç™¾ä¸‡äººç ”ç©¶äººå‘˜(FTE)"),
        ("ARE", 2020, 1350, "æ¯ç™¾ä¸‡äººç ”ç©¶äººå‘˜(FTE)"),
        ("IND", 2020, 253, "æ¯ç™¾ä¸‡äººç ”ç©¶äººå‘˜(FTE)"),
    ]
    
    # R&Dæ”¯å‡ºå GDPæ¯”ä¾‹ (%) - æ¥æº: UNESCO UIS / OECD 2022-2023
    rd_gdp_data = [
        ("USA", 2021, 3.46, "R&Dæ”¯å‡ºå GDPæ¯”ä¾‹(%)"),
        ("CHN", 2021, 2.43, "R&Dæ”¯å‡ºå GDPæ¯”ä¾‹(%)"),
        ("GBR", 2021, 2.93, "R&Dæ”¯å‡ºå GDPæ¯”ä¾‹(%)"),
        ("DEU", 2021, 3.13, "R&Dæ”¯å‡ºå GDPæ¯”ä¾‹(%)"),
        ("KOR", 2021, 4.93, "R&Dæ”¯å‡ºå GDPæ¯”ä¾‹(%)"),
        ("JPN", 2021, 3.30, "R&Dæ”¯å‡ºå GDPæ¯”ä¾‹(%)"),
        ("FRA", 2021, 2.21, "R&Dæ”¯å‡ºå GDPæ¯”ä¾‹(%)"),
        ("CAN", 2021, 1.69, "R&Dæ”¯å‡ºå GDPæ¯”ä¾‹(%)"),
        ("ARE", 2019, 1.30, "R&Dæ”¯å‡ºå GDPæ¯”ä¾‹(%)"),
        ("IND", 2020, 0.65, "R&Dæ”¯å‡ºå GDPæ¯”ä¾‹(%)"),
    ]
    
    # STEMæ¯•ä¸šç”Ÿå æ¯” (%) - æ¥æº:  OECD Education at a Glance
    stem_data = [
        ("USA", 2021, 21, "STEMæ¯•ä¸šç”Ÿå æ¯”(%)"),
        ("CHN", 2020, 35, "STEMæ¯•ä¸šç”Ÿå æ¯”(%)"),
        ("GBR", 2021, 27, "STEMæ¯•ä¸šç”Ÿå æ¯”(%)"),
        ("DEU", 2021, 35, "STEMæ¯•ä¸šç”Ÿå æ¯”(%)"),
        ("KOR", 2021, 32, "STEMæ¯•ä¸šç”Ÿå æ¯”(%)"),
        ("JPN", 2021, 22, "STEMæ¯•ä¸šç”Ÿå æ¯”(%)"),
        ("FRA", 2021, 27, "STEMæ¯•ä¸šç”Ÿå æ¯”(%)"),
        ("CAN", 2021, 24, "STEMæ¯•ä¸šç”Ÿå æ¯”(%)"),
        ("IND", 2020, 32, "STEMæ¯•ä¸šç”Ÿå æ¯”(%)"),
    ]
    
    # é«˜ç­‰æ•™è‚²å®Œæˆç‡ 25-34å² (%) - æ¥æº: OECD
    tertiary_data = [
        ("USA", 2022, 50, "25-34å²é«˜ç­‰æ•™è‚²å®Œæˆç‡(%)"),
        ("GBR", 2022, 57, "25-34å²é«˜ç­‰æ•™è‚²å®Œæˆç‡(%)"),
        ("DEU", 2022, 37, "25-34å²é«˜ç­‰æ•™è‚²å®Œæˆç‡(%)"),
        ("KOR", 2022, 69, "25-34å²é«˜ç­‰æ•™è‚²å®Œæˆç‡(%)"),
        ("JPN", 2022, 66, "25-34å²é«˜ç­‰æ•™è‚²å®Œæˆç‡(%)"),
        ("FRA", 2022, 51, "25-34å²é«˜ç­‰æ•™è‚²å®Œæˆç‡(%)"),
        ("CAN", 2022, 66, "25-34å²é«˜ç­‰æ•™è‚²å®Œæˆç‡(%)"),
    ]
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    all_records = []
    
    for country, year, value, indicator in researchers_data + rd_gdp_data + stem_data + tertiary_data: 
        all_records.append({
            "country_code": country,
            "country_name": COUNTRIES[country]. name_en,
            "country_cn": COUNTRIES[country].name_cn,
            "year":  year,
            "value": value,
            "indicator_name_cn": indicator,
            "source": "Supplementary (UNESCO/OECD)",
            "category": "supplementary"
        })
    
    return pd.DataFrame(all_records)


# ============================================================================
# ä¸»çˆ¬å–å™¨
# ============================================================================

class AITalentScraperV2:
    """AIäººæ‰æ•°æ®ç»¼åˆçˆ¬å–å™¨ V2"""
    
    def __init__(self, output_dir: str = "ai_talent_data_v2"):
        self.output_dir = output_dir
        
        # åˆå§‹åŒ–å„çˆ¬å–å™¨
        self.world_bank = WorldBankScraper()
        self.unesco = UNESCOScraper()
        self.oecd = OECDScraper()
        
        # åˆ›å»ºç›®å½•
        for subdir in ["raw", "processed", "reports"]:
            os.makedirs(f"{output_dir}/{subdir}", exist_ok=True)
        
        logger.info(f"çˆ¬å–å™¨åˆå§‹åŒ–å®Œæˆï¼Œè¾“å‡ºç›®å½•: {output_dir}")
    
    def run(self, start_year: int = 2015, end_year: int = 2024) -> pd.DataFrame:
        """æ‰§è¡Œå®Œæ•´çˆ¬å–"""
        
        logger.info("\n" + "=" * 70)
        logger.info("AIäººæ‰æ•°æ®çˆ¬å–ç¨‹åº V2 å¯åŠ¨")
        logger.info("=" * 70)
        
        countries = list(COUNTRIES.keys())
        all_data = []
        
        # 1. World Bankæ•°æ®
        logger.info("\n" + "-" * 50)
        logger.info("[1/4] çˆ¬å–World Bankæ•°æ®")
        logger.info("-" * 50)
        wb_df = self.world_bank.fetch_all(countries, start_year, end_year)
        if not wb_df.empty:
            all_data.append(wb_df)
            self._save_raw(wb_df, "world_bank")
        
        # 2. UNESCOæ•°æ®
        logger.info("\n" + "-" * 50)
        logger.info("[2/4] çˆ¬å–UNESCOæ•°æ®")
        logger.info("-" * 50)
        unesco_df = self.unesco.fetch_all(countries, start_year, end_year)
        if not unesco_df.empty:
            all_data.append(unesco_df)
            self._save_raw(unesco_df, "unesco")
        
        # 3. OECDæ•°æ®
        logger.info("\n" + "-" * 50)
        logger.info("[3/4] çˆ¬å–OECDæ•°æ®")
        logger.info("-" * 50)
        oecd_df = self.oecd.fetch_all(countries, start_year, end_year)
        if not oecd_df.empty:
            all_data.append(oecd_df)
            self._save_raw(oecd_df, "oecd")
        
        # 4. è¡¥å……æ•°æ®
        logger.info("\n" + "-" * 50)
        logger.info("[4/4] æ·»åŠ è¡¥å……æ•°æ®")
        logger.info("-" * 50)
        supp_df = get_supplementary_data()
        all_data.append(supp_df)
        logger.info(f"  âœ“ æ·»åŠ  {len(supp_df)} æ¡è¡¥å……æ•°æ®")
        self._save_raw(supp_df, "supplementary")
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        if all_data:
            combined = pd.concat(all_data, ignore_index=True)
            
            # æ·»åŠ /è¡¥å…¨å›½å®¶ä¸­æ–‡åï¼ˆå³ä½¿åˆ—å·²å­˜åœ¨ï¼Œä¹Ÿè¦æŠŠ NaN å¡«ä¸Šï¼‰
            if "country_code" in combined.columns:
                if "country_cn" not in combined.columns:
                    combined["country_cn"] = pd.NA
                combined["country_cn"] = combined["country_cn"].fillna(
                    combined["country_code"].map(
                        lambda x: COUNTRIES.get(x, CountryConfig("","","","",False)).name_cn
                    )
                )
            
            self._save_processed(combined)
            self._generate_report(combined)
            
            return combined
        
        return pd.DataFrame()
    
    def _save_raw(self, df: pd.DataFrame, name: str):
        """ä¿å­˜åŸå§‹æ•°æ®"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filepath = f"{self.output_dir}/raw/{name}_{timestamp}.csv"
        df.to_csv(filepath, index=False, encoding="utf-8-sig")
        logger.info(f"  ä¿å­˜: {filepath}")
    
    def _save_processed(self, df: pd.DataFrame):
        """ä¿å­˜å¤„ç†åçš„æ•°æ®"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # CSV
        csv_path = f"{self.output_dir}/processed/combined_data_{timestamp}.csv"
        df.to_csv(csv_path, index=False, encoding="utf-8-sig")
        
        # Excel
        try:
            excel_path = f"{self.output_dir}/processed/combined_data_{timestamp}.xlsx"
            df.to_excel(excel_path, index=False)
        except Exception as e:
            logger.warning(f"Excelä¿å­˜å¤±è´¥: {e}")
        
        logger.info(f"\næ•°æ®å·²ä¿å­˜åˆ° {self.output_dir}/processed/")
    
    def _generate_report(self, df: pd. DataFrame):
        """ç”ŸæˆæŠ¥å‘Š"""
        
        report = [
            "=" * 70,
            "AIäººæ‰æ•°æ®çˆ¬å–æŠ¥å‘Š V2",
            "=" * 70,
            f"\nç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"æ€»è®°å½•æ•°: {len(df)}",
            "",
            "-" * 50,
            "æ•°æ®æºç»Ÿè®¡",
            "-" * 50,
        ]
        
        if "source" in df.columns:
            for source in df["source"].unique():
                count = len(df[df["source"] == source])
                valid = df[df["source"] == source]["value"].notna().sum()
                report. append(f"  {source}: {count} æ¡ ({valid} æ¡æœ‰æ•ˆ)")
        
        report.extend([
            "",
            "-" * 50,
            "å„å›½æ•°æ®è¦†ç›–",
            "-" * 50,
        ])
        
        if "country_code" in df. columns:
            country_stats = df.groupby("country_code").agg({
                "value":  lambda x: x.notna().sum()
            }).sort_values("value", ascending=False)
            
            for country, row in country_stats.iterrows():
                info = COUNTRIES.get(country)
                if info:
                    tag = "[OECD]" if info. oecd_member else "[éOECD]"
                    report.append(f"  {info.name_cn} ({country}) {tag}:  {int(row['value'])} æ¡æœ‰æ•ˆæ•°æ®")
        
        report. extend([
            "",
            "-" * 50,
            "æŒ‡æ ‡è¦†ç›–",
            "-" * 50,
        ])
        
        if "indicator_name_cn" in df.columns:
            for ind in df["indicator_name_cn"]. unique():
                count = df[df["indicator_name_cn"] == ind]["value"].notna().sum()
                report.append(f"  â€¢ {ind}: {count} æ¡æœ‰æ•ˆæ•°æ®")
        
        report_text = "\n".join(report)
        print("\n" + report_text)
        
        # ä¿å­˜æŠ¥å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        with open(f"{self.output_dir}/reports/report_{timestamp}.txt", "w", encoding="utf-8") as f:
            f.write(report_text)


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘              AIäººæ‰æ•°æ®çˆ¬å–å™¨ V2 (ä¿®å¤ç‰ˆ)                              â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘  æ•°æ®æº: World Bank + UNESCO + OECD + è¡¥å……æ•°æ®                        â•‘
    â•‘  ç›®æ ‡å›½å®¶: ç¾å›½ã€ä¸­å›½ã€è‹±å›½ã€å¾·å›½ã€éŸ©å›½ã€æ—¥æœ¬ã€æ³•å›½ã€åŠ æ‹¿å¤§ã€é˜¿è”é…‹ã€å°åº¦ â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    scraper = AITalentScraperV2(output_dir="ai_talent_data_v2")
    df = scraper.run(start_year=2015, end_year=2024)
    
    if not df.empty:
        print("\n" + "=" * 70)
        print("æ•°æ®é¢„è§ˆ (å‰20æ¡)")
        print("=" * 70)
        
        # æ˜¾ç¤ºæœ‰æ•ˆæ•°æ®é¢„è§ˆ
        valid_df = df[df["value"].notna()].head(20)
        if "country_cn" in valid_df.columns and "indicator_name_cn" in valid_df.columns:
            display_cols = ["country_cn", "year", "indicator_name_cn", "value", "source"]
            display_cols = [c for c in display_cols if c in valid_df.columns]
            print(valid_df[display_cols]. to_string(index=False))


if __name__ == "__main__":
    main()