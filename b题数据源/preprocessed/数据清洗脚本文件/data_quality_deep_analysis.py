# -*- coding: utf-8 -*-
"""
åæ•°æ¯ Bé¢˜ - æ•°æ®è´¨é‡æ·±åº¦åˆ†ææŠ¥å‘Š
Deep Data Quality Analysis for O-Award Level

æœ¬è„šæœ¬é’ˆå¯¹Oå¥–ç­–ç•¥çš„6å¤§è¦æ±‚è¿›è¡Œå…¨é¢æ£€æŸ¥ï¼š
1. æ—¶é—´ç»´åº¦å¯¹é½ä¸æ’è¡¥
2. å›½å®¶å®ä½“å¼‚è´¨æ€§å¤„ç†
3. è´§å¸ä¸é€šèƒ€æ ‡å‡†åŒ–
4. å¼‚å¸¸å€¼ä¸é•¿å°¾åˆ†å¸ƒå¤„ç†
5. æ»åæ•ˆåº”ç‰¹å¾å·¥ç¨‹
6. ç»Ÿä¸€é¢—ç²’åº¦
"""

import pandas as pd
import numpy as np
import os
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ===================== é…ç½® =====================
DATA_DIR = Path(r"d:\åæ•°æ¯\bé¢˜æ•°æ®æº")
OUTPUT_DIR = DATA_DIR / "preprocessed"
OUTPUT_DIR.mkdir(exist_ok=True)

# ç›®æ ‡å›½å®¶ï¼ˆå«ä¸­è‹±æ–‡æ˜ å°„ï¼‰
TARGET_COUNTRIES = {
    'USA': 'ç¾å›½', 'CHN': 'ä¸­å›½', 'GBR': 'è‹±å›½', 'DEU': 'å¾·å›½', 
    'FRA': 'æ³•å›½', 'CAN': 'åŠ æ‹¿å¤§', 'JPN': 'æ—¥æœ¬', 'KOR': 'éŸ©å›½', 
    'ARE': 'é˜¿è”é…‹', 'IND': 'å°åº¦'
}
TARGET_YEARS = list(range(2016, 2026))

# å›½å®¶ä»£ç åˆ«åæ˜ å°„ï¼ˆå¤„ç†ä¸åŒæ•°æ®æºçš„å‘½åå·®å¼‚ï¼‰
COUNTRY_ALIASES = {
    'United States': 'USA', 'United States of America': 'USA', 'US': 'USA',
    'China': 'CHN', "People's Republic of China": 'CHN', 'PRC': 'CHN',
    'United Kingdom': 'GBR', 'UK': 'GBR', 'Great Britain': 'GBR',
    'Germany': 'DEU', 'Deutschland': 'DEU',
    'France': 'FRA',
    'Canada': 'CAN',
    'Japan': 'JPN',
    'South Korea': 'KOR', 'Korea, Republic of': 'KOR', 'Republic of Korea': 'KOR', 'Korea': 'KOR',
    'United Arab Emirates': 'ARE', 'UAE': 'ARE',
    'India': 'IND'
}

# æ•°æ®é›†åˆ†ç±»
DATASET_CATEGORIES = {
    'å‡ºç‰ˆç‰©æ•°æ®': [
        'å„å›½å†å¹´äººå·¥æ™ºèƒ½å‡ºç‰ˆç‰©æ•°é‡.csv',
        'å„å›½å†å¹´äººå·¥æ™ºèƒ½å‡ºç‰ˆç‰©ç™¾åˆ†æ¯”.csv',
        'å„å›½å†å¹´äººå·¥æ™ºèƒ½é«˜å½±å“åŠ›å‡ºç‰ˆç‰©æ•°é‡.csv',
        'å„å›½å†å¹´äººå·¥æ™ºèƒ½é«˜å½±å“åŠ›å‡ºç‰ˆç‰©ç™¾åˆ†æ¯”.csv',
        'å„å›½å†å¹´äººå·¥æ™ºèƒ½Articleæ•°é‡.csv',
        'å„å›½å†å¹´äººå·¥æ™ºèƒ½Bookæ•°é‡.csv',
        'å„å›½å†å¹´äººå·¥æ™ºèƒ½Datasetæ•°é‡.csv',
        'å„å›½å†å¹´äººå·¥æ™ºèƒ½Dissertationæ•°é‡.csv',
    ],
    'æŠ•èµ„æ•°æ®': [
        'å„å›½å†å¹´åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸæ‰€æœ‰è¡Œä¸šçš„é£é™©æŠ•èµ„ï¼ˆç™¾ä¸‡ç¾å…ƒï¼‰.csv',
        'å„å›½å†å¹´å¯¹ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åˆåˆ›ä¼ä¸šçš„é£é™©æŠ•èµ„ï¼ˆç™¾ä¸‡ç¾å…ƒï¼‰.csv',
        'å„å›½å†å¹´å¯¹AIè®¡ç®—åˆåˆ›ä¼ä¸šçš„é£é™©æŠ•èµ„ï¼ˆç™¾ä¸‡ç¾å…ƒï¼‰.csv',
        'å„å›½å†å¹´åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå¯¹å¤–çš„é£é™©æŠ•èµ„ï¼ˆå›½å®¶é—´ï¼‰ï¼ˆç™¾ä¸‡ç¾å…ƒï¼‰.csv',
    ],
    'åŸºç¡€è®¾æ–½æ•°æ®': [
        'å„å›½å†å¹´ç”µèƒ½ç”Ÿäº§æƒ…å†µ.csv',
    ],
    'äººæ‰ä¸æ’åæ•°æ®': [
        f'{year}_AIé¢†åŸŸå¤§å­¦è®¡ç®—æœºæ’å.csv' for year in range(2000, 2026)
    ],
    'GitHubæ•°æ®': [
        'å„å›½å†å¹´åœ¨GitHubä¸Šçš„é¡¹ç›®æ•°.csv',
    ]
}


class DataQualityAnalyzer:
    """æ•°æ®è´¨é‡æ·±åº¦åˆ†æå™¨"""
    
    def __init__(self):
        self.issues = []
        self.warnings = []
        self.recommendations = []
        self.dataset_reports = {}
    
    def analyze_all_datasets(self):
        """åˆ†ææ‰€æœ‰æ•°æ®é›†"""
        print("=" * 100)
        print("ğŸ” åæ•°æ¯ Bé¢˜ - æ•°æ®è´¨é‡æ·±åº¦åˆ†ææŠ¥å‘Š")
        print("=" * 100)
        print(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"æ•°æ®ç›®å½•: {DATA_DIR}")
        print()
        
        # è·å–æ‰€æœ‰CSVæ–‡ä»¶
        all_csv_files = list(DATA_DIR.glob("*.csv"))
        print(f"ğŸ“‚ å‘ç° {len(all_csv_files)} ä¸ªCSVæ–‡ä»¶")
        print()
        
        # 1. æ—¶é—´ç»´åº¦åˆ†æ
        self.analyze_temporal_coverage(all_csv_files)
        
        # 2. å›½å®¶è¦†ç›–åº¦åˆ†æ
        self.analyze_country_coverage(all_csv_files)
        
        # 3. 2025å¹´æ•°æ®å¯ç”¨æ€§åˆ†æï¼ˆå…³é”®ï¼ï¼‰
        self.analyze_2025_data_availability(all_csv_files)
        
        # 4. é˜¿è”é…‹ç­‰æ–°å…´å›½å®¶æ•°æ®åˆ†æ
        self.analyze_emerging_countries_data()
        
        # 5. æ•°å€¼åˆ†å¸ƒåˆ†æï¼ˆé•¿å°¾æ£€æµ‹ï¼‰
        self.analyze_value_distributions()
        
        # 6. è´§å¸æ•°æ®åˆ†æ
        self.analyze_currency_data()
        
        # 7. æ•°æ®é¢—ç²’åº¦åˆ†æ
        self.analyze_granularity()
        
        # 8. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        self.generate_comprehensive_report()
        
        return self.dataset_reports
    
    def analyze_temporal_coverage(self, csv_files):
        """åˆ†ææ—¶é—´ç»´åº¦è¦†ç›–åº¦"""
        print("\n" + "=" * 80)
        print("ğŸ“… 1. æ—¶é—´ç»´åº¦è¦†ç›–åº¦åˆ†æ")
        print("=" * 80)
        
        temporal_summary = []
        
        for file_path in csv_files:
            try:
                df = pd.read_csv(file_path, nrows=1000)  # è¯»å–å‰1000è¡Œå¿«é€Ÿåˆ†æ
                
                # è¯†åˆ«å¹´ä»½åˆ—
                year_col = None
                for col in df.columns:
                    col_lower = col.lower()
                    if col_lower in ['year', 'date', 'quarter', 'å¹´ä»½']:
                        year_col = col
                        break
                
                if year_col is None:
                    # æ£€æŸ¥æ˜¯å¦æœ‰å¹´ä»½åœ¨åˆ—åä¸­
                    year_cols = [c for c in df.columns if str(c).isdigit() and 2000 <= int(c) <= 2030]
                    if year_cols:
                        continue  # å®½æ ¼å¼æ•°æ®
                    continue
                
                # åˆ†æå¹´ä»½èŒƒå›´
                years = df[year_col].dropna().unique()
                
                # å°è¯•æå–å¹´ä»½ï¼ˆå¤„ç†å­£åº¦æ•°æ®å¦‚ "2023 Q1"ï¼‰
                def extract_year(val):
                    try:
                        if isinstance(val, (int, float)):
                            return int(val)
                        val_str = str(val)
                        for i in range(2000, 2030):
                            if str(i) in val_str:
                                return i
                        return None
                    except:
                        return None
                
                extracted_years = [extract_year(y) for y in years]
                extracted_years = [y for y in extracted_years if y is not None]
                
                if extracted_years:
                    min_year = min(extracted_years)
                    max_year = max(extracted_years)
                    
                    # æ£€æŸ¥2016-2025è¦†ç›–åº¦
                    target_coverage = [y for y in TARGET_YEARS if y in extracted_years]
                    missing_years = [y for y in TARGET_YEARS if y not in extracted_years]
                    
                    temporal_summary.append({
                        'æ–‡ä»¶å': file_path.name,
                        'æœ€æ—©å¹´ä»½': min_year,
                        'æœ€æ–°å¹´ä»½': max_year,
                        'è¦†ç›–2016-2025': f"{len(target_coverage)}/10",
                        'ç¼ºå¤±å¹´ä»½': missing_years if missing_years else 'æ— '
                    })
                    
                    if 2025 not in extracted_years:
                        self.warnings.append(f"âš ï¸ {file_path.name}: ç¼ºå¤±2025å¹´æ•°æ®")
                    if 2024 not in extracted_years:
                        self.warnings.append(f"âš ï¸ {file_path.name}: ç¼ºå¤±2024å¹´æ•°æ®")
                        
            except Exception as e:
                continue
        
        if temporal_summary:
            summary_df = pd.DataFrame(temporal_summary)
            print(summary_df.to_string(index=False))
            
            # ç»Ÿè®¡å°¾éƒ¨ç¼ºå¤±æƒ…å†µ
            print("\nğŸ“Š å°¾éƒ¨ç¼ºå¤±ç»Ÿè®¡ï¼ˆéœ€è¦å¤–æ¨ï¼‰:")
            missing_2025 = len([s for s in temporal_summary if 2025 in (s['ç¼ºå¤±å¹´ä»½'] if isinstance(s['ç¼ºå¤±å¹´ä»½'], list) else [])])
            missing_2024 = len([s for s in temporal_summary if 2024 in (s['ç¼ºå¤±å¹´ä»½'] if isinstance(s['ç¼ºå¤±å¹´ä»½'], list) else [])])
            print(f"  - ç¼ºå¤±2025å¹´: {missing_2025}/{len(temporal_summary)} ä¸ªæ•°æ®é›†")
            print(f"  - ç¼ºå¤±2024å¹´: {missing_2024}/{len(temporal_summary)} ä¸ªæ•°æ®é›†")
            
            if missing_2025 > 0:
                self.recommendations.append("ğŸ’¡ å»ºè®®: å¯¹ç¼ºå¤±2025å¹´æ•°æ®çš„æŒ‡æ ‡ä½¿ç”¨Holt-Wintersæˆ–ARIMAè¿›è¡ŒçŸ­æœŸå¤–æ¨")
    
    def analyze_country_coverage(self, csv_files):
        """åˆ†æå›½å®¶è¦†ç›–åº¦"""
        print("\n" + "=" * 80)
        print("ğŸŒ 2. ç›®æ ‡å›½å®¶è¦†ç›–åº¦åˆ†æ")
        print("=" * 80)
        
        country_coverage = {}
        
        for file_path in csv_files:
            try:
                df = pd.read_csv(file_path, nrows=5000)
                
                # è¯†åˆ«å›½å®¶åˆ—
                country_col = None
                for col in df.columns:
                    col_lower = col.lower()
                    if 'country' in col_lower or 'territory' in col_lower or 'å›½å®¶' in col_lower:
                        country_col = col
                        break
                
                if country_col is None:
                    continue
                
                countries = df[country_col].dropna().unique()
                
                # æ ‡å‡†åŒ–å›½å®¶åç§°
                standardized = set()
                for c in countries:
                    c_str = str(c).strip()
                    if c_str in TARGET_COUNTRIES:
                        standardized.add(c_str)
                    elif c_str in COUNTRY_ALIASES:
                        standardized.add(COUNTRY_ALIASES[c_str])
                
                target_found = [c for c in TARGET_COUNTRIES if c in standardized]
                target_missing = [c for c in TARGET_COUNTRIES if c not in standardized]
                
                country_coverage[file_path.name] = {
                    'æ‰¾åˆ°': target_found,
                    'ç¼ºå¤±': target_missing,
                    'è¦†ç›–ç‡': f"{len(target_found)}/10"
                }
                
                if target_missing:
                    self.warnings.append(f"âš ï¸ {file_path.name}: ç¼ºå¤±å›½å®¶ {target_missing}")
                    
            except Exception as e:
                continue
        
        # æ‰“å°æ±‡æ€»
        print(f"\næ£€æŸ¥äº† {len(country_coverage)} ä¸ªå«å›½å®¶ä¿¡æ¯çš„æ•°æ®é›†")
        
        # æ‰¾å‡ºæœ€å¸¸ç¼ºå¤±çš„å›½å®¶
        missing_counts = {}
        for info in country_coverage.values():
            for country in info['ç¼ºå¤±']:
                missing_counts[country] = missing_counts.get(country, 0) + 1
        
        if missing_counts:
            print("\nğŸ“Š å„å›½æ•°æ®ç¼ºå¤±é¢‘ç‡:")
            for country, count in sorted(missing_counts.items(), key=lambda x: -x[1]):
                print(f"  - {country} ({TARGET_COUNTRIES[country]}): åœ¨ {count} ä¸ªæ•°æ®é›†ä¸­ç¼ºå¤±")
    
    def analyze_2025_data_availability(self, csv_files):
        """è¯¦ç»†åˆ†æ2025å¹´æ•°æ®å¯ç”¨æ€§"""
        print("\n" + "=" * 80)
        print("ğŸ¯ 3. 2025å¹´æ•°æ®å¯ç”¨æ€§è¯¦ç»†åˆ†æï¼ˆé¢˜ç›®å…³é”®è¦æ±‚ï¼‰")
        print("=" * 80)
        
        has_2025 = []
        missing_2025 = []
        
        for file_path in csv_files:
            try:
                df = pd.read_csv(file_path)
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«2025å¹´æ•°æ®
                has_2025_data = False
                
                # æ–¹æ³•1: æ£€æŸ¥å¹´ä»½åˆ—
                for col in df.columns:
                    col_lower = str(col).lower()
                    if col_lower in ['year', 'date', 'quarter']:
                        values = df[col].astype(str)
                        if any('2025' in str(v) for v in values):
                            has_2025_data = True
                            break
                
                # æ–¹æ³•2: æ£€æŸ¥åˆ—å
                if not has_2025_data:
                    if any('2025' in str(c) for c in df.columns):
                        has_2025_data = True
                
                # æ–¹æ³•3: æ£€æŸ¥æ•°æ®å†…å®¹
                if not has_2025_data:
                    df_str = df.astype(str)
                    for col in df.columns:
                        if any('2025' in str(v) for v in df[col]):
                            has_2025_data = True
                            break
                
                if has_2025_data:
                    has_2025.append(file_path.name)
                else:
                    missing_2025.append(file_path.name)
                    
            except Exception as e:
                continue
        
        print(f"\nâœ… åŒ…å«2025å¹´æ•°æ®çš„æ–‡ä»¶ ({len(has_2025)}):")
        for f in has_2025[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"  - {f}")
        if len(has_2025) > 10:
            print(f"  ... åŠå…¶ä»– {len(has_2025) - 10} ä¸ªæ–‡ä»¶")
        
        print(f"\nâŒ ç¼ºå¤±2025å¹´æ•°æ®çš„æ–‡ä»¶ ({len(missing_2025)}):")
        for f in missing_2025:
            print(f"  - {f}")
        
        if missing_2025:
            self.issues.append(f"ğŸš¨ å…³é”®é—®é¢˜: {len(missing_2025)} ä¸ªæ•°æ®é›†ç¼ºå¤±2025å¹´æ•°æ®ï¼Œéœ€è¦è¿›è¡Œå¤–æ¨é¢„æµ‹")
            self.recommendations.append("ğŸ’¡ å¯¹äºç¼ºå¤±2025å¹´çš„æ•°æ®ï¼Œå»ºè®®ä½¿ç”¨Holt-WintersæŒ‡æ•°å¹³æ»‘è¿›è¡ŒçŸ­æœŸå¤–æ¨")
    
    def analyze_emerging_countries_data(self):
        """åˆ†æé˜¿è”é…‹ã€å°åº¦ç­‰æ–°å…´å›½å®¶æ•°æ®è´¨é‡"""
        print("\n" + "=" * 80)
        print("ğŸŒŸ 4. æ–°å…´å›½å®¶æ•°æ®å¼‚è´¨æ€§åˆ†æï¼ˆé˜¿è”é…‹AREã€å°åº¦INDï¼‰")
        print("=" * 80)
        
        emerging_countries = ['ARE', 'IND']
        
        # è¯»å–ä¸»è¦æ•°æ®é›†è¿›è¡Œåˆ†æ
        key_datasets = [
            ('å„å›½å†å¹´äººå·¥æ™ºèƒ½å‡ºç‰ˆç‰©æ•°é‡.csv', 'publications', 'Country/territory'),
            ('å„å›½å†å¹´åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸæ‰€æœ‰è¡Œä¸šçš„é£é™©æŠ•èµ„ï¼ˆç™¾ä¸‡ç¾å…ƒï¼‰.csv', 'Sum_of_deals', 'Country'),
            ('å„å›½å†å¹´åœ¨GitHubä¸Šçš„é¡¹ç›®æ•°.csv', 'AI_projects_fractional_count_based_on_contributions', 'Country'),
        ]
        
        for filename, value_col, country_col in key_datasets:
            file_path = DATA_DIR / filename
            if not file_path.exists():
                continue
            
            try:
                df = pd.read_csv(file_path)
                print(f"\nğŸ“Š {filename}")
                print("-" * 60)
                
                for country in emerging_countries:
                    country_data = df[df[country_col] == country]
                    
                    if len(country_data) == 0:
                        print(f"  âš ï¸ {country}: æ— æ•°æ®")
                        self.issues.append(f"ğŸš¨ {filename} ä¸­ {country} æ— æ•°æ®")
                        continue
                    
                    if value_col in country_data.columns:
                        values = country_data[value_col].dropna()
                        zero_count = (values == 0).sum()
                        missing_count = country_data[value_col].isna().sum()
                        
                        print(f"  {country} ({TARGET_COUNTRIES[country]}):")
                        print(f"    - è®°å½•æ•°: {len(country_data)}")
                        print(f"    - ç¼ºå¤±å€¼: {missing_count} ({missing_count/len(country_data)*100:.1f}%)")
                        print(f"    - é›¶å€¼: {zero_count} ({zero_count/len(values)*100:.1f}%)")
                        
                        if len(values) > 0:
                            print(f"    - æ•°å€¼èŒƒå›´: [{values.min():.2f}, {values.max():.2f}]")
                            print(f"    - å‡å€¼: {values.mean():.2f}")
                        
                        if zero_count > len(values) * 0.5:
                            self.warnings.append(f"âš ï¸ {filename} ä¸­ {country} è¶…è¿‡50%ä¸ºé›¶å€¼ï¼Œéœ€ç‰¹æ®Šå¤„ç†")
                            
            except Exception as e:
                print(f"  âŒ è¯»å–å¤±è´¥: {e}")
        
        # å¯¹æ¯”ç¬¬ä¸€æ¢¯é˜Ÿå›½å®¶
        print("\nğŸ“Š ä¸ç¬¬ä¸€æ¢¯é˜Ÿï¼ˆç¾ä¸­ï¼‰çš„æ•°æ®å¯¹æ¯”:")
        print("-" * 60)
        
        try:
            pub_df = pd.read_csv(DATA_DIR / 'å„å›½å†å¹´äººå·¥æ™ºèƒ½å‡ºç‰ˆç‰©æ•°é‡.csv')
            pub_2023 = pub_df[pub_df['year'] == 2023]
            
            comparison = []
            for country in ['USA', 'CHN', 'ARE', 'IND']:
                country_data = pub_2023[pub_2023['Country/territory'] == country]
                if len(country_data) > 0:
                    value = country_data['publications'].values[0]
                    comparison.append({'å›½å®¶': f"{country} ({TARGET_COUNTRIES[country]})", '2023å¹´AIå‡ºç‰ˆç‰©': value})
            
            if comparison:
                comp_df = pd.DataFrame(comparison)
                print(comp_df.to_string(index=False))
                
                # è®¡ç®—å·®è·å€æ•°
                usa_val = [c['2023å¹´AIå‡ºç‰ˆç‰©'] for c in comparison if c['å›½å®¶'].startswith('USA')]
                are_val = [c['2023å¹´AIå‡ºç‰ˆç‰©'] for c in comparison if c['å›½å®¶'].startswith('ARE')]
                if usa_val and are_val and are_val[0] > 0:
                    ratio = usa_val[0] / are_val[0]
                    print(f"\n  ğŸ“ˆ ç¾å›½æ˜¯é˜¿è”é…‹çš„ {ratio:.1f} å€ â†’ å¼ºçƒˆå»ºè®®å¯¹æ•°å˜æ¢")
                    self.recommendations.append(f"ğŸ’¡ å‡ºç‰ˆç‰©æ•°æ®: ç¾å›½æ˜¯é˜¿è”é…‹çš„{ratio:.0f}å€ï¼Œå¿…é¡»è¿›è¡Œå¯¹æ•°å˜æ¢")
                    
        except Exception as e:
            print(f"  âŒ å¯¹æ¯”åˆ†æå¤±è´¥: {e}")
    
    def analyze_value_distributions(self):
        """åˆ†ææ•°å€¼åˆ†å¸ƒï¼ˆæ£€æµ‹é•¿å°¾/å¹‚å¾‹åˆ†å¸ƒï¼‰"""
        print("\n" + "=" * 80)
        print("ğŸ“ˆ 5. æ•°å€¼åˆ†å¸ƒåˆ†æï¼ˆé•¿å°¾/ååº¦æ£€æµ‹ï¼‰")
        print("=" * 80)
        
        key_datasets = [
            ('å„å›½å†å¹´äººå·¥æ™ºèƒ½å‡ºç‰ˆç‰©æ•°é‡.csv', 'publications'),
            ('å„å›½å†å¹´äººå·¥æ™ºèƒ½é«˜å½±å“åŠ›å‡ºç‰ˆç‰©æ•°é‡.csv', 'publications'),
            ('å„å›½å†å¹´åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸæ‰€æœ‰è¡Œä¸šçš„é£é™©æŠ•èµ„ï¼ˆç™¾ä¸‡ç¾å…ƒï¼‰.csv', 'Sum_of_deals'),
            ('å„å›½å†å¹´å¯¹ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åˆåˆ›ä¼ä¸šçš„é£é™©æŠ•èµ„ï¼ˆç™¾ä¸‡ç¾å…ƒï¼‰.csv', 'Sum_of_deals'),
            ('å„å›½å†å¹´åœ¨GitHubä¸Šçš„é¡¹ç›®æ•°.csv', 'AI_projects_fractional_count_based_on_contributions'),
        ]
        
        distribution_report = []
        
        for filename, value_col in key_datasets:
            file_path = DATA_DIR / filename
            if not file_path.exists():
                continue
            
            try:
                df = pd.read_csv(file_path)
                if value_col not in df.columns:
                    continue
                
                values = df[value_col].dropna()
                if len(values) < 10:
                    continue
                
                # è®¡ç®—ç»Ÿè®¡é‡
                skewness = values.skew()
                kurtosis = values.kurtosis()
                cv = values.std() / values.mean() if values.mean() != 0 else 0
                
                # åˆ¤æ–­æ˜¯å¦éœ€è¦å¯¹æ•°å˜æ¢
                need_log = abs(skewness) > 2 or cv > 2
                
                distribution_report.append({
                    'æ•°æ®é›†': filename[:40] + '...' if len(filename) > 40 else filename,
                    'ååº¦': f"{skewness:.2f}",
                    'å³°åº¦': f"{kurtosis:.2f}",
                    'å˜å¼‚ç³»æ•°': f"{cv:.2f}",
                    'å»ºè®®å¯¹æ•°å˜æ¢': 'âœ… æ˜¯' if need_log else 'âŒ å¦',
                    'Min-Maxæ¯”': f"{values.max()/values.min():.0f}x" if values.min() > 0 else 'N/A'
                })
                
                if need_log:
                    self.recommendations.append(f"ğŸ’¡ {filename}: ååº¦={skewness:.2f}ï¼Œå¼ºçƒˆå»ºè®®å¯¹æ•°å˜æ¢")
                    
            except Exception as e:
                continue
        
        if distribution_report:
            report_df = pd.DataFrame(distribution_report)
            print(report_df.to_string(index=False))
            
        print("\nğŸ“ å¯¹æ•°å˜æ¢è¯´æ˜:")
        print("  - ååº¦ > 2: æ•°æ®ä¸¥é‡å³åï¼Œå¯¹æ•°å˜æ¢å¯æ”¹å–„")
        print("  - å˜å¼‚ç³»æ•° > 2: æ•°æ®ç¦»æ•£åº¦è¿‡å¤§")
        print("  - Min-Maxæ¯”è¿‡å¤§: å¤´éƒ¨ä¸å°¾éƒ¨å·®è·æ‚¬æ®Š")
    
    def analyze_currency_data(self):
        """åˆ†æè´§å¸æ•°æ®ï¼ˆæ£€æµ‹æ˜¯å¦éœ€è¦é€šèƒ€/PPPè°ƒæ•´ï¼‰"""
        print("\n" + "=" * 80)
        print("ğŸ’° 6. è´§å¸æ•°æ®åˆ†æï¼ˆé€šèƒ€/PPPè°ƒæ•´éœ€æ±‚ï¼‰")
        print("=" * 80)
        
        currency_files = [f for f in DATA_DIR.glob("*.csv") if 'ç¾å…ƒ' in f.name or 'USD' in f.name.upper() or 'æŠ•èµ„' in f.name]
        
        print(f"å‘ç° {len(currency_files)} ä¸ªè´§å¸ç›¸å…³æ•°æ®é›†:\n")
        
        for file_path in currency_files:
            try:
                df = pd.read_csv(file_path)
                
                print(f"ğŸ“„ {file_path.name}")
                
                # æ£€æŸ¥å¹´ä»½èŒƒå›´
                year_col = None
                for col in df.columns:
                    if 'year' in col.lower() or col.lower() == 'quarter':
                        year_col = col
                        break
                
                if year_col:
                    years = df[year_col].dropna().unique()
                    years_numeric = []
                    for y in years:
                        try:
                            if isinstance(y, (int, float)):
                                years_numeric.append(int(y))
                            else:
                                for i in range(2010, 2030):
                                    if str(i) in str(y):
                                        years_numeric.append(i)
                                        break
                        except:
                            pass
                    
                    if years_numeric:
                        year_span = max(years_numeric) - min(years_numeric)
                        print(f"  - æ—¶é—´è·¨åº¦: {min(years_numeric)}-{max(years_numeric)} ({year_span}å¹´)")
                        
                        if year_span >= 5:
                            self.recommendations.append(f"ğŸ’¡ {file_path.name}: æ—¶é—´è·¨åº¦{year_span}å¹´ï¼Œå¿…é¡»è¿›è¡Œé€šèƒ€è°ƒæ•´ï¼ˆè½¬æ¢ä¸º2020å¹´ä¸å˜ä»·ç¾å…ƒï¼‰")
                            print(f"  - âš ï¸ å»ºè®®: éœ€è¦é€šèƒ€è°ƒæ•´ (CPI deflator)")
                        
                # æ£€æŸ¥é‡‘é¢æ•°æ®åˆ—
                value_cols = [c for c in df.columns if 'deal' in c.lower() or 'amount' in c.lower() or 'value' in c.lower()]
                if value_cols:
                    print(f"  - é‡‘é¢åˆ—: {value_cols}")
                    
            except Exception as e:
                print(f"  âŒ åˆ†æå¤±è´¥: {e}")
            print()
        
        print("\nğŸ“ è´§å¸è°ƒæ•´å»ºè®®:")
        print("  1. å°†æ‰€æœ‰ç¾å…ƒé‡‘é¢è½¬æ¢ä¸º2020å¹´ä¸å˜ä»·ç¾å…ƒ")
        print("  2. å¯¹äºè–ªèµ„ã€åŸºç¡€è®¾æ–½æŠ•å…¥ï¼Œè€ƒè™‘ä½¿ç”¨PPPè°ƒæ•´")
        print("  3. ä¸­å›½çš„'1ä¸‡äº¿å…ƒ'æŠ•èµ„éœ€æŒ‰å½“å¹´æ±‡ç‡è½¬æ¢")
    
    def analyze_granularity(self):
        """åˆ†ææ•°æ®é¢—ç²’åº¦"""
        print("\n" + "=" * 80)
        print("â° 7. æ•°æ®é¢—ç²’åº¦åˆ†æï¼ˆæ—¶é—´ç²’åº¦ç»Ÿä¸€ï¼‰")
        print("=" * 80)
        
        granularity_report = []
        
        for file_path in DATA_DIR.glob("*.csv"):
            try:
                df = pd.read_csv(file_path, nrows=100)
                
                # æ£€æµ‹æ—¶é—´é¢—ç²’åº¦
                granularity = 'æœªçŸ¥'
                
                if 'Quarter' in df.columns:
                    granularity = 'å­£åº¦'
                elif any('æœˆ' in str(c) or 'Month' in str(c) for c in df.columns):
                    granularity = 'æœˆåº¦'
                elif 'Year' in df.columns or 'year' in df.columns:
                    granularity = 'å¹´åº¦'
                elif 'Date' in df.columns:
                    # æ£€æŸ¥æ—¥æœŸæ ¼å¼
                    date_vals = df['Date'].dropna().astype(str)
                    if len(date_vals) > 0:
                        sample = str(date_vals.iloc[0])
                        if len(sample) == 4 and sample.isdigit():
                            granularity = 'å¹´åº¦'
                        elif 'Q' in sample:
                            granularity = 'å­£åº¦'
                        else:
                            granularity = 'æ—¥æœŸ'
                
                if granularity != 'å¹´åº¦' and granularity != 'æœªçŸ¥':
                    granularity_report.append({
                        'æ–‡ä»¶': file_path.name[:50],
                        'é¢—ç²’åº¦': granularity,
                        'éœ€è¦é™é‡‡æ ·': 'âœ… æ˜¯'
                    })
                    self.recommendations.append(f"ğŸ’¡ {file_path.name}: {granularity}æ•°æ®ï¼Œéœ€é™é‡‡æ ·åˆ°å¹´åº¦")
                    
            except Exception as e:
                continue
        
        if granularity_report:
            print("éœ€è¦é™é‡‡æ ·çš„æ•°æ®é›†:")
            report_df = pd.DataFrame(granularity_report)
            print(report_df.to_string(index=False))
        else:
            print("âœ… æ‰€æœ‰æ•°æ®é›†å‡ä¸ºå¹´åº¦é¢—ç²’åº¦ï¼Œæ— éœ€é™é‡‡æ ·")
        
        print("\nğŸ“ é™é‡‡æ ·è§„åˆ™:")
        print("  - å­˜é‡æŒ‡æ ‡ï¼ˆäººæ‰æ•°ã€è¶…ç®—æ•°ï¼‰: å–å¹´æœ«å€¼")
        print("  - æµé‡æŒ‡æ ‡ï¼ˆè®ºæ–‡æ•°ã€æŠ•èµ„é¢ï¼‰: å¹´åº¦æ±‚å’Œ")
        print("  - ç‡å€¼æŒ‡æ ‡ï¼ˆå¢é•¿ç‡ã€å æ¯”ï¼‰: åŠ æƒå¹³å‡")
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        print("\n" + "=" * 100)
        print("ğŸ“‹ ç»¼åˆæ•°æ®è´¨é‡æŠ¥å‘Š")
        print("=" * 100)
        
        # é—®é¢˜æ±‡æ€»
        print("\nğŸš¨ å…³é”®é—®é¢˜ (Issues):")
        if self.issues:
            for i, issue in enumerate(self.issues, 1):
                print(f"  {i}. {issue}")
        else:
            print("  âœ… æ— å…³é”®é—®é¢˜")
        
        # è­¦å‘Šæ±‡æ€»
        print(f"\nâš ï¸ è­¦å‘Š ({len(self.warnings)} æ¡):")
        unique_warnings = list(set(self.warnings))[:10]  # å»é‡å¹¶é™åˆ¶æ•°é‡
        for warning in unique_warnings:
            print(f"  - {warning}")
        if len(self.warnings) > 10:
            print(f"  ... åŠå…¶ä»– {len(self.warnings) - 10} æ¡è­¦å‘Š")
        
        # å»ºè®®æ±‡æ€»
        print(f"\nğŸ’¡ Oå¥–çº§å¤„ç†å»ºè®® ({len(self.recommendations)} æ¡):")
        unique_recs = list(set(self.recommendations))
        for rec in unique_recs:
            print(f"  - {rec}")
        
        # ä¿å­˜æŠ¥å‘Š
        report_content = f"""# åæ•°æ¯ Bé¢˜ - æ•°æ®è´¨é‡æ·±åº¦åˆ†ææŠ¥å‘Š

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸš¨ å…³é”®é—®é¢˜ (Issues)

"""
        for issue in self.issues:
            report_content += f"- {issue}\n"
        
        report_content += f"""

## âš ï¸ è­¦å‘Š ({len(self.warnings)} æ¡)

"""
        for warning in list(set(self.warnings)):
            report_content += f"- {warning}\n"
        
        report_content += f"""

## ğŸ’¡ Oå¥–çº§å¤„ç†å»ºè®® ({len(self.recommendations)} æ¡)

"""
        for rec in list(set(self.recommendations)):
            report_content += f"- {rec}\n"
        
        report_content += """

## ğŸ“ é¢„å¤„ç†æ¸…å• (Checklist)

### 1. æ—¶é—´ç»´åº¦å¯¹é½
- [ ] æ£€æŸ¥2016-2025å¹´æ•°æ®å®Œæ•´æ€§
- [ ] å¯¹ä¸­é—´ç¼ºå¤±ä½¿ç”¨ä¸‰æ¬¡æ ·æ¡æ’å€¼
- [ ] å¯¹å°¾éƒ¨ç¼ºå¤±(2024-2025)ä½¿ç”¨Holt-Winterså¤–æ¨

### 2. å›½å®¶å®ä½“å¤„ç†
- [ ] ç»Ÿä¸€å›½å®¶ä»£ç ï¼ˆUSA, CHN, GBR...ï¼‰
- [ ] å¤„ç†é˜¿è”é…‹(ARE)çš„ç»“æ„æ€§ç¼ºå¤±
- [ ] æ˜ç¡®ä¸­å›½æ•°æ®å£å¾„ï¼ˆæ˜¯å¦å«æ¸¯æ¾³å°ï¼‰

### 3. è´§å¸æ ‡å‡†åŒ–
- [ ] æ‰€æœ‰é‡‘é¢è½¬æ¢ä¸º2020å¹´ä¸å˜ä»·ç¾å…ƒ
- [ ] è€ƒè™‘PPPè°ƒæ•´ï¼ˆå¦‚é€‚ç”¨ï¼‰
- [ ] äººæ°‘å¸æŒ‰å½“å¹´æ±‡ç‡è½¬æ¢

### 4. é•¿å°¾åˆ†å¸ƒå¤„ç†
- [ ] å¯¹ååº¦>2çš„æŒ‡æ ‡è¿›è¡Œlog1på˜æ¢
- [ ] å½’ä¸€åŒ–å‰å…ˆå¯¹æ•°å˜æ¢
- [ ] éªŒè¯ç¬¬äºŒæ¢¯é˜Ÿå›½å®¶åŒºåˆ†åº¦

### 5. æ»åç‰¹å¾
- [ ] åˆ›å»º1-3å¹´æ»åç‰¹å¾
- [ ] è®¡ç®—å¹´åº¦å¢é•¿ç‡
- [ ] åˆ›å»º3å¹´ç§»åŠ¨å¹³å‡

### 6. é¢—ç²’åº¦ç»Ÿä¸€
- [ ] å­£åº¦/æœˆåº¦æ•°æ®é™é‡‡æ ·åˆ°å¹´åº¦
- [ ] å­˜é‡æŒ‡æ ‡å–å¹´æœ«å€¼
- [ ] æµé‡æŒ‡æ ‡å–å¹´åº¦æ±‚å’Œ
"""
        
        report_path = OUTPUT_DIR / 'data_quality_report.md'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"\nâœ… è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")


def main():
    """ä¸»å‡½æ•°"""
    analyzer = DataQualityAnalyzer()
    analyzer.analyze_all_datasets()


if __name__ == "__main__":
    main()
