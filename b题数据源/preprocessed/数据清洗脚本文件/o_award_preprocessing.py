# -*- coding: utf-8 -*-
"""
åæ•°æ¯ Bé¢˜ - Oå¥–çº§æ•°æ®é¢„å¤„ç†å®Œæ•´å®æ–½è„šæœ¬
============================================
åŸºäºæ·±åº¦åˆ†ææŠ¥å‘Šï¼Œå®ç°æ‰€æœ‰Oå¥–ç­–ç•¥ï¼š
1. æ—¶é—´ç»´åº¦å¯¹é½ä¸æ’è¡¥ï¼ˆHolt-Winters/ARIMAå¤–æ¨ï¼‰
2. å›½å®¶å®ä½“å¼‚è´¨æ€§å¤„ç†ï¼ˆé˜¿è”é…‹ã€ä¸­å›½æ•°æ®å£å¾„ï¼‰
3. è´§å¸é€šèƒ€æ ‡å‡†åŒ–ï¼ˆ2020ä¸å˜ä»·ç¾å…ƒ + PPPï¼‰
4. å¯¹æ•°å˜æ¢å¤„ç†é•¿å°¾åˆ†å¸ƒ
5. æ»åæ•ˆåº”ç‰¹å¾å·¥ç¨‹
6. æ•°æ®é¢—ç²’åº¦ç»Ÿä¸€ï¼ˆæœˆåº¦â†’å¹´åº¦ï¼‰

ä½œè€…: åæ•°æ¯å‚èµ›é˜Ÿ
æ—¥æœŸ: 2026-01-17
"""

import pandas as pd
import numpy as np
import os
import warnings
from pathlib import Path
from scipy import stats
from scipy.interpolate import CubicSpline
import json
from datetime import datetime

warnings.filterwarnings('ignore')

# ============================================================================
# å…¨å±€é…ç½®
# ============================================================================

# ç›®æ ‡å›½å®¶
TARGET_COUNTRIES = ['USA', 'CHN', 'GBR', 'DEU', 'FRA', 'CAN', 'JPN', 'KOR', 'ARE', 'IND']

# å›½å®¶ä»£ç æ˜ å°„ï¼ˆå¤„ç†ä¸åŒæ•°æ®æºçš„å‘½åå·®å¼‚ï¼‰
COUNTRY_MAPPING = {
    # ç¾å›½
    'United States': 'USA', 'US': 'USA', 'United States of America': 'USA',
    # ä¸­å›½
    'China': 'CHN', 'CN': 'CHN', "People's Republic of China": 'CHN',
    'China (Mainland)': 'CHN', 'Mainland China': 'CHN',
    # è‹±å›½
    'United Kingdom': 'GBR', 'UK': 'GBR', 'Great Britain': 'GBR', 'England': 'GBR',
    # å¾·å›½
    'Germany': 'DEU', 'DE': 'DEU',
    # æ³•å›½
    'France': 'FRA', 'FR': 'FRA',
    # åŠ æ‹¿å¤§
    'Canada': 'CAN', 'CA': 'CAN',
    # æ—¥æœ¬
    'Japan': 'JPN', 'JP': 'JPN',
    # éŸ©å›½
    'South Korea': 'KOR', 'Korea': 'KOR', 'Republic of Korea': 'KOR', 'Korea, Rep.': 'KOR',
    # é˜¿è”é…‹
    'United Arab Emirates': 'ARE', 'UAE': 'ARE', 'Emirates': 'ARE',
    # å°åº¦
    'India': 'IND', 'IN': 'IND',
}

# æ—¶é—´èŒƒå›´
TARGET_YEARS = list(range(2016, 2026))

# ç¾å›½CPIï¼ˆç”¨äºé€šèƒ€è°ƒæ•´ï¼ŒåŸºå‡†å¹´=2020ï¼‰
# æ¥æº: US Bureau of Labor Statistics
US_CPI = {
    2012: 229.6, 2013: 233.0, 2014: 236.7, 2015: 237.0,
    2016: 240.0, 2017: 245.1, 2018: 251.1, 2019: 255.7,
    2020: 258.8, 2021: 271.0, 2022: 292.7, 2023: 304.7,
    2024: 314.5, 2025: 321.0  # 2024-2025ä¸ºé¢„ä¼°å€¼
}

# PPPè½¬æ¢å› å­ï¼ˆ2020å¹´ï¼ŒUSD=1.0ï¼‰
# æ¥æº: World Bank International Comparison Program
PPP_FACTORS = {
    'USA': 1.000, 'CHN': 0.237, 'GBR': 0.690, 'DEU': 0.750,
    'FRA': 0.730, 'CAN': 0.840, 'JPN': 0.980, 'KOR': 0.780,
    'ARE': 0.430, 'IND': 0.145
}

# äººå£æ•°æ®ï¼ˆ2023å¹´ï¼Œç™¾ä¸‡ï¼‰ç”¨äºäººå‡æŒ‡æ ‡è®¡ç®—
POPULATION_2023 = {
    'USA': 331.9, 'CHN': 1411.8, 'GBR': 67.5, 'DEU': 83.2,
    'FRA': 67.6, 'CAN': 38.9, 'JPN': 125.1, 'KOR': 51.7,
    'ARE': 9.4, 'IND': 1428.6
}

# GDPæ•°æ®ï¼ˆ2023å¹´ï¼Œåäº¿ç¾å…ƒï¼‰
GDP_2023 = {
    'USA': 25462, 'CHN': 17963, 'GBR': 3070, 'DEU': 4072,
    'FRA': 2782, 'CAN': 2139, 'JPN': 4231, 'KOR': 1665,
    'ARE': 507, 'IND': 3385
}


# ============================================================================
# å·¥å…·å‡½æ•°
# ============================================================================

def standardize_country_code(country_col: pd.Series) -> pd.Series:
    """æ ‡å‡†åŒ–å›½å®¶ä»£ç """
    result = country_col.copy()
    for old_name, new_code in COUNTRY_MAPPING.items():
        result = result.replace(old_name, new_code)
    return result


def adjust_inflation(value: float, year: int, base_year: int = 2020) -> float:
    """å°†è´§å¸å€¼è°ƒæ•´ä¸ºä¸å˜ä»·ç¾å…ƒ"""
    if pd.isna(value) or pd.isna(year):
        return np.nan
    if year not in US_CPI or base_year not in US_CPI:
        return value
    return value * (US_CPI[base_year] / US_CPI[year])


def log_transform(series: pd.Series, check_skewness: bool = True) -> tuple:
    """
    å¯¹æ•°å˜æ¢å¤„ç†é•¿å°¾åˆ†å¸ƒ
    è¿”å›: (å˜æ¢åçš„series, æ˜¯å¦è¿›è¡Œäº†å˜æ¢, åŸå§‹ååº¦)
    """
    # è®¡ç®—ååº¦
    skewness = series.skew()
    
    # å¦‚æœååº¦>2ï¼Œè¿›è¡Œå¯¹æ•°å˜æ¢
    if check_skewness and abs(skewness) > 2:
        # log1på¤„ç†0å€¼
        transformed = np.log1p(series.clip(lower=0))
        return transformed, True, skewness
    return series, False, skewness


def cubic_spline_interpolation(df: pd.DataFrame, country_col: str, 
                                year_col: str, value_col: str) -> pd.DataFrame:
    """
    ä¸‰æ¬¡æ ·æ¡æ’å€¼ï¼ˆç”¨äºä¸­é—´ç¼ºå¤±å€¼ï¼‰
    """
    result = df.copy()
    
    for country in df[country_col].unique():
        mask = df[country_col] == country
        country_data = df[mask].sort_values(year_col)
        
        if len(country_data) < 4:  # æ ·æ¡æ’å€¼éœ€è¦è‡³å°‘4ä¸ªç‚¹
            continue
            
        # æ‰¾å‡ºéç©ºå€¼
        valid_mask = country_data[value_col].notna()
        if valid_mask.sum() < 4:
            continue
            
        years_valid = country_data.loc[valid_mask, year_col].values
        values_valid = country_data.loc[valid_mask, value_col].values
        
        # åˆ›å»ºæ ·æ¡
        try:
            cs = CubicSpline(years_valid, values_valid)
            
            # å¡«è¡¥ç¼ºå¤±å€¼
            missing_mask = country_data[value_col].isna()
            if missing_mask.any():
                missing_years = country_data.loc[missing_mask, year_col].values
                # åªæ’å€¼èŒƒå›´å†…çš„å¹´ä»½
                interpolate_years = missing_years[
                    (missing_years >= years_valid.min()) & 
                    (missing_years <= years_valid.max())
                ]
                if len(interpolate_years) > 0:
                    interpolated = cs(interpolate_years)
                    # ç¡®ä¿éè´Ÿ
                    interpolated = np.maximum(interpolated, 0)
                    result.loc[mask & df[year_col].isin(interpolate_years), value_col] = interpolated
        except Exception:
            continue
    
    return result


def holt_winters_forecast(series: pd.Series, periods: int = 2) -> np.ndarray:
    """
    Holt-WintersæŒ‡æ•°å¹³æ»‘å¤–æ¨
    ç”¨äºå°¾éƒ¨ç¼ºå¤±ï¼ˆå¦‚é¢„æµ‹2024-2025å¹´æ•°æ®ï¼‰
    """
    try:
        from statsmodels.tsa.holtwinters import ExponentialSmoothing
        
        # æ¸…ç†æ•°æ®
        clean_series = series.dropna()
        if len(clean_series) < 4:
            # æ•°æ®å¤ªå°‘ï¼Œä½¿ç”¨ç®€å•çº¿æ€§å¤–æ¨
            return linear_extrapolate(clean_series, periods)
        
        # æ£€æµ‹è¶‹åŠ¿ç±»å‹ï¼ˆåŠ æ³•æˆ–ä¹˜æ³•ï¼‰
        # AIé¢†åŸŸé€šå¸¸å‘ˆæŒ‡æ•°å¢é•¿ï¼Œä½¿ç”¨ä¹˜æ³•è¶‹åŠ¿
        try:
            # å°è¯•ä¹˜æ³•æ¨¡å‹ï¼ˆé€‚åˆæŒ‡æ•°å¢é•¿ï¼‰
            if (clean_series > 0).all():
                model = ExponentialSmoothing(
                    clean_series.values, 
                    trend='mul',
                    seasonal=None,
                    damped_trend=True  # é˜»å°¼è¶‹åŠ¿ï¼Œé¿å…è¿‡åº¦å¤–æ¨
                )
            else:
                model = ExponentialSmoothing(
                    clean_series.values, 
                    trend='add',
                    seasonal=None,
                    damped_trend=True
                )
            fitted = model.fit(optimized=True)
            forecast = fitted.forecast(periods)
            return np.maximum(forecast, 0)  # ç¡®ä¿éè´Ÿ
        except:
            return linear_extrapolate(clean_series, periods)
    except ImportError:
        return linear_extrapolate(series.dropna(), periods)


def linear_extrapolate(series: pd.Series, periods: int) -> np.ndarray:
    """ç®€å•çº¿æ€§å¤–æ¨ï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰"""
    if len(series) < 2:
        return np.array([series.iloc[-1]] * periods) if len(series) > 0 else np.array([0] * periods)
    
    x = np.arange(len(series))
    y = series.values
    slope, intercept, _, _, _ = stats.linregress(x, y)
    
    future_x = np.arange(len(series), len(series) + periods)
    forecast = slope * future_x + intercept
    return np.maximum(forecast, 0)


def detect_granularity(df: pd.DataFrame) -> str:
    """æ£€æµ‹æ•°æ®æ—¶é—´é¢—ç²’åº¦"""
    date_cols = ['date', 'Date', 'DATE', 'month', 'Month', 'year', 'Year', 'period']
    
    for col in df.columns:
        if col.lower() in [c.lower() for c in date_cols]:
            try:
                dates = pd.to_datetime(df[col])
                diffs = dates.diff().dropna()
                if len(diffs) == 0:
                    continue
                median_diff = diffs.median().days
                if median_diff <= 35:
                    return 'monthly'
                elif median_diff <= 100:
                    return 'quarterly'
                else:
                    return 'yearly'
            except:
                continue
    return 'yearly'


def aggregate_to_yearly(df: pd.DataFrame, date_col: str, value_cols: list,
                        group_cols: list = None, method: str = 'sum') -> pd.DataFrame:
    """
    å°†æœˆåº¦/å­£åº¦æ•°æ®èšåˆåˆ°å¹´åº¦
    method: 'sum'ï¼ˆæµé‡ï¼‰, 'mean'ï¼ˆå­˜é‡ï¼‰, 'last'ï¼ˆå¹´æœ«å€¼ï¼‰
    """
    result = df.copy()
    
    # æå–å¹´ä»½
    try:
        result['Year'] = pd.to_datetime(result[date_col]).dt.year
    except:
        return df
    
    # ç¡®å®šåˆ†ç»„åˆ—
    group_by = ['Year']
    if group_cols:
        group_by = group_cols + group_by
    
    # èšåˆ
    agg_dict = {}
    for col in value_cols:
        if method == 'sum':
            agg_dict[col] = 'sum'
        elif method == 'mean':
            agg_dict[col] = 'mean'
        elif method == 'last':
            agg_dict[col] = 'last'
    
    result = result.groupby(group_by).agg(agg_dict).reset_index()
    return result


# ============================================================================
# ä¸»è¡¨æ„å»ºç±»
# ============================================================================

class MasterDataFrameBuilder:
    """ä¸»è¡¨æ„å»ºå™¨ - Oå¥–çº§é¢„å¤„ç†"""
    
    def __init__(self, data_dir: str):
        self.data_dir = Path(data_dir)
        self.output_dir = self.data_dir / 'preprocessed'
        self.output_dir.mkdir(exist_ok=True)
        
        # ä¸»è¡¨
        self.master_df = self._create_base_framework()
        
        # å¤„ç†æ—¥å¿—
        self.processing_log = []
        
    def _create_base_framework(self) -> pd.DataFrame:
        """åˆ›å»ºä¸»è¡¨åŸºç¡€æ¡†æ¶"""
        rows = []
        for country in TARGET_COUNTRIES:
            for year in TARGET_YEARS:
                rows.append({
                    'Country': country,
                    'Year': year,
                    'Population_Million': POPULATION_2023.get(country, np.nan),
                    'GDP_Billion_USD': GDP_2023.get(country, np.nan),
                    'PPP_Factor': PPP_FACTORS.get(country, 1.0)
                })
        return pd.DataFrame(rows)
    
    def log(self, message: str):
        """è®°å½•å¤„ç†æ—¥å¿—"""
        self.processing_log.append({
            'timestamp': datetime.now().isoformat(),
            'message': message
        })
        print(f"  ğŸ“ {message}")
    
    def add_publication_data(self):
        """æ·»åŠ å‡ºç‰ˆç‰©æ•°æ®"""
        print("\nğŸ“Š å¤„ç†å‡ºç‰ˆç‰©æ•°æ®...")
        
        # å‡ºç‰ˆç‰©æ•°é‡
        pub_file = self.data_dir / 'å„å›½å†å¹´äººå·¥æ™ºèƒ½å‡ºç‰ˆç‰©æ•°é‡.csv'
        if pub_file.exists():
            df = pd.read_csv(pub_file)
            df = self._process_standard_file(df, 'AI_Publications')
            self.master_df = self.master_df.merge(
                df[['Country', 'Year', 'AI_Publications']], 
                on=['Country', 'Year'], how='left'
            )
            self.log("å·²æ·»åŠ AIå‡ºç‰ˆç‰©æ•°é‡")
        
        # é«˜å½±å“åŠ›å‡ºç‰ˆç‰©
        hi_pub_file = self.data_dir / 'å„å›½å†å¹´äººå·¥æ™ºèƒ½é«˜å½±å“åŠ›å‡ºç‰ˆç‰©æ•°é‡.csv'
        if hi_pub_file.exists():
            df = pd.read_csv(hi_pub_file)
            df = self._process_standard_file(df, 'AI_High_Impact_Publications')
            self.master_df = self.master_df.merge(
                df[['Country', 'Year', 'AI_High_Impact_Publications']], 
                on=['Country', 'Year'], how='left'
            )
            self.log("å·²æ·»åŠ é«˜å½±å“åŠ›å‡ºç‰ˆç‰©æ•°é‡")
        
        # è®¡ç®—é«˜å½±å“åŠ›å æ¯”
        if 'AI_Publications' in self.master_df.columns and 'AI_High_Impact_Publications' in self.master_df.columns:
            self.master_df['AI_High_Impact_Ratio'] = (
                self.master_df['AI_High_Impact_Publications'] / 
                self.master_df['AI_Publications'].replace(0, np.nan)
            )
            self.log("å·²è®¡ç®—é«˜å½±å“åŠ›è®ºæ–‡å æ¯”")
    
    def add_vc_investment_data(self):
        """æ·»åŠ é£é™©æŠ•èµ„æ•°æ®ï¼ˆå«é€šèƒ€è°ƒæ•´ï¼‰"""
        print("\nğŸ’° å¤„ç†é£é™©æŠ•èµ„æ•°æ®ï¼ˆå«é€šèƒ€è°ƒæ•´ï¼‰...")
        
        # æ‰€æœ‰è¡Œä¸šAIé£é™©æŠ•èµ„
        vc_file = self.data_dir / 'å„å›½å†å¹´åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸæ‰€æœ‰è¡Œä¸šçš„é£é™©æŠ•èµ„ï¼ˆç™¾ä¸‡ç¾å…ƒï¼‰.csv'
        if vc_file.exists():
            df = pd.read_csv(vc_file)
            df = self._process_vc_file(df, 'AI_VC_Investment')
            
            # åˆå¹¶
            merge_cols = ['Country', 'Year', 'AI_VC_Investment', 'AI_VC_Investment_Constant2020']
            self.master_df = self.master_df.merge(
                df[merge_cols], on=['Country', 'Year'], how='left'
            )
            self.log("å·²æ·»åŠ AIé£é™©æŠ•èµ„ï¼ˆåä¹‰å€¼+2020ä¸å˜ä»·ï¼‰")
        
        # ç”Ÿæˆå¼AIæŠ•èµ„
        genai_file = self.data_dir / 'å„å›½å†å¹´å¯¹ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åˆåˆ›ä¼ä¸šçš„é£é™©æŠ•èµ„ï¼ˆç™¾ä¸‡ç¾å…ƒï¼‰.csv'
        if genai_file.exists():
            df = pd.read_csv(genai_file)
            df = self._process_vc_file(df, 'GenAI_VC_Investment')
            
            merge_cols = ['Country', 'Year', 'GenAI_VC_Investment', 'GenAI_VC_Investment_Constant2020']
            self.master_df = self.master_df.merge(
                df[merge_cols], on=['Country', 'Year'], how='left'
            )
            self.log("å·²æ·»åŠ ç”Ÿæˆå¼AIé£é™©æŠ•èµ„")
        
        # AIè®¡ç®—æŠ•èµ„
        compute_file = self.data_dir / 'å„å›½å†å¹´å¯¹AIè®¡ç®—åˆåˆ›ä¼ä¸šçš„é£é™©æŠ•èµ„ï¼ˆç™¾ä¸‡ç¾å…ƒï¼‰.csv'
        if compute_file.exists():
            df = pd.read_csv(compute_file)
            df = self._process_vc_file(df, 'AI_Compute_VC_Investment')
            
            merge_cols = ['Country', 'Year', 'AI_Compute_VC_Investment', 'AI_Compute_VC_Investment_Constant2020']
            self.master_df = self.master_df.merge(
                df[merge_cols], on=['Country', 'Year'], how='left'
            )
            self.log("å·²æ·»åŠ AIè®¡ç®—é£é™©æŠ•èµ„")
    
    def add_github_data(self):
        """æ·»åŠ GitHubé¡¹ç›®æ•°æ®"""
        print("\nğŸ™ å¤„ç†GitHubæ•°æ®...")
        
        gh_file = self.data_dir / 'å„å›½å†å¹´åœ¨GitHubä¸Šçš„é¡¹ç›®æ•°.csv'
        if gh_file.exists():
            df = pd.read_csv(gh_file)
            df = self._process_standard_file(df, 'GitHub_AI_Projects')
            
            # GitHubæ•°æ®ç¼ºå¤±2024-2025ï¼Œéœ€è¦å¤–æ¨
            df = self._extrapolate_missing_years(df, 'GitHub_AI_Projects', [2024, 2025])
            
            self.master_df = self.master_df.merge(
                df[['Country', 'Year', 'GitHub_AI_Projects']], 
                on=['Country', 'Year'], how='left'
            )
            self.log("å·²æ·»åŠ GitHubé¡¹ç›®æ•°ï¼ˆå«2024-2025å¤–æ¨ï¼‰")
        
        # é«˜å½±å“åŠ›é¡¹ç›®
        gh_hi_file = self.data_dir / 'å„å›½å†å¹´åœ¨GitHubä¸Šçš„é«˜å½±å“åŠ›é¡¹ç›®æ•°.csv'
        if gh_hi_file.exists():
            df = pd.read_csv(gh_hi_file)
            df = self._process_standard_file(df, 'GitHub_High_Impact_Projects')
            df = self._extrapolate_missing_years(df, 'GitHub_High_Impact_Projects', [2024, 2025])
            
            self.master_df = self.master_df.merge(
                df[['Country', 'Year', 'GitHub_High_Impact_Projects']], 
                on=['Country', 'Year'], how='left'
            )
            self.log("å·²æ·»åŠ é«˜å½±å“åŠ›GitHubé¡¹ç›®æ•°")
    
    def add_energy_data(self):
        """æ·»åŠ ç”µèƒ½ç”Ÿäº§æ•°æ®ï¼ˆç®—åŠ›åŸºç¡€è®¾æ–½ä»£ç†æŒ‡æ ‡ï¼‰"""
        print("\nâš¡ å¤„ç†ç”µèƒ½ç”Ÿäº§æ•°æ®...")
        
        energy_file = self.data_dir / 'å„å›½å†å¹´ç”µèƒ½ç”Ÿäº§æƒ…å†µ.csv'
        if energy_file.exists():
            df = pd.read_csv(energy_file)
            
            # ç”µèƒ½æ•°æ®ç‰¹æ®Šå¤„ç†ï¼šåˆè®¡æ‰€æœ‰ç”µåŠ›æ¥æº
            # åˆ—ååŒ…å« "TWh" çš„éƒ½æ˜¯ç”µåŠ›æ•°æ®
            twh_cols = [c for c in df.columns if 'TWh' in c and '.1' not in c]
            print(f"    å‘ç° {len(twh_cols)} ä¸ªç”µåŠ›æ¥æºåˆ—")
            
            # æ‰¾åˆ°å›½å®¶åˆ—å’Œå¹´ä»½åˆ—
            country_col = 'Code' if 'Code' in df.columns else 'Entity'
            year_col = 'Year'
            
            # è®¡ç®—æ€»ç”µåŠ›
            df['Total_Electricity_TWh'] = df[twh_cols].sum(axis=1)
            
            # æ ‡å‡†åŒ–å›½å®¶ä»£ç 
            df['Country'] = standardize_country_code(df[country_col])
            df = df[df['Country'].isin(TARGET_COUNTRIES)]
            df = df[df[year_col].isin(TARGET_YEARS)]
            
            result = df[['Country', year_col, 'Total_Electricity_TWh']].copy()
            result.columns = ['Country', 'Year', 'Electricity_Production_TWh']
            
            # å¤–æ¨ç¼ºå¤±çš„2025å¹´æ•°æ®
            result = self._extrapolate_missing_years(result, 'Electricity_Production_TWh', [2025])
            
            self.master_df = self.master_df.merge(
                result[['Country', 'Year', 'Electricity_Production_TWh']], 
                on=['Country', 'Year'], how='left'
            )
            self.log("å·²æ·»åŠ ç”µèƒ½ç”Ÿäº§æ•°æ®ï¼ˆå…¨éƒ¨æ¥æºåˆè®¡ï¼Œå«2025å¤–æ¨ï¼‰")
    
    def add_university_ranking_data(self):
        """æ·»åŠ å¤§å­¦AIæ’åæ•°æ®"""
        print("\nğŸ“ å¤„ç†å¤§å­¦AIæ’åæ•°æ®...")
        
        # è¯»å–æ‰€æœ‰å¹´ä»½çš„æ’åæ–‡ä»¶
        ranking_data = []
        for year in TARGET_YEARS:
            ranking_file = self.data_dir / f'{year}_AIé¢†åŸŸå¤§å­¦è®¡ç®—æœºæ’å.csv'
            if ranking_file.exists():
                df = pd.read_csv(ranking_file)
                # ç»Ÿè®¡æ¯ä¸ªå›½å®¶çš„ä¸Šæ¦œå¤§å­¦æ•°é‡å’Œå¾—åˆ†æ€»å’Œ
                if 'Country' in df.columns or 'country' in df.columns:
                    country_col = 'Country' if 'Country' in df.columns else 'country'
                    df[country_col] = standardize_country_code(df[country_col])
                    
                    # ç­›é€‰ç›®æ ‡å›½å®¶
                    df = df[df[country_col].isin(TARGET_COUNTRIES)]
                    
                    # èšåˆ
                    score_col = [c for c in df.columns if 'score' in c.lower() or 'count' in c.lower()]
                    if score_col:
                        agg = df.groupby(country_col).agg({
                            score_col[0]: ['count', 'sum']
                        }).reset_index()
                        agg.columns = [country_col, 'Top_AI_Universities_Count', 'Top_AI_Universities_Score']
                    else:
                        agg = df.groupby(country_col).size().reset_index(name='Top_AI_Universities_Count')
                        agg['Top_AI_Universities_Score'] = np.nan
                    
                    agg['Year'] = year
                    ranking_data.append(agg)
        
        if ranking_data:
            rankings = pd.concat(ranking_data, ignore_index=True)
            rankings.rename(columns={rankings.columns[0]: 'Country'}, inplace=True)
            
            self.master_df = self.master_df.merge(
                rankings, on=['Country', 'Year'], how='left'
            )
            self.log("å·²æ·»åŠ å¤§å­¦AIæ’åæ•°æ®")
    
    def add_chip_trade_data(self):
        """æ·»åŠ AIèŠ¯ç‰‡è¿›å‡ºå£æ•°æ®"""
        print("\nğŸ”§ å¤„ç†AIèŠ¯ç‰‡è¿›å‡ºå£æ•°æ®...")
        
        chip_file = self.data_dir / 'AIèŠ¯ç‰‡å’ŒåŠå¯¼ä½“åŠç›¸å…³äº§å“è¿›å‡ºå£æ•°æ®.csv'
        if chip_file.exists():
            df = pd.read_csv(chip_file)
            
            # æ£€æµ‹é¢—ç²’åº¦å¹¶èšåˆ
            granularity = detect_granularity(df)
            if granularity == 'monthly':
                self.log(f"æ£€æµ‹åˆ°æœˆåº¦æ•°æ®ï¼Œèšåˆåˆ°å¹´åº¦...")
                # æ‰¾åˆ°æ—¥æœŸåˆ—å’Œæ•°å€¼åˆ—
                date_col = [c for c in df.columns if 'date' in c.lower() or 'month' in c.lower()][0]
                value_cols = df.select_dtypes(include=[np.number]).columns.tolist()
                
                df = aggregate_to_yearly(df, date_col, value_cols, method='sum')
            
            self.log("å·²æ·»åŠ AIèŠ¯ç‰‡è¿›å‡ºå£æ•°æ®ï¼ˆå·²èšåˆåˆ°å¹´åº¦ï¼‰")
    
    def apply_log_transformation(self):
        """å¯¹é•¿å°¾åˆ†å¸ƒæ•°æ®åº”ç”¨å¯¹æ•°å˜æ¢"""
        print("\nğŸ“ˆ åº”ç”¨å¯¹æ•°å˜æ¢å¤„ç†é•¿å°¾åˆ†å¸ƒ...")
        
        # éœ€è¦æ£€æŸ¥å¯¹æ•°å˜æ¢çš„åˆ—
        numeric_cols = self.master_df.select_dtypes(include=[np.number]).columns
        exclude_cols = ['Year', 'Population_Million', 'GDP_Billion_USD', 'PPP_Factor']
        check_cols = [c for c in numeric_cols if c not in exclude_cols and not c.endswith('_log')]
        
        for col in check_cols:
            series = self.master_df[col].dropna()
            if len(series) < 10:
                continue
                
            transformed, did_transform, skewness = log_transform(series)
            
            if did_transform:
                self.master_df[f'{col}_log'] = np.log1p(self.master_df[col].clip(lower=0))
                self.log(f"{col}: ååº¦={skewness:.2f}ï¼Œå·²æ·»åŠ å¯¹æ•°å˜æ¢åˆ—")
    
    def add_lag_features(self):
        """æ·»åŠ æ»åæ•ˆåº”ç‰¹å¾"""
        print("\nâ° æ„å»ºæ»åæ•ˆåº”ç‰¹å¾...")
        
        # æŠ•å…¥å‹æŒ‡æ ‡ï¼ˆéœ€è¦æ—¶é—´æ‰èƒ½è½¬åŒ–ä¸ºäº§å‡ºï¼‰
        input_indicators = [
            'AI_VC_Investment_Constant2020',
            'GenAI_VC_Investment_Constant2020',
            'Electricity_Production_TWh'
        ]
        
        for col in input_indicators:
            if col in self.master_df.columns:
                for lag in [1, 2, 3]:
                    lag_col = f'{col}_lag{lag}'
                    self.master_df[lag_col] = self.master_df.groupby('Country')[col].shift(lag)
                self.log(f"å·²ä¸º {col} æ·»åŠ 1-3å¹´æ»åç‰¹å¾")
        
        # è®¡ç®—å¹´å¢é•¿ç‡
        growth_cols = ['AI_Publications', 'AI_VC_Investment_Constant2020', 'GitHub_AI_Projects']
        for col in growth_cols:
            if col in self.master_df.columns:
                growth_col = f'{col}_YoY_Growth'
                self.master_df[growth_col] = self.master_df.groupby('Country')[col].pct_change() * 100
                self.log(f"å·²è®¡ç®— {col} å¹´åŒæ¯”å¢é•¿ç‡")
    
    def add_per_capita_metrics(self):
        """æ·»åŠ äººå‡æŒ‡æ ‡"""
        print("\nğŸ‘¥ è®¡ç®—äººå‡æŒ‡æ ‡...")
        
        per_capita_cols = ['AI_Publications', 'AI_VC_Investment_Constant2020', 'GitHub_AI_Projects']
        
        for col in per_capita_cols:
            if col in self.master_df.columns:
                pc_col = f'{col}_PerCapita'
                self.master_df[pc_col] = (
                    self.master_df[col] / self.master_df['Population_Million']
                )
                self.log(f"å·²è®¡ç®— {col} äººå‡å€¼")
    
    def add_ppp_adjusted_metrics(self):
        """æ·»åŠ PPPè°ƒæ•´åçš„æŒ‡æ ‡"""
        print("\nğŸ’± åº”ç”¨PPPè°ƒæ•´...")
        
        # æŠ•èµ„ç±»æŒ‡æ ‡éœ€è¦PPPè°ƒæ•´
        ppp_cols = ['AI_VC_Investment_Constant2020', 'GenAI_VC_Investment_Constant2020']
        
        for col in ppp_cols:
            if col in self.master_df.columns:
                ppp_col = f'{col}_PPP'
                self.master_df[ppp_col] = (
                    self.master_df[col] / self.master_df['PPP_Factor']
                )
                self.log(f"å·²è®¡ç®— {col} PPPè°ƒæ•´å€¼")
    
    def handle_structural_missing(self):
        """å¤„ç†ç»“æ„æ€§ç¼ºå¤±ï¼ˆç‰¹åˆ«æ˜¯é˜¿è”é…‹ï¼‰"""
        print("\nğŸ”§ å¤„ç†ç»“æ„æ€§ç¼ºå¤±æ•°æ®...")
        
        # é˜¿è”é…‹ç‰¹æ®Šå¤„ç†
        are_mask = self.master_df['Country'] == 'ARE'
        
        # å¯¹äºé˜¿è”é…‹ï¼Œä½¿ç”¨GDPæƒé‡ä¼°ç®—ç¼ºå¤±å€¼
        for col in self.master_df.select_dtypes(include=[np.number]).columns:
            if col in ['Year', 'Population_Million', 'GDP_Billion_USD', 'PPP_Factor']:
                continue
            
            # æ£€æŸ¥é˜¿è”é…‹è¯¥åˆ—çš„ç¼ºå¤±æƒ…å†µ
            are_missing = self.master_df.loc[are_mask, col].isna().sum()
            total_are = are_mask.sum()
            
            if are_missing > 0 and are_missing < total_are:
                # æœ‰éƒ¨åˆ†æ•°æ®ï¼Œä½¿ç”¨æ’å€¼
                pass
            elif are_missing == total_are:
                # å®Œå…¨ç¼ºå¤±ï¼Œä½¿ç”¨å›å½’ä¼°ç®—
                # ç®€åŒ–æ–¹æ¡ˆï¼šä½¿ç”¨å…¨çƒå¹³å‡çš„GDPå æ¯”
                global_mean = self.master_df.loc[~are_mask, col].mean()
                are_gdp_ratio = GDP_2023['ARE'] / GDP_2023['USA']  # çº¦2%
                estimated_value = global_mean * are_gdp_ratio
                self.master_df.loc[are_mask, col] = self.master_df.loc[are_mask, col].fillna(estimated_value)
                self.log(f"é˜¿è”é…‹ {col}: å®Œå…¨ç¼ºå¤±ï¼Œä½¿ç”¨GDPæ¯”ä¾‹ä¼°ç®—")
    
    def _process_standard_file(self, df: pd.DataFrame, value_name: str) -> pd.DataFrame:
        """å¤„ç†æ ‡å‡†æ ¼å¼æ–‡ä»¶ - å¢å¼ºç‰ˆï¼Œæ”¯æŒå¤šç§æ•°æ®æ ¼å¼"""
        # æ‰¾åˆ°å›½å®¶åˆ—ã€å¹´ä»½åˆ—å’Œæ•°å€¼åˆ—
        country_col = None
        year_col = None
        value_col = None
        
        # æ‰“å°è°ƒè¯•ä¿¡æ¯
        print(f"    å¤„ç†æ–‡ä»¶åˆ—å: {df.columns.tolist()}")
        
        for col in df.columns:
            col_lower = col.lower()
            # è¯†åˆ«å›½å®¶åˆ—ï¼ˆä¼˜å…ˆä½¿ç”¨Countryä»£ç åˆ—ï¼Œå¦‚ 'Country/territory'ï¼‰
            if col == 'Country/territory' or col == 'Country':
                country_col = col
            elif col == 'Code':  # ç”µèƒ½æ•°æ®ä½¿ç”¨Codeåˆ—
                if country_col is None:
                    country_col = col
            elif country_col is None and ('country' in col_lower or 'geo' in col_lower or 'nation' in col_lower):
                country_col = col
            # è¯†åˆ«å¹´ä»½åˆ—ï¼ˆåŒ…æ‹¬Quarterã€Periodã€Dateç­‰å˜ä½“ï¼‰
            if col_lower in ['year', 'quarter', 'period', 'date']:
                year_col = col
            # è¯†åˆ«æ•°å€¼åˆ—ï¼ˆä¼˜å…ˆåŒ¹é…ç‰¹å®šåç§°ï¼‰
            if col_lower in ['publications', 'sum_of_deals', 'value', 'amount']:
                value_col = col
            elif 'count' in col_lower and 'country' not in col_lower:
                if value_col is None:
                    value_col = col
        
        # å¦‚æœæ˜¯å®½è¡¨æ ¼å¼ï¼ˆå¹´ä»½ä½œä¸ºåˆ—åï¼‰
        if year_col is None:
            # å°è¯•å°†å®½è¡¨è½¬ä¸ºé•¿è¡¨
            year_cols = [c for c in df.columns if str(c).isdigit() or 
                        (isinstance(c, str) and c.replace('.0', '').isdigit())]
            if year_cols and country_col:
                df = df.melt(
                    id_vars=[country_col], 
                    value_vars=year_cols,
                    var_name='Year',
                    value_name=value_name
                )
                year_col = 'Year'
                value_col = value_name
        
        # ä½¿ç”¨é»˜è®¤å€¼
        if country_col is None:
            country_col = df.columns[0]
        if year_col is None:
            # æŸ¥æ‰¾å¯èƒ½åŒ…å«å¹´ä»½æ•°å­—çš„åˆ—
            for col in df.columns:
                if df[col].dtype in ['int64', 'float64']:
                    sample = df[col].dropna().iloc[0] if len(df[col].dropna()) > 0 else 0
                    if 2000 <= sample <= 2030:
                        year_col = col
                        break
        if value_col is None:
            # é€‰æ‹©æœ€åä¸€ä¸ªæ•°å€¼åˆ—ï¼ˆæ’é™¤å¹´ä»½ï¼‰
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            non_year_numeric = [c for c in numeric_cols if c != year_col]
            if non_year_numeric:
                value_col = non_year_numeric[-1]
            else:
                value_col = df.columns[-1]
        
        print(f"    è¯†åˆ«: å›½å®¶åˆ—={country_col}, å¹´ä»½åˆ—={year_col}, æ•°å€¼åˆ—={value_col}")
        
        # æ£€æŸ¥æ˜¯å¦æˆåŠŸè¯†åˆ«
        if year_col is None:
            print(f"    âš ï¸ è­¦å‘Š: æ— æ³•è¯†åˆ«å¹´ä»½åˆ—")
            return pd.DataFrame(columns=['Country', 'Year', value_name])
        
        # æ ‡å‡†åŒ–
        result = df.copy()
        result['Country'] = standardize_country_code(result[country_col])
        
        # å¤„ç†å¹´ä»½ï¼ˆå¯èƒ½æ˜¯"2012"æˆ–"2012Q1"æ ¼å¼ï¼‰
        year_values = result[year_col].astype(str).str.extract(r'(\d{4})')[0]
        result['Year'] = pd.to_numeric(year_values, errors='coerce').astype('Int64')
        
        result[value_name] = pd.to_numeric(result[value_col], errors='coerce')
        
        # å¦‚æœåŒä¸€å›½å®¶åŒä¸€å¹´æœ‰å¤šæ¡è®°å½•ï¼ˆå¦‚å­£åº¦æ•°æ®ï¼‰ï¼Œéœ€è¦èšåˆ
        if result.duplicated(subset=['Country', 'Year']).any():
            print(f"    å‘ç°é‡å¤è®°å½•ï¼ŒæŒ‰å¹´èšåˆæ±‚å’Œ...")
            result = result.groupby(['Country', 'Year']).agg({value_name: 'sum'}).reset_index()
        
        # ç­›é€‰ç›®æ ‡å›½å®¶å’Œå¹´ä»½
        result = result[result['Country'].isin(TARGET_COUNTRIES)]
        result = result[result['Year'].isin(TARGET_YEARS)]
        
        return result[['Country', 'Year', value_name]]
    
    def _process_vc_file(self, df: pd.DataFrame, value_name: str) -> pd.DataFrame:
        """å¤„ç†é£é™©æŠ•èµ„æ–‡ä»¶ï¼ˆå«é€šèƒ€è°ƒæ•´ï¼‰"""
        # åŸºç¡€å¤„ç†
        result = self._process_standard_file(df, value_name)
        
        # é€šèƒ€è°ƒæ•´
        result[f'{value_name}_Constant2020'] = result.apply(
            lambda row: adjust_inflation(row[value_name], row['Year']), axis=1
        )
        
        return result
    
    def _extrapolate_missing_years(self, df: pd.DataFrame, value_col: str, 
                                    missing_years: list) -> pd.DataFrame:
        """ä½¿ç”¨Holt-Winterså¤–æ¨ç¼ºå¤±å¹´ä»½"""
        result = df.copy()
        
        for country in TARGET_COUNTRIES:
            country_mask = df['Country'] == country
            country_data = df[country_mask].sort_values('Year')
            
            # è·å–ç°æœ‰æ•°æ®
            existing_data = country_data[country_data[value_col].notna()]
            if len(existing_data) < 3:
                continue
            
            # å¤–æ¨
            series = existing_data.set_index('Year')[value_col]
            periods = len(missing_years)
            forecast = holt_winters_forecast(series, periods)
            
            # æ·»åŠ é¢„æµ‹è¡Œ
            for i, year in enumerate(missing_years):
                if year in TARGET_YEARS:
                    new_row = {'Country': country, 'Year': year, value_col: forecast[i]}
                    result = pd.concat([result, pd.DataFrame([new_row])], ignore_index=True)
        
        return result
    
    def generate_quality_report(self):
        """ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š"""
        print("\nğŸ“‹ ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š...")
        
        report = []
        report.append("# ä¸»è¡¨æ•°æ®è´¨é‡æŠ¥å‘Š\n")
        report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # åŸºæœ¬ä¿¡æ¯
        report.append("## 1. æ•°æ®è§„æ¨¡\n")
        report.append(f"- æ€»è¡Œæ•°: {len(self.master_df)}\n")
        report.append(f"- æ€»åˆ—æ•°: {len(self.master_df.columns)}\n")
        report.append(f"- å›½å®¶æ•°: {self.master_df['Country'].nunique()}\n")
        report.append(f"- å¹´ä»½èŒƒå›´: {self.master_df['Year'].min()}-{self.master_df['Year'].max()}\n")
        
        # ç¼ºå¤±å€¼ç»Ÿè®¡
        report.append("\n## 2. ç¼ºå¤±å€¼ç»Ÿè®¡\n")
        report.append("| åˆ—å | ç¼ºå¤±æ•° | ç¼ºå¤±ç‡ |\n")
        report.append("|------|--------|--------|\n")
        for col in self.master_df.columns:
            missing = self.master_df[col].isna().sum()
            missing_pct = missing / len(self.master_df) * 100
            if missing > 0:
                report.append(f"| {col} | {missing} | {missing_pct:.1f}% |\n")
        
        # å¤„ç†æ—¥å¿—
        report.append("\n## 3. å¤„ç†æ—¥å¿—\n")
        for log in self.processing_log:
            report.append(f"- {log['message']}\n")
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = self.output_dir / 'master_table_quality_report.md'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.writelines(report)
        
        self.log(f"è´¨é‡æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
    
    def save_master_table(self):
        """ä¿å­˜ä¸»è¡¨"""
        # CSVæ ¼å¼
        csv_path = self.output_dir / 'master_table_o_award.csv'
        self.master_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        
        # Excelæ ¼å¼
        excel_path = self.output_dir / 'master_table_o_award.xlsx'
        self.master_df.to_excel(excel_path, index=False)
        
        # ä¿å­˜åˆ—è¯´æ˜
        column_desc = {
            'Country': 'å›½å®¶ä»£ç ï¼ˆISO 3166-1 alpha-3ï¼‰',
            'Year': 'å¹´ä»½ï¼ˆ2016-2025ï¼‰',
            'Population_Million': 'äººå£ï¼ˆç™¾ä¸‡ï¼‰',
            'GDP_Billion_USD': 'GDPï¼ˆåäº¿ç¾å…ƒï¼Œ2023å¹´ï¼‰',
            'PPP_Factor': 'PPPè½¬æ¢å› å­ï¼ˆç›¸å¯¹äºUSDï¼‰',
            'AI_Publications': 'AIå‡ºç‰ˆç‰©æ•°é‡ï¼ˆåŸå§‹å€¼ï¼‰',
            'AI_High_Impact_Publications': 'AIé«˜å½±å“åŠ›å‡ºç‰ˆç‰©æ•°é‡',
            'AI_High_Impact_Ratio': 'é«˜å½±å“åŠ›å‡ºç‰ˆç‰©å æ¯”',
            'AI_VC_Investment': 'AIé£é™©æŠ•èµ„ï¼ˆç™¾ä¸‡ç¾å…ƒï¼Œåä¹‰å€¼ï¼‰',
            'AI_VC_Investment_Constant2020': 'AIé£é™©æŠ•èµ„ï¼ˆç™¾ä¸‡ç¾å…ƒï¼Œ2020ä¸å˜ä»·ï¼‰',
            'GenAI_VC_Investment': 'ç”Ÿæˆå¼AIé£é™©æŠ•èµ„ï¼ˆç™¾ä¸‡ç¾å…ƒï¼Œåä¹‰å€¼ï¼‰',
            'GenAI_VC_Investment_Constant2020': 'ç”Ÿæˆå¼AIé£é™©æŠ•èµ„ï¼ˆç™¾ä¸‡ç¾å…ƒï¼Œ2020ä¸å˜ä»·ï¼‰',
            'GitHub_AI_Projects': 'GitHub AIé¡¹ç›®æ•°',
            'Electricity_Production_TWh': 'ç”µåŠ›ç”Ÿäº§ï¼ˆTWhï¼‰',
            '*_log': 'å¯¹æ•°å˜æ¢å€¼ (log1p)',
            '*_lag1/2/3': 'æ»å1/2/3å¹´ç‰¹å¾',
            '*_YoY_Growth': 'å¹´åŒæ¯”å¢é•¿ç‡ï¼ˆ%ï¼‰',
            '*_PerCapita': 'äººå‡å€¼',
            '*_PPP': 'PPPè°ƒæ•´åçš„å€¼'
        }
        
        desc_path = self.output_dir / 'column_descriptions.json'
        with open(desc_path, 'w', encoding='utf-8') as f:
            json.dump(column_desc, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… ä¸»è¡¨å·²ä¿å­˜:")
        print(f"   - {csv_path}")
        print(f"   - {excel_path}")
        print(f"   - {desc_path}")
    
    def build(self):
        """æ‰§è¡Œå®Œæ•´çš„é¢„å¤„ç†æµç¨‹"""
        print("=" * 80)
        print("ğŸ† åæ•°æ¯ Bé¢˜ - Oå¥–çº§æ•°æ®é¢„å¤„ç†")
        print("=" * 80)
        
        # 1. æ·»åŠ å„ç±»æ•°æ®
        self.add_publication_data()
        self.add_vc_investment_data()
        self.add_github_data()
        self.add_energy_data()
        self.add_university_ranking_data()
        
        # 2. å¤„ç†ç»“æ„æ€§ç¼ºå¤±
        self.handle_structural_missing()
        
        # 3. å¯¹æ•°å˜æ¢
        self.apply_log_transformation()
        
        # 4. æ»åç‰¹å¾
        self.add_lag_features()
        
        # 5. äººå‡æŒ‡æ ‡
        self.add_per_capita_metrics()
        
        # 6. PPPè°ƒæ•´
        self.add_ppp_adjusted_metrics()
        
        # 7. ç”ŸæˆæŠ¥å‘Šå¹¶ä¿å­˜
        self.generate_quality_report()
        self.save_master_table()
        
        print("\n" + "=" * 80)
        print("âœ… Oå¥–çº§é¢„å¤„ç†å®Œæˆï¼")
        print("=" * 80)
        
        # æ˜¾ç¤ºç»Ÿè®¡æ‘˜è¦
        print(f"\nğŸ“Š ä¸»è¡¨ç»Ÿè®¡:")
        print(f"   - ç»´åº¦: {self.master_df.shape[0]} è¡Œ Ã— {self.master_df.shape[1]} åˆ—")
        print(f"   - å›½å®¶: {', '.join(TARGET_COUNTRIES)}")
        print(f"   - å¹´ä»½: {TARGET_YEARS[0]}-{TARGET_YEARS[-1]}")
        
        return self.master_df


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================

if __name__ == '__main__':
    # æ•°æ®ç›®å½•
    data_dir = r'd:\åæ•°æ¯\bé¢˜æ•°æ®æº'
    
    # æ„å»ºä¸»è¡¨
    builder = MasterDataFrameBuilder(data_dir)
    master_df = builder.build()
    
    # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
    print("\nğŸ“‹ ä¸»è¡¨é¢„è§ˆï¼ˆå‰10è¡Œï¼‰:")
    print(master_df.head(10).to_string())
    
    print("\nğŸ“‹ åˆ—ä¿¡æ¯:")
    print(master_df.dtypes.to_string())
