# -*- coding: utf-8 -*-
"""
åæ•°æ¯ Bé¢˜ - AIäººæ‰æ•°æ®é¢„å¤„ç†è„šæœ¬
============================================
é’ˆå¯¹ Supply, Mobility and Quality of AI Talents æ–‡ä»¶å¤¹
çš„æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œç”Ÿæˆå¯ä¸AIä¸»è¡¨åˆå¹¶çš„æ ‡å‡†åŒ–æ•°æ®

æ•°æ®ç‰¹ç‚¹ï¼š
- AIäººæ‰ä¾›ç»™ï¼šç ”ç©¶äººå‘˜å¯†åº¦ã€æŠ€æœ¯äººå‘˜å¯†åº¦
- äººæ‰åŸ¹å…»ï¼šé«˜ç­‰æ•™è‚²å…¥å­¦ç‡ã€STEMæ¯•ä¸šç”Ÿæ¯”ä¾‹ã€å­¦ä½å®Œæˆç‡
- æ•™è‚²æŠ•å…¥ï¼šæ•™è‚²æ”¯å‡ºå GDPæ¯”ä¾‹ã€é«˜ç­‰æ•™è‚²ç”Ÿå‡æ”¯å‡º
- äººå£åŸºç¡€ï¼šæ€»äººå£ã€åŠ³åŠ¨å¹´é¾„äººå£å æ¯”
- æ¥æºï¼šWorld Bankã€UNESCO UIS
- ä¸»è¦ä¸ºæ¯”ä¾‹æŒ‡æ ‡å’Œå¯†åº¦æŒ‡æ ‡ï¼ˆæ— éœ€å¤§è§„æ¨¡å¯¹æ•°å˜æ¢ï¼‰

è¾“å‡ºï¼š
- ai_talent_preprocessed.csv: é¢„å¤„ç†åçš„å®½è¡¨
- å¯ç›´æ¥ä¸ä¸»AIæ•°æ®è¡¨æŒ‰Country+Yearåˆå¹¶

ä½œè€…: åæ•°æ¯å‚èµ›é˜Ÿ
æ—¥æœŸ: 2026-01-17
"""

import pandas as pd
import numpy as np
from pathlib import Path
from scipy import stats
from scipy.interpolate import CubicSpline
from datetime import datetime
import warnings
import json

warnings.filterwarnings('ignore')

# ============================================================================
# å…¨å±€é…ç½®
# ============================================================================

BASE_DIR = Path(r"d:\åæ•°æ¯\Supply, Mobility and Quality of AI Talents")
MERGED_DATA_DIR = BASE_DIR / "merged_wide"
OUTPUT_DIR = BASE_DIR / "preprocessed"
OUTPUT_DIR.mkdir(exist_ok=True)

# ç›®æ ‡å›½å®¶ï¼ˆä¸ä¸»æ•°æ®é›†ä¸€è‡´ï¼‰
TARGET_COUNTRIES = ['USA', 'CHN', 'GBR', 'DEU', 'FRA', 'CAN', 'JPN', 'KOR', 'ARE', 'IND']

# å›½å®¶ä¸­æ–‡å
COUNTRY_CN = {
    'USA': 'ç¾å›½', 'CHN': 'ä¸­å›½', 'GBR': 'è‹±å›½', 'DEU': 'å¾·å›½',
    'FRA': 'æ³•å›½', 'CAN': 'åŠ æ‹¿å¤§', 'JPN': 'æ—¥æœ¬', 'KOR': 'éŸ©å›½',
    'ARE': 'é˜¿è”é…‹', 'IND': 'å°åº¦'
}

# ç›®æ ‡å¹´ä»½ï¼ˆä¸AIä¸»æ•°æ®å¯¹é½: 2016-2025ï¼‰
TARGET_YEARS = list(range(2016, 2026))

# æŒ‡æ ‡é…ç½®ï¼šç±»å‹ã€å•ä½ã€æ˜¯å¦å¯¹æ•°å˜æ¢ã€æè¿°
INDICATOR_CONFIG = {
    # å¯†åº¦å‹æŒ‡æ ‡
    'researchers_per_million': {
        'type': 'density', 'unit': 'per million', 'log_transform': False,
        'description': 'æ¯ç™¾ä¸‡äººç ”ç©¶äººå‘˜æ•°', 'priority': 'high'
    },
    'researchers_per_million_fte': {
        'type': 'density', 'unit': 'per million', 'log_transform': False,
        'description': 'æ¯ç™¾ä¸‡äººç ”ç©¶äººå‘˜æ•°(FTE)', 'priority': 'medium'
    },
    'technicians_per_million': {
        'type': 'density', 'unit': 'per million', 'log_transform': False,
        'description': 'æ¯ç™¾ä¸‡äººç ”å‘æŠ€æœ¯äººå‘˜æ•°', 'priority': 'medium'
    },
    
    # æ¯”ä¾‹æŒ‡æ ‡ï¼ˆç™¾åˆ†æ¯”ï¼‰
    'tertiary_gross_enrollment_pct': {
        'type': 'ratio', 'unit': '%', 'log_transform': False,
        'description': 'é«˜ç­‰æ•™è‚²æ¯›å…¥å­¦ç‡', 'priority': 'high'
    },
    'tertiary_female_share_pct': {
        'type': 'ratio', 'unit': '%', 'log_transform': False,
        'description': 'é«˜ç­‰æ•™è‚²å¥³æ€§å æ¯”', 'priority': 'low'
    },
    'education_expenditure_pct_gdp': {
        'type': 'ratio', 'unit': '%', 'log_transform': False,
        'description': 'æ•™è‚²æ”¯å‡ºå GDPæ¯”ä¾‹', 'priority': 'high'
    },
    'tertiary_spend_per_student_pct_gdp_pc': {
        'type': 'ratio', 'unit': '%', 'log_transform': False,
        'description': 'é«˜ç­‰æ•™è‚²ç”Ÿå‡æ”¯å‡ºå äººå‡GDPæ¯”ä¾‹', 'priority': 'medium'
    },
    'rd_expenditure_pct_gdp': {
        'type': 'ratio', 'unit': '%', 'log_transform': False,
        'description': 'R&Dæ”¯å‡ºå GDPæ¯”ä¾‹', 'priority': 'high'
    },
    'pop_15_64_pct': {
        'type': 'ratio', 'unit': '%', 'log_transform': False,
        'description': '15-64å²åŠ³åŠ¨å¹´é¾„äººå£å æ¯”', 'priority': 'high'
    },
    'stem_graduates_pct': {
        'type': 'ratio', 'unit': '%', 'log_transform': False,
        'description': 'STEMæ¯•ä¸šç”Ÿå æ¯”', 'priority': 'high'
    },
    'tertiary_completion_25_34_pct': {
        'type': 'ratio', 'unit': '%', 'log_transform': False,
        'description': '25-34å²é«˜ç­‰æ•™è‚²å®Œæˆç‡', 'priority': 'medium'
    },
    
    # ç»å¯¹æ•°é‡æŒ‡æ ‡ï¼ˆéœ€è¦å¯¹æ•°å˜æ¢ï¼‰
    'population_total': {
        'type': 'count', 'unit': 'count', 'log_transform': True,
        'description': 'æ€»äººå£', 'priority': 'high'
    },
    'tertiary_enrollment_total': {
        'type': 'count', 'unit': 'count', 'log_transform': True,
        'description': 'é«˜ç­‰æ•™è‚²åœ¨æ ¡ç”Ÿæ€»æ•°', 'priority': 'medium'
    }
}


# ============================================================================
# å·¥å…·å‡½æ•°
# ============================================================================

def holt_winters_forecast(series: pd.Series, periods: int = 2) -> np.ndarray:
    """
    Holt-WintersæŒ‡æ•°å¹³æ»‘å¤–æ¨
    ç”¨äºå°¾éƒ¨ç¼ºå¤±ï¼ˆå¦‚é¢„æµ‹2024-2025å¹´æ•°æ®ï¼‰
    """
    try:
        from statsmodels.tsa.holtwinters import ExponentialSmoothing
        
        clean_series = series.dropna()
        if len(clean_series) < 4:
            return linear_extrapolate(clean_series, periods)
        
        try:
            # äººæ‰æ•°æ®é€šå¸¸å¹³ç¨³å¢é•¿ï¼Œä½¿ç”¨åŠ æ³•è¶‹åŠ¿+é˜»å°¼
            model = ExponentialSmoothing(
                clean_series.values,
                trend='add',
                seasonal=None,
                damped_trend=True
            )
            fitted = model.fit(optimized=True)
            forecast = fitted.forecast(periods)
            
            # å¯¹äºæ¯”ä¾‹æŒ‡æ ‡ï¼Œç¡®ä¿åœ¨åˆç†èŒƒå›´å†…
            return np.clip(forecast, 0, 200)  # å…¥å­¦ç‡å¯èƒ½è¶…è¿‡100%
        except:
            return linear_extrapolate(clean_series, periods)
    except ImportError:
        return linear_extrapolate(series.dropna(), periods)


def linear_extrapolate(series: pd.Series, periods: int) -> np.ndarray:
    """ç®€å•çº¿æ€§å¤–æ¨"""
    if len(series) < 2:
        return np.array([series.iloc[-1]] * periods) if len(series) > 0 else np.array([np.nan] * periods)
    
    x = np.arange(len(series))
    y = series.values
    slope, intercept, _, _, _ = stats.linregress(x, y)
    
    future_x = np.arange(len(series), len(series) + periods)
    forecast = slope * future_x + intercept
    return forecast


def cubic_spline_interpolate(df: pd.DataFrame, country: str, 
                             year_col: str, value_col: str) -> pd.DataFrame:
    """
    ä¸‰æ¬¡æ ·æ¡æ’å€¼å¡«è¡¥ä¸­é—´ç¼ºå¤±å€¼
    """
    country_data = df[df['country_code'] == country].copy()
    country_data = country_data.sort_values(year_col)
    
    if len(country_data) < 4:
        return df
    
    valid_mask = country_data[value_col].notna()
    if valid_mask.sum() < 4:
        return df
    
    years_valid = country_data.loc[valid_mask, year_col].values
    values_valid = country_data.loc[valid_mask, value_col].values
    
    try:
        cs = CubicSpline(years_valid, values_valid)
        
        missing_mask = country_data[value_col].isna()
        if missing_mask.any():
            missing_years = country_data.loc[missing_mask, year_col].values
            # åªæ’å€¼èŒƒå›´å†…çš„å¹´ä»½
            for year in missing_years:
                if years_valid.min() <= year <= years_valid.max():
                    idx = country_data[country_data[year_col] == year].index[0]
                    interpolated_value = float(cs(year))
                    # ç¡®ä¿æ’å€¼ç»“æœåˆç†
                    if value_col.endswith('_pct') and interpolated_value < 0:
                        interpolated_value = 0
                    df.loc[idx, value_col] = interpolated_value
    except Exception as e:
        pass
    
    return df


def log_transform_column(series: pd.Series, check_skewness: bool = True) -> tuple:
    """
    å¯¹æ•°å˜æ¢ï¼ˆlog1pï¼‰
    è¿”å›: (å˜æ¢åçš„series, æ˜¯å¦å˜æ¢, åŸå§‹ååº¦)
    """
    clean = series.dropna()
    if len(clean) < 10:
        return series, False, np.nan
    
    skewness = clean.skew()
    
    if check_skewness and abs(skewness) > 1.5:
        transformed = np.log1p(series.clip(lower=0))
        return transformed, True, skewness
    
    return series, False, skewness


# ============================================================================
# ä¸»é¢„å¤„ç†ç±»
# ============================================================================

class AITalentPreprocessor:
    """AIäººæ‰æ•°æ®é¢„å¤„ç†å™¨"""
    
    def __init__(self):
        self.processing_log = []
        self.master_df = None
        
    def log(self, message: str):
        """è®°å½•å¤„ç†æ—¥å¿—"""
        self.processing_log.append({
            'timestamp': datetime.now().isoformat(),
            'message': message
        })
        print(f"  ğŸ“ {message}")
    
    def load_source_data(self) -> pd.DataFrame:
        """åŠ è½½æºæ•°æ®"""
        print("\n" + "=" * 80)
        print("ğŸ“‚ 1. åŠ è½½æ•°æ®")
        print("=" * 80)
        
        source_file = MERGED_DATA_DIR / "ai_talent_wide.csv"
        if not source_file.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æºæ•°æ®æ–‡ä»¶: {source_file}")
        
        df = pd.read_csv(source_file)
        self.log(f"å·²åŠ è½½ {len(df)} è¡Œ, {len(df.columns)} åˆ—")
        
        # ç­›é€‰ç›®æ ‡å›½å®¶
        df = df[df['country_code'].isin(TARGET_COUNTRIES)]
        self.log(f"ç­›é€‰ç›®æ ‡å›½å®¶å: {len(df)} è¡Œ")
        
        # æ˜¾ç¤ºå¹´ä»½èŒƒå›´
        self.log(f"åŸå§‹å¹´ä»½èŒƒå›´: {df['year'].min()}-{df['year'].max()}")
        
        return df
    
    def create_base_framework(self) -> pd.DataFrame:
        """åˆ›å»ºä¸»è¡¨åŸºç¡€æ¡†æ¶ï¼ˆç¡®ä¿æ‰€æœ‰å›½å®¶-å¹´ä»½ç»„åˆéƒ½å­˜åœ¨ï¼‰"""
        print("\n" + "=" * 80)
        print("ğŸ—ï¸ 2. åˆ›å»ºåŸºç¡€æ¡†æ¶")
        print("=" * 80)
        
        rows = []
        for country in TARGET_COUNTRIES:
            for year in TARGET_YEARS:
                rows.append({
                    'country_code': country,
                    'year': year,
                    'country_cn': COUNTRY_CN.get(country, country)
                })
        
        framework = pd.DataFrame(rows)
        self.log(f"åˆ›å»º {len(framework)} è¡ŒåŸºç¡€æ¡†æ¶ ({len(TARGET_COUNTRIES)}å›½ Ã— {len(TARGET_YEARS)}å¹´)")
        
        return framework
    
    def merge_with_framework(self, framework: pd.DataFrame, 
                             source_df: pd.DataFrame) -> pd.DataFrame:
        """å°†æºæ•°æ®åˆå¹¶åˆ°åŸºç¡€æ¡†æ¶"""
        # è·å–æŒ‡æ ‡åˆ—ï¼ˆæ’é™¤å…ƒæ•°æ®åˆ—ï¼‰
        meta_cols = ['country_code', 'year', 'country_cn', 'country_en']
        indicator_cols = [c for c in source_df.columns if c not in meta_cols]
        
        # ç­›é€‰ç›®æ ‡å¹´ä»½
        source_df = source_df[source_df['year'].isin(TARGET_YEARS)]
        
        # åˆå¹¶
        merge_cols = ['country_code', 'year'] + indicator_cols
        available_cols = [c for c in merge_cols if c in source_df.columns]
        
        result = framework.merge(
            source_df[available_cols],
            on=['country_code', 'year'],
            how='left'
        )
        
        self.log(f"å·²åˆå¹¶ {len(indicator_cols)} ä¸ªæŒ‡æ ‡åˆ—")
        
        return result
    
    def interpolate_missing(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ’å€¼å¤„ç†ä¸­é—´ç¼ºå¤±å€¼ï¼ˆä¸‰æ¬¡æ ·æ¡ï¼‰"""
        print("\n" + "=" * 80)
        print("ğŸ”§ 3. ç¼ºå¤±å€¼æ’å€¼ï¼ˆä¸‰æ¬¡æ ·æ¡ï¼‰")
        print("=" * 80)
        
        meta_cols = ['country_code', 'year', 'country_cn']
        indicator_cols = [c for c in df.columns if c not in meta_cols]
        
        interpolated_count = 0
        
        for col in indicator_cols:
            for country in TARGET_COUNTRIES:
                country_mask = df['country_code'] == country
                country_data = df[country_mask].sort_values('year')
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ä¸­é—´ç¼ºå¤±ï¼ˆä¸¤ç«¯æœ‰å€¼ï¼Œä¸­é—´ç¼ºå¤±ï¼‰
                values = country_data[col].values
                try:
                    valid_indices = np.where(~pd.isna(values))[0]
                except:
                    continue
                
                if len(valid_indices) < 4:
                    continue
                
                # æ£€æµ‹ä¸­é—´ç¼ºå¤±
                first_valid = valid_indices[0]
                last_valid = valid_indices[-1]
                
                has_middle_missing = False
                for i in range(first_valid + 1, last_valid):
                    if pd.isna(values[i]):
                        has_middle_missing = True
                        break
                
                if has_middle_missing:
                    df = cubic_spline_interpolate(df, country, 'year', col)
                    interpolated_count += 1
        
        self.log(f"å®Œæˆ {interpolated_count} æ¬¡ä¸‰æ¬¡æ ·æ¡æ’å€¼")
        
        return df
    
    def extrapolate_tail_missing(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¤–æ¨å°¾éƒ¨ç¼ºå¤±å€¼ï¼ˆ2024-2025ï¼‰"""
        print("\n" + "=" * 80)
        print("ğŸ“ˆ 4. å°¾éƒ¨ç¼ºå¤±å¤–æ¨ï¼ˆHolt-Wintersï¼‰")
        print("=" * 80)
        
        meta_cols = ['country_code', 'year', 'country_cn']
        indicator_cols = [c for c in df.columns if c not in meta_cols]
        
        extrapolated_count = 0
        
        for col in indicator_cols:
            for country in TARGET_COUNTRIES:
                country_mask = df['country_code'] == country
                country_data = df[country_mask].sort_values('year')
                
                # æ£€æŸ¥å“ªäº›å¹´ä»½éœ€è¦å¤–æ¨
                missing_years = []
                for year in [2023, 2024, 2025]:
                    year_data = country_data[country_data['year'] == year]
                    if len(year_data) == 0 or pd.isna(year_data[col].values[0]):
                        missing_years.append(year)
                
                if not missing_years:
                    continue
                
                # è·å–å†å²æ•°æ®è¿›è¡Œå¤–æ¨
                min_missing_year = min(missing_years)
                historical = country_data[country_data['year'] < min_missing_year][col].dropna()
                if len(historical) < 3:
                    continue
                
                # å¤–æ¨
                periods = len(missing_years)
                try:
                    forecast = holt_winters_forecast(historical, periods)
                    
                    for i, year in enumerate(missing_years):
                        idx = df[(df['country_code'] == country) & (df['year'] == year)].index
                        if len(idx) > 0:
                            df.loc[idx[0], col] = forecast[i]
                            extrapolated_count += 1
                except Exception as e:
                    continue
        
        self.log(f"å®Œæˆ {extrapolated_count} ä¸ªå€¼çš„Holt-Winterså¤–æ¨")
        
        return df
    
    def apply_log_transform(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¯¹ç»å¯¹æ•°é‡æŒ‡æ ‡åº”ç”¨å¯¹æ•°å˜æ¢"""
        print("\n" + "=" * 80)
        print("ğŸ“Š 5. å¯¹æ•°å˜æ¢ï¼ˆä»…ç»å¯¹æ•°é‡æŒ‡æ ‡ï¼‰")
        print("=" * 80)
        
        transform_cols = [col for col, config in INDICATOR_CONFIG.items() 
                         if config.get('log_transform', False) and col in df.columns]
        
        for col in transform_cols:
            transformed, did_transform, skewness = log_transform_column(df[col])
            if did_transform:
                df[f'{col}_log'] = transformed
                self.log(f"{col}: ååº¦={skewness:.2f}, å·²æ·»åŠ å¯¹æ•°å˜æ¢åˆ—")
            else:
                # å¯¹äºäººå£ç­‰ï¼Œå³ä½¿ååº¦ä¸é«˜ä¹Ÿåšå¯¹æ•°å˜æ¢
                if col in ['population_total', 'tertiary_enrollment_total']:
                    df[f'{col}_log'] = np.log1p(df[col].clip(lower=0))
                    self.log(f"{col}: å¼ºåˆ¶å¯¹æ•°å˜æ¢ï¼ˆç»å¯¹æ•°é‡æŒ‡æ ‡ï¼‰")
        
        return df
    
    def add_derived_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ·»åŠ è¡ç”Ÿç‰¹å¾"""
        print("\n" + "=" * 80)
        print("ğŸ”¨ 6. æ„å»ºè¡ç”Ÿç‰¹å¾")
        print("=" * 80)
        
        # æŒ‰å›½å®¶æ’åºä»¥è®¡ç®—æ»åå’Œå¢é•¿ç‡
        df = df.sort_values(['country_code', 'year'])
        
        # 1. å¹´å¢é•¿ç‡
        growth_cols = ['researchers_per_million', 'tertiary_gross_enrollment_pct', 
                       'rd_expenditure_pct_gdp', 'education_expenditure_pct_gdp']
        for col in growth_cols:
            if col in df.columns:
                growth_col = f'{col}_YoY_Growth'
                df[growth_col] = df.groupby('country_code')[col].pct_change() * 100
                self.log(f"å·²è®¡ç®— {col} å¹´å¢é•¿ç‡")
        
        # 2. 3å¹´ç§»åŠ¨å¹³å‡ï¼ˆå¹³æ»‘æ³¢åŠ¨ï¼‰
        ma_cols = ['researchers_per_million', 'tertiary_gross_enrollment_pct']
        for col in ma_cols:
            if col in df.columns:
                ma_col = f'{col}_MA3'
                df[ma_col] = df.groupby('country_code')[col].transform(
                    lambda x: x.rolling(window=3, min_periods=2).mean()
                )
                self.log(f"å·²è®¡ç®— {col} 3å¹´ç§»åŠ¨å¹³å‡")
        
        # 3. æ»åç‰¹å¾ï¼ˆç”¨äºä¸AIäº§å‡ºçš„å› æœåˆ†æï¼‰
        lag_cols = ['researchers_per_million', 'tertiary_gross_enrollment_pct', 
                    'education_expenditure_pct_gdp', 'stem_graduates_pct']
        for col in lag_cols:
            if col in df.columns:
                for lag in [1, 2, 3]:
                    lag_col = f'{col}_lag{lag}'
                    df[lag_col] = df.groupby('country_code')[col].shift(lag)
                self.log(f"å·²ä¸º {col} æ·»åŠ 1-3å¹´æ»åç‰¹å¾")
        
        # 4. äººæ‰ç»¼åˆæŒ‡æ•°
        talent_components = ['researchers_per_million', 'tertiary_gross_enrollment_pct',
                            'rd_expenditure_pct_gdp', 'education_expenditure_pct_gdp']
        available_components = [c for c in talent_components if c in df.columns]
        
        if len(available_components) >= 3:
            # Min-Maxæ ‡å‡†åŒ–
            for col in available_components:
                min_val = df[col].min()
                max_val = df[col].max()
                if max_val > min_val:
                    df[f'{col}_normalized'] = (df[col] - min_val) / (max_val - min_val)
            
            # åŠ æƒå¹³å‡
            norm_cols = [f'{c}_normalized' for c in available_components]
            df['AI_Talent_Index'] = df[norm_cols].mean(axis=1)
            self.log(f"å·²è®¡ç®—AIäººæ‰ç»¼åˆæŒ‡æ•°ï¼ˆåŸºäº{len(available_components)}ä¸ªæŒ‡æ ‡ï¼‰")
            
            # åˆ é™¤ä¸­é—´æ ‡å‡†åŒ–åˆ—
            df = df.drop(columns=norm_cols, errors='ignore')
        
        # 5. ç ”ç©¶äººå‘˜å¢é•¿åŠ¨åŠ›æŒ‡æ ‡ï¼ˆç ”ç©¶äººå‘˜å¢é€Ÿ vs æ•™è‚²æŠ•å…¥ï¼‰
        if 'researchers_per_million' in df.columns and 'education_expenditure_pct_gdp' in df.columns:
            df['researcher_growth_efficiency'] = (
                df['researchers_per_million_YoY_Growth'] / 
                df['education_expenditure_pct_gdp'].replace(0, np.nan)
            )
            self.log("å·²è®¡ç®—ç ”ç©¶äººå‘˜å¢é•¿æ•ˆç‡æŒ‡æ ‡")
        
        # 6. äººæ‰å¯†åº¦ï¼ˆç ”ç©¶äººå‘˜/åŠ³åŠ¨å¹´é¾„äººå£ï¼‰
        if 'researchers_per_million' in df.columns and 'pop_15_64_pct' in df.columns:
            df['researcher_density_adjusted'] = (
                df['researchers_per_million'] * df['pop_15_64_pct'] / 100
            )
            self.log("å·²è®¡ç®—åŠ³åŠ¨äººå£è°ƒæ•´åçš„ç ”ç©¶äººå‘˜å¯†åº¦")
        
        return df
    
    def handle_sparse_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¤„ç†ç¨€ç–æŒ‡æ ‡"""
        print("\n" + "=" * 80)
        print("ğŸ” 7. ç¨€ç–æŒ‡æ ‡å¤„ç†")
        print("=" * 80)
        
        meta_cols = ['country_code', 'year', 'country_cn']
        indicator_cols = [c for c in df.columns if c not in meta_cols 
                         and not c.endswith('_normalized') 
                         and not c.endswith('_YoY_Growth')
                         and not c.endswith('_MA3')
                         and not c.endswith('_lag1')
                         and not c.endswith('_lag2')
                         and not c.endswith('_lag3')
                         and not c.endswith('_log')]
        
        sparse_indicators = []
        for col in indicator_cols:
            coverage = df[col].notna().sum() / len(df) * 100
            if coverage < 20:
                sparse_indicators.append((col, coverage))
        
        if sparse_indicators:
            self.log(f"å‘ç° {len(sparse_indicators)} ä¸ªç¨€ç–æŒ‡æ ‡ (è¦†ç›–ç‡<20%):")
            for col, coverage in sparse_indicators:
                self.log(f"   â€¢ {col}: {coverage:.1f}% è¦†ç›–ç‡")
            
            # ä¸åˆ é™¤ï¼Œä½†æ ‡è®°
            self.log("   â„¹ï¸ ç¨€ç–æŒ‡æ ‡å·²ä¿ç•™ï¼Œå»ºæ¨¡æ—¶è¯·è€ƒè™‘é™æƒæˆ–å‰”é™¤")
        else:
            self.log("æ— ç¨€ç–æŒ‡æ ‡")
        
        return df
    
    def validate_output(self, df: pd.DataFrame) -> dict:
        """éªŒè¯è¾“å‡ºæ•°æ®è´¨é‡"""
        print("\n" + "=" * 80)
        print("âœ… 8. è¾“å‡ºéªŒè¯")
        print("=" * 80)
        
        validation = {
            'total_rows': len(df),
            'total_columns': len(df.columns),
            'countries': df['country_code'].nunique(),
            'years': df['year'].nunique(),
            'year_range': (int(df['year'].min()), int(df['year'].max())),
            'missing_summary': {}
        }
        
        # æ£€æŸ¥æ¯ä¸ªæŒ‡æ ‡çš„ç¼ºå¤±ç‡
        meta_cols = ['country_code', 'year', 'country_cn']
        indicator_cols = [c for c in df.columns if c not in meta_cols]
        
        for col in indicator_cols:
            missing_pct = df[col].isna().sum() / len(df) * 100
            validation['missing_summary'][col] = missing_pct
        
        # æ‰“å°éªŒè¯ç»“æœ
        print(f"   æ€»è¡Œæ•°: {validation['total_rows']}")
        print(f"   æ€»åˆ—æ•°: {validation['total_columns']}")
        print(f"   å›½å®¶æ•°: {validation['countries']}")
        print(f"   å¹´ä»½èŒƒå›´: {validation['year_range'][0]}-{validation['year_range'][1]}")
        
        # æ£€æŸ¥æ ¸å¿ƒæŒ‡æ ‡è¦†ç›–
        core_indicators = ['researchers_per_million', 'tertiary_gross_enrollment_pct',
                          'rd_expenditure_pct_gdp', 'pop_15_64_pct']
        print(f"\n   æ ¸å¿ƒæŒ‡æ ‡è¦†ç›–ç‡:")
        for col in core_indicators:
            if col in validation['missing_summary']:
                coverage = 100 - validation['missing_summary'][col]
                status = "âœ…" if coverage > 70 else "âš ï¸" if coverage > 40 else "âŒ"
                print(f"      {status} {col}: {coverage:.1f}%")
        
        # æ£€æŸ¥é«˜ç¼ºå¤±ç‡æŒ‡æ ‡
        high_missing = {k: v for k, v in validation['missing_summary'].items() if v > 50}
        if high_missing:
            print(f"\n   âš ï¸ é«˜ç¼ºå¤±ç‡æŒ‡æ ‡ (>50%):")
            for col, pct in sorted(high_missing.items(), key=lambda x: -x[1])[:5]:
                print(f"      â€¢ {col}: {pct:.1f}%")
        
        return validation
    
    def save_output(self, df: pd.DataFrame, validation: dict):
        """ä¿å­˜é¢„å¤„ç†ç»“æœ"""
        print("\n" + "=" * 80)
        print("ğŸ’¾ 9. ä¿å­˜è¾“å‡º")
        print("=" * 80)
        
        # ä¿å­˜CSV
        csv_path = OUTPUT_DIR / "ai_talent_preprocessed.csv"
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        self.log(f"CSVå·²ä¿å­˜: {csv_path}")
        
        # ä¿å­˜Excel
        excel_path = OUTPUT_DIR / "ai_talent_preprocessed.xlsx"
        df.to_excel(excel_path, index=False)
        self.log(f"Excelå·²ä¿å­˜: {excel_path}")
        
        # ä¿å­˜åˆ—è¯´æ˜
        column_desc = {
            'country_code': 'å›½å®¶ä»£ç ï¼ˆISO 3166-1 alpha-3ï¼‰',
            'year': 'å¹´ä»½',
            'country_cn': 'å›½å®¶ä¸­æ–‡å',
            'researchers_per_million': 'æ¯ç™¾ä¸‡äººç ”ç©¶äººå‘˜æ•°',
            'researchers_per_million_fte': 'æ¯ç™¾ä¸‡äººç ”ç©¶äººå‘˜æ•°(FTE)',
            'technicians_per_million': 'æ¯ç™¾ä¸‡äººç ”å‘æŠ€æœ¯äººå‘˜æ•°',
            'tertiary_gross_enrollment_pct': 'é«˜ç­‰æ•™è‚²æ¯›å…¥å­¦ç‡(%)',
            'education_expenditure_pct_gdp': 'æ•™è‚²æ”¯å‡ºå GDPæ¯”ä¾‹(%)',
            'rd_expenditure_pct_gdp': 'R&Dæ”¯å‡ºå GDPæ¯”ä¾‹(%)',
            'pop_15_64_pct': '15-64å²äººå£å æ¯”(%)',
            'stem_graduates_pct': 'STEMæ¯•ä¸šç”Ÿå æ¯”(%)',
            'tertiary_completion_25_34_pct': '25-34å²é«˜ç­‰æ•™è‚²å®Œæˆç‡(%)',
            'population_total': 'æ€»äººå£',
            'population_total_log': 'æ€»äººå£(å¯¹æ•°)',
            'tertiary_enrollment_total': 'é«˜ç­‰æ•™è‚²åœ¨æ ¡ç”Ÿæ€»æ•°',
            'tertiary_enrollment_total_log': 'é«˜ç­‰æ•™è‚²åœ¨æ ¡ç”Ÿæ€»æ•°(å¯¹æ•°)',
            '*_YoY_Growth': 'å¹´åŒæ¯”å¢é•¿ç‡(%)',
            '*_MA3': '3å¹´ç§»åŠ¨å¹³å‡',
            '*_lag1/2/3': 'æ»å1/2/3å¹´ç‰¹å¾',
            'AI_Talent_Index': 'AIäººæ‰ç»¼åˆæŒ‡æ•°(0-1æ ‡å‡†åŒ–)',
            'researcher_density_adjusted': 'åŠ³åŠ¨äººå£è°ƒæ•´åçš„ç ”ç©¶äººå‘˜å¯†åº¦'
        }
        
        desc_path = OUTPUT_DIR / "column_descriptions.json"
        with open(desc_path, 'w', encoding='utf-8') as f:
            json.dump(column_desc, f, ensure_ascii=False, indent=2)
        self.log(f"åˆ—è¯´æ˜å·²ä¿å­˜: {desc_path}")
        
        # ä¿å­˜å¤„ç†æ—¥å¿—
        log_path = OUTPUT_DIR / "preprocessing_log.json"
        with open(log_path, 'w', encoding='utf-8') as f:
            json.dump(self.processing_log, f, ensure_ascii=False, indent=2)
        self.log(f"å¤„ç†æ—¥å¿—å·²ä¿å­˜: {log_path}")
        
        # ä¿å­˜éªŒè¯æŠ¥å‘Š
        validation_path = OUTPUT_DIR / "validation_report.json"
        
        def convert(obj):
            if isinstance(obj, (np.integer, np.int64)):
                return int(obj)
            elif isinstance(obj, (np.floating, np.float64)):
                return float(obj)
            return obj
        
        validation_converted = {k: convert(v) if not isinstance(v, dict) 
                               else {kk: convert(vv) for kk, vv in v.items()} 
                               for k, v in validation.items()}
        
        with open(validation_path, 'w', encoding='utf-8') as f:
            json.dump(validation_converted, f, ensure_ascii=False, indent=2)
        self.log(f"éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {validation_path}")
    
    def run(self):
        """æ‰§è¡Œå®Œæ•´é¢„å¤„ç†æµç¨‹"""
        print("=" * 100)
        print("ğŸ“ åæ•°æ¯ Bé¢˜ - AIäººæ‰æ•°æ®é¢„å¤„ç†")
        print("=" * 100)
        print(f"æ‰§è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # 1. åŠ è½½æ•°æ®
        source_df = self.load_source_data()
        
        # 2. åˆ›å»ºåŸºç¡€æ¡†æ¶
        framework = self.create_base_framework()
        
        # 3. åˆå¹¶æ•°æ®
        df = self.merge_with_framework(framework, source_df)
        
        # 4. æ’å€¼ä¸­é—´ç¼ºå¤±
        df = self.interpolate_missing(df)
        
        # 5. å¤–æ¨å°¾éƒ¨ç¼ºå¤±
        df = self.extrapolate_tail_missing(df)
        
        # 6. å¯¹æ•°å˜æ¢
        df = self.apply_log_transform(df)
        
        # 7. è¡ç”Ÿç‰¹å¾
        df = self.add_derived_features(df)
        
        # 8. å¤„ç†ç¨€ç–æŒ‡æ ‡
        df = self.handle_sparse_indicators(df)
        
        # 9. éªŒè¯
        validation = self.validate_output(df)
        
        # 10. ä¿å­˜
        self.save_output(df, validation)
        
        self.master_df = df
        
        print("\n" + "=" * 100)
        print("âœ… AIäººæ‰æ•°æ®é¢„å¤„ç†å®Œæˆ!")
        print("=" * 100)
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        print(f"\nğŸ“Š è¾“å‡ºç»Ÿè®¡:")
        print(f"   - ç»´åº¦: {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")
        print(f"   - å›½å®¶: {', '.join(TARGET_COUNTRIES)}")
        print(f"   - å¹´ä»½: {TARGET_YEARS[0]}-{TARGET_YEARS[-1]}")
        print(f"   - è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
        
        return df


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================

def main():
    """ä¸»å‡½æ•°"""
    preprocessor = AITalentPreprocessor()
    df = preprocessor.run()
    
    # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
    print("\nğŸ“‹ è¾“å‡ºæ•°æ®é¢„è§ˆï¼ˆå‰5è¡Œï¼Œæ ¸å¿ƒåˆ—ï¼‰:")
    core_cols = ['country_code', 'year', 'researchers_per_million', 
                 'tertiary_gross_enrollment_pct', 'rd_expenditure_pct_gdp', 
                 'AI_Talent_Index']
    display_cols = [c for c in core_cols if c in df.columns]
    print(df[display_cols].head().to_string())


if __name__ == "__main__":
    main()
