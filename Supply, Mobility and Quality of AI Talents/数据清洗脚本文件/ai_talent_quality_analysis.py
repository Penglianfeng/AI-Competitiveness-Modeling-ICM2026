# -*- coding: utf-8 -*-
"""
åæ•°æ¯ Bé¢˜ - AIäººæ‰æ•°æ®è´¨é‡åˆ†æè„šæœ¬
============================================
é’ˆå¯¹ Supply, Mobility and Quality of AI Talents æ–‡ä»¶å¤¹
çš„æ•°æ®è¿›è¡Œæ·±åº¦è´¨é‡åˆ†æ

æ•°æ®ç‰¹ç‚¹ï¼š
- AIäººæ‰ä¾›ç»™ï¼šç ”ç©¶äººå‘˜å¯†åº¦ã€æŠ€æœ¯äººå‘˜å¯†åº¦
- äººæ‰åŸ¹å…»ï¼šé«˜ç­‰æ•™è‚²å…¥å­¦ç‡ã€STEMæ¯•ä¸šç”Ÿæ¯”ä¾‹ã€å­¦ä½å®Œæˆç‡
- æ•™è‚²æŠ•å…¥ï¼šæ•™è‚²æ”¯å‡ºå GDPæ¯”ä¾‹ã€é«˜ç­‰æ•™è‚²ç”Ÿå‡æ”¯å‡º
- äººå£åŸºç¡€ï¼šæ€»äººå£ã€åŠ³åŠ¨å¹´é¾„äººå£å æ¯”
- æ¥æºï¼šWorld Bankã€UNESCO UIS

ä½œè€…: åæ•°æ¯å‚èµ›é˜Ÿ
æ—¥æœŸ: 2026-01-17
"""

import pandas as pd
import numpy as np
import os
from pathlib import Path
from datetime import datetime
import warnings
import json
warnings.filterwarnings('ignore')

# ===================== é…ç½® =====================
BASE_DIR = Path(r"d:\åæ•°æ¯\Supply, Mobility and Quality of AI Talents")
MERGED_DATA_DIR = BASE_DIR / "merged_wide"
PROCESSED_DIR = BASE_DIR / "ai_talent_data_v2" / "processed"
OUTPUT_DIR = BASE_DIR / "preprocessed"
OUTPUT_DIR.mkdir(exist_ok=True)

# ç›®æ ‡å›½å®¶ï¼ˆä¸ä¸»æ•°æ®é›†ä¿æŒä¸€è‡´ï¼‰
TARGET_COUNTRIES = {
    'USA': 'ç¾å›½', 'CHN': 'ä¸­å›½', 'GBR': 'è‹±å›½', 'DEU': 'å¾·å›½', 
    'FRA': 'æ³•å›½', 'CAN': 'åŠ æ‹¿å¤§', 'JPN': 'æ—¥æœ¬', 'KOR': 'éŸ©å›½', 
    'ARE': 'é˜¿è”é…‹', 'IND': 'å°åº¦'
}

# ç›®æ ‡å¹´ä»½èŒƒå›´
TARGET_YEARS = list(range(2015, 2026))
FOCUS_YEARS = list(range(2016, 2026))  # é‡ç‚¹åˆ†æå¹´ä»½ï¼ˆä¸AIä¸»æ•°æ®å¯¹é½ï¼‰

# æŒ‡æ ‡åˆ†ç±»ï¼ˆç”¨äºç¡®å®šå¤„ç†æ–¹å¼ï¼‰
INDICATOR_CATEGORIES = {
    'density': [  # å¯†åº¦å‹æŒ‡æ ‡ï¼ˆæ¯ç™¾ä¸‡äººï¼‰
        'researchers_per_million',
        'researchers_per_million_fte',
        'technicians_per_million',
    ],
    'ratio_pct': [  # æ¯”ä¾‹/ç™¾åˆ†æ¯”æŒ‡æ ‡ï¼Œæ— éœ€å¯¹æ•°å˜æ¢
        'tertiary_gross_enrollment_pct',
        'tertiary_female_share_pct',
        'education_expenditure_pct_gdp',
        'tertiary_spend_per_student_pct_gdp_pc',
        'rd_expenditure_pct_gdp',
        'pop_15_64_pct',
        'stem_graduates_pct',
        'tertiary_completion_25_34_pct',
    ],
    'count_absolute': [  # ç»å¯¹æ•°é‡æŒ‡æ ‡ï¼Œå¯èƒ½éœ€è¦å¯¹æ•°å˜æ¢
        'population_total',
        'tertiary_enrollment_total',
    ]
}

# æŒ‡æ ‡ä¸­æ–‡åç§°
INDICATOR_CN = {
    'researchers_per_million': 'æ¯ç™¾ä¸‡äººç ”ç©¶äººå‘˜æ•°',
    'researchers_per_million_fte': 'æ¯ç™¾ä¸‡äººç ”ç©¶äººå‘˜æ•°(FTE)',
    'technicians_per_million': 'æ¯ç™¾ä¸‡äººç ”å‘æŠ€æœ¯äººå‘˜æ•°',
    'tertiary_gross_enrollment_pct': 'é«˜ç­‰æ•™è‚²æ¯›å…¥å­¦ç‡(%)',
    'tertiary_female_share_pct': 'é«˜ç­‰æ•™è‚²å¥³æ€§å æ¯”(%)',
    'education_expenditure_pct_gdp': 'æ•™è‚²æ”¯å‡ºå GDPæ¯”ä¾‹(%)',
    'tertiary_spend_per_student_pct_gdp_pc': 'é«˜ç­‰æ•™è‚²ç”Ÿå‡æ”¯å‡ºå äººå‡GDPæ¯”ä¾‹(%)',
    'rd_expenditure_pct_gdp': 'R&Dæ”¯å‡ºå GDPæ¯”ä¾‹(%)',
    'pop_15_64_pct': '15-64å²äººå£å æ¯”(%)',
    'stem_graduates_pct': 'STEMæ¯•ä¸šç”Ÿå æ¯”(%)',
    'tertiary_completion_25_34_pct': '25-34å²é«˜ç­‰æ•™è‚²å®Œæˆç‡(%)',
    'population_total': 'æ€»äººå£',
    'tertiary_enrollment_total': 'é«˜ç­‰æ•™è‚²åœ¨æ ¡ç”Ÿæ€»æ•°',
}


class AITalentQualityAnalyzer:
    """AIäººæ‰æ•°æ®è´¨é‡åˆ†æå™¨"""
    
    def __init__(self):
        self.issues = []
        self.warnings = []
        self.recommendations = []
        self.analysis_results = {}
        
    def run_full_analysis(self):
        """æ‰§è¡Œå®Œæ•´åˆ†æ"""
        print("=" * 100)
        print("ğŸ“ åæ•°æ¯ Bé¢˜ - AIäººæ‰æ•°æ®è´¨é‡åˆ†ææŠ¥å‘Š")
        print("=" * 100)
        print(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"æ•°æ®ç›®å½•: {BASE_DIR}")
        print()
        
        # 1. æ•°æ®æºæ¦‚è§ˆ
        self.analyze_data_sources()
        
        # 2. ä¸»æ•°æ®é›†åˆ†æ
        self.analyze_main_dataset()
        
        # 3. æ—¶é—´è¦†ç›–åº¦åˆ†æ
        self.analyze_temporal_coverage()
        
        # 4. å›½å®¶è¦†ç›–åº¦åˆ†æ  
        self.analyze_country_coverage()
        
        # 5. ç¼ºå¤±å€¼æ¨¡å¼åˆ†æ
        self.analyze_missing_patterns()
        
        # 6. æ•°å€¼åˆ†å¸ƒåˆ†æ
        self.analyze_value_distributions()
        
        # 7. 2023-2025å¹´æ•°æ®å¯ç”¨æ€§
        self.analyze_recent_data_availability()
        
        # 8. æŒ‡æ ‡ç›¸å…³æ€§åˆ†æ
        self.analyze_indicator_correlations()
        
        # 9. ä¸ä¸»AIæ•°æ®é›†æ—¶é—´å¯¹é½åˆ†æ
        self.analyze_alignment_with_ai_data()
        
        # 10. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        self.generate_report()
        
        return self.analysis_results
    
    def load_main_data(self) -> pd.DataFrame:
        """åŠ è½½ä¸»æ•°æ®é›†"""
        main_file = MERGED_DATA_DIR / "ai_talent_wide.csv"
        if not main_file.exists():
            raise FileNotFoundError(f"ä¸»æ•°æ®é›†ä¸å­˜åœ¨: {main_file}")
        return pd.read_csv(main_file)
    
    def analyze_data_sources(self):
        """åˆ†ææ•°æ®æºç»“æ„"""
        print("\n" + "=" * 80)
        print("ğŸ“‚ 1. æ•°æ®æºæ¦‚è§ˆ")
        print("=" * 80)
        
        sources = {}
        
        # åŸå§‹æ•°æ®
        raw_dir = BASE_DIR / "ai_talent_data_v2" / "raw"
        if raw_dir.exists():
            raw_files = list(raw_dir.glob("*.csv"))
            sources['raw'] = [f.name for f in raw_files]
            print(f"\nğŸ“ åŸå§‹æ•°æ® ({raw_dir}):")
            for f in raw_files:
                df = pd.read_csv(f, nrows=5)
                print(f"   â€¢ {f.name}: {len(df.columns)} åˆ—")
        
        # å¤„ç†åæ•°æ®
        if PROCESSED_DIR.exists():
            processed_files = list(PROCESSED_DIR.glob("*.csv"))
            sources['processed'] = [f.name for f in processed_files]
            print(f"\nğŸ“ å¤„ç†åæ•°æ® ({PROCESSED_DIR}):")
            for f in processed_files:
                df = pd.read_csv(f)
                print(f"   â€¢ {f.name}: {len(df)} è¡Œ, {len(df.columns)} åˆ—")
        
        # åˆå¹¶å®½è¡¨
        if MERGED_DATA_DIR.exists():
            merged_files = list(MERGED_DATA_DIR.glob("*.csv"))
            sources['merged'] = [f.name for f in merged_files]
            print(f"\nğŸ“ åˆå¹¶å®½è¡¨ ({MERGED_DATA_DIR}):")
            for f in merged_files:
                df = pd.read_csv(f)
                print(f"   â€¢ {f.name}: {len(df)} è¡Œ, {len(df.columns)} åˆ—")
        
        self.analysis_results['data_sources'] = sources
    
    def analyze_main_dataset(self):
        """åˆ†æä¸»æ•°æ®é›†ç»“æ„"""
        print("\n" + "=" * 80)
        print("ğŸ“Š 2. ä¸»æ•°æ®é›†åˆ†æ (ai_talent_wide.csv)")
        print("=" * 80)
        
        df = self.load_main_data()
        
        print(f"\nåŸºæœ¬ä¿¡æ¯:")
        print(f"   - æ€»è¡Œæ•°: {len(df)}")
        print(f"   - æ€»åˆ—æ•°: {len(df.columns)}")
        print(f"   - å›½å®¶æ•°: {df['country_code'].nunique()}")
        print(f"   - å¹´ä»½èŒƒå›´: {df['year'].min()} - {df['year'].max()}")
        
        # åˆ—ä¿¡æ¯
        print(f"\nğŸ“‹ åˆ—å:")
        meta_cols = ['country_code', 'country_cn', 'country_en', 'year']
        indicator_cols = [c for c in df.columns if c not in meta_cols]
        
        print(f"   å…ƒæ•°æ®åˆ—: {meta_cols}")
        print(f"\n   æŒ‡æ ‡åˆ— ({len(indicator_cols)} ä¸ª):")
        for col in indicator_cols:
            non_null = df[col].notna().sum()
            non_null_pct = non_null / len(df) * 100
            cn_name = INDICATOR_CN.get(col, col)
            print(f"      â€¢ {col}: {non_null_pct:.1f}% éç©º ({cn_name})")
        
        self.analysis_results['main_dataset'] = {
            'rows': len(df),
            'columns': len(df.columns),
            'countries': df['country_code'].unique().tolist(),
            'year_range': (int(df['year'].min()), int(df['year'].max())),
            'indicators': indicator_cols
        }
    
    def analyze_temporal_coverage(self):
        """åˆ†ææ—¶é—´è¦†ç›–åº¦"""
        print("\n" + "=" * 80)
        print("ğŸ“… 3. æ—¶é—´è¦†ç›–åº¦åˆ†æ")
        print("=" * 80)
        
        df = self.load_main_data()
        meta_cols = ['country_code', 'country_cn', 'country_en', 'year']
        indicator_cols = [c for c in df.columns if c not in meta_cols]
        
        coverage = {}
        print(f"\nå„æŒ‡æ ‡æ—¶é—´è¦†ç›–:")
        print("-" * 80)
        print(f"{'æŒ‡æ ‡':<40} {'èµ·å§‹å¹´':<8} {'ç»“æŸå¹´':<8} {'2016-2025è¦†ç›–':<15}")
        print("-" * 80)
        
        for col in indicator_cols:
            valid_data = df[df[col].notna()]
            if len(valid_data) > 0:
                min_year = int(valid_data['year'].min())
                max_year = int(valid_data['year'].max())
                valid_years = valid_data['year'].unique()
                
                coverage[col] = {
                    'min_year': min_year,
                    'max_year': max_year,
                    'years_count': len(valid_years)
                }
                
                # æ£€æŸ¥2016-2025è¦†ç›–
                focus_coverage = len([y for y in valid_years if y in FOCUS_YEARS])
                coverage_str = f"{focus_coverage}/10"
                
                print(f"   {col[:38]:<40} {min_year:<8} {max_year:<8} {coverage_str:<15}")
                
                if max_year < 2023:
                    self.warnings.append(f"âš ï¸ {col}: æœ€æ–°æ•°æ®ä»…åˆ°{max_year}å¹´")
        
        self.analysis_results['temporal_coverage'] = coverage
    
    def analyze_country_coverage(self):
        """åˆ†æå„å›½æ•°æ®è¦†ç›–æƒ…å†µ"""
        print("\n" + "=" * 80)
        print("ğŸŒ 4. å„å›½æ•°æ®è¦†ç›–åº¦åˆ†æ")
        print("=" * 80)
        
        df = self.load_main_data()
        meta_cols = ['country_code', 'country_cn', 'country_en', 'year']
        indicator_cols = [c for c in df.columns if c not in meta_cols]
        
        print(f"\nå„å›½æŒ‡æ ‡éç©ºç‡:")
        print("-" * 70)
        
        country_coverage = {}
        for country in TARGET_COUNTRIES.keys():
            country_data = df[df['country_code'] == country]
            if len(country_data) == 0:
                print(f"   âŒ {country} ({TARGET_COUNTRIES[country]}): æ— æ•°æ®")
                self.issues.append(f"ğŸš¨ {country} æ— æ•°æ®")
                continue
            
            # è®¡ç®—å„æŒ‡æ ‡éç©ºç‡
            non_null_rates = {}
            for col in indicator_cols:
                non_null = country_data[col].notna().sum()
                non_null_pct = non_null / len(country_data) * 100
                non_null_rates[col] = non_null_pct
            
            avg_rate = np.mean(list(non_null_rates.values()))
            country_coverage[country] = {
                'avg_coverage': avg_rate,
                'years': len(country_data),
                'detail': non_null_rates
            }
            
            status = "âœ…" if avg_rate > 60 else "âš ï¸" if avg_rate > 30 else "âŒ"
            print(f"   {status} {country} ({TARGET_COUNTRIES[country]:4s}): "
                  f"å¹³å‡è¦†ç›–ç‡ {avg_rate:.1f}%, {len(country_data)}å¹´æ•°æ®")
            
            if avg_rate < 40:
                self.warnings.append(f"âš ï¸ {country}: æ•°æ®è¦†ç›–ç‡ä»…{avg_rate:.1f}%")
        
        # ç‰¹åˆ«å…³æ³¨ä½è¦†ç›–ç‡æŒ‡æ ‡
        print(f"\nğŸ” ä½è¦†ç›–ç‡æŒ‡æ ‡è¯¦æƒ…:")
        for country in ['ARE', 'IND', 'CHN']:
            if country in country_coverage:
                detail = country_coverage[country]['detail']
                low_coverage = [(k, v) for k, v in detail.items() if v < 30]
                if low_coverage:
                    print(f"   {country} ({TARGET_COUNTRIES[country]}):")
                    for col, pct in sorted(low_coverage, key=lambda x: x[1])[:3]:
                        cn_name = INDICATOR_CN.get(col, col)
                        print(f"      â€¢ {cn_name}: {pct:.1f}%")
        
        self.analysis_results['country_coverage'] = country_coverage
    
    def analyze_missing_patterns(self):
        """åˆ†æç¼ºå¤±å€¼æ¨¡å¼"""
        print("\n" + "=" * 80)
        print("ğŸ” 5. ç¼ºå¤±å€¼æ¨¡å¼åˆ†æ")
        print("=" * 80)
        
        df = self.load_main_data()
        meta_cols = ['country_code', 'country_cn', 'country_en', 'year']
        indicator_cols = [c for c in df.columns if c not in meta_cols]
        
        print(f"\nç¼ºå¤±å€¼ç»Ÿè®¡:")
        print("-" * 70)
        
        missing_summary = []
        for col in indicator_cols:
            missing = df[col].isna().sum()
            missing_pct = missing / len(df) * 100
            cn_name = INDICATOR_CN.get(col, col)
            missing_summary.append({
                'indicator': col,
                'indicator_cn': cn_name,
                'missing_count': missing,
                'missing_pct': missing_pct
            })
        
        # æŒ‰ç¼ºå¤±ç‡æ’åº
        missing_summary.sort(key=lambda x: x['missing_pct'], reverse=True)
        
        print(f"{'æŒ‡æ ‡':<35} {'ç¼ºå¤±æ•°':>10} {'ç¼ºå¤±ç‡':>10}")
        print("-" * 60)
        for item in missing_summary:
            print(f"{item['indicator'][:34]:<35} {item['missing_count']:>10} "
                  f"{item['missing_pct']:>9.1f}%")
        
        # åˆ†æç¼ºå¤±æ¨¡å¼ç±»å‹
        print(f"\nç¼ºå¤±æ¨¡å¼åˆ†ç±»:")
        
        tail_missing = []  # å°¾éƒ¨ç¼ºå¤±
        sparse_indicators = []  # ç¨€ç–æŒ‡æ ‡
        
        for col in indicator_cols:
            # å°¾éƒ¨ç¼ºå¤±æ£€æµ‹
            recent_data = df[df['year'] >= 2023][col]
            if recent_data.isna().sum() / len(recent_data) > 0.8:
                tail_missing.append(col)
            
            # ç¨€ç–æŒ‡æ ‡æ£€æµ‹ï¼ˆè¦†ç›–ç‡<30%ï¼‰
            if df[col].notna().sum() / len(df) < 0.3:
                sparse_indicators.append(col)
        
        if tail_missing:
            print(f"   ğŸ“ å°¾éƒ¨ç¼ºå¤± (2023+æ•°æ®ç¨€å°‘): {len(tail_missing)} ä¸ªæŒ‡æ ‡")
            for col in tail_missing:
                cn_name = INDICATOR_CN.get(col, col)
                print(f"      â€¢ {cn_name}")
            self.recommendations.append(f"ğŸ’¡ {len(tail_missing)}ä¸ªæŒ‡æ ‡ç¼ºå°‘2023å¹´åæ•°æ®ï¼Œå»ºè®®Holt-Winterså¤–æ¨")
        
        if sparse_indicators:
            print(f"   ğŸ“ ç¨€ç–æŒ‡æ ‡ (è¦†ç›–ç‡<30%): {len(sparse_indicators)} ä¸ª")
            for col in sparse_indicators:
                cn_name = INDICATOR_CN.get(col, col)
                print(f"      â€¢ {cn_name}")
            self.warnings.append(f"âš ï¸ {len(sparse_indicators)}ä¸ªæŒ‡æ ‡è¦†ç›–ç‡ä½äº30%ï¼Œå»ºæ¨¡æ—¶è€ƒè™‘å‰”é™¤æˆ–é™æƒ")
        
        self.analysis_results['missing_patterns'] = {
            'summary': missing_summary,
            'tail_missing': tail_missing,
            'sparse_indicators': sparse_indicators
        }
    
    def analyze_value_distributions(self):
        """åˆ†ææ•°å€¼åˆ†å¸ƒç‰¹å¾"""
        print("\n" + "=" * 80)
        print("ğŸ“ˆ 6. æ•°å€¼åˆ†å¸ƒåˆ†æ")
        print("=" * 80)
        
        df = self.load_main_data()
        meta_cols = ['country_code', 'country_cn', 'country_en', 'year']
        indicator_cols = [c for c in df.columns if c not in meta_cols]
        
        print(f"\nåˆ†å¸ƒç‰¹å¾åˆ†æ:")
        print("-" * 90)
        print(f"{'æŒ‡æ ‡':<30} {'æœ€å°å€¼':>12} {'æœ€å¤§å€¼':>12} {'ååº¦':>8} {'å»ºè®®å¯¹æ•°':>10}")
        print("-" * 90)
        
        distribution_analysis = []
        for col in indicator_cols:
            values = df[col].dropna()
            if len(values) < 10:
                continue
            
            min_val = values.min()
            max_val = values.max()
            skewness = values.skew()
            
            # åˆ¤æ–­æ˜¯å¦éœ€è¦å¯¹æ•°å˜æ¢
            need_log = False
            if col in INDICATOR_CATEGORIES.get('count_absolute', []):
                if abs(skewness) > 2:
                    need_log = True
            
            distribution_analysis.append({
                'indicator': col,
                'min': min_val,
                'max': max_val,
                'skewness': skewness,
                'need_log': need_log
            })
            
            log_mark = "âœ… æ˜¯" if need_log else "âŒ å¦"
            print(f"   {col[:28]:<30} {min_val:>12.2f} {max_val:>12.2f} "
                  f"{skewness:>8.2f} {log_mark:>10}")
        
        # ç»“è®º
        need_log_count = sum(1 for d in distribution_analysis if d['need_log'])
        print(f"\nğŸ“ ç»“è®º:")
        print(f"   â€¢ {need_log_count} ä¸ªæŒ‡æ ‡å»ºè®®å¯¹æ•°å˜æ¢ï¼ˆç»å¯¹æ•°é‡æŒ‡æ ‡ï¼šäººå£ã€åœ¨æ ¡ç”Ÿæ•°ï¼‰")
        print(f"   â€¢ å¯†åº¦æŒ‡æ ‡å’Œæ¯”ä¾‹æŒ‡æ ‡é€šå¸¸ä¸éœ€è¦å¯¹æ•°å˜æ¢")
        
        self.analysis_results['distribution_analysis'] = distribution_analysis
    
    def analyze_recent_data_availability(self):
        """åˆ†æ2023-2025å¹´æ•°æ®å¯ç”¨æ€§"""
        print("\n" + "=" * 80)
        print("ğŸ¯ 7. 2023-2025å¹´æ•°æ®å¯ç”¨æ€§åˆ†æï¼ˆå…³é”®ï¼ï¼‰")
        print("=" * 80)
        
        df = self.load_main_data()
        meta_cols = ['country_code', 'country_cn', 'country_en', 'year']
        indicator_cols = [c for c in df.columns if c not in meta_cols]
        
        availability = {}
        
        for year in [2023, 2024, 2025]:
            year_data = df[df['year'] == year]
            if len(year_data) == 0:
                print(f"\n{year}å¹´æ•°æ®: âŒ æ— è®°å½•")
                availability[year] = {'has_data': False, 'indicators': []}
                continue
            
            has_data = []
            missing_data = []
            
            for col in indicator_cols:
                if year_data[col].notna().any():
                    has_data.append(col)
                else:
                    missing_data.append(col)
            
            availability[year] = {
                'has_data': True,
                'indicators_with_data': has_data,
                'indicators_missing': missing_data
            }
            
            print(f"\n{year}å¹´æ•°æ®:")
            print(f"   âœ… æœ‰æ•°æ®: {len(has_data)} ä¸ªæŒ‡æ ‡")
            if has_data:
                print(f"      {', '.join([INDICATOR_CN.get(c, c)[:10] for c in has_data[:5]])}")
            print(f"   âŒ ç¼ºå¤±: {len(missing_data)} ä¸ªæŒ‡æ ‡")
            if missing_data:
                print(f"      {', '.join([INDICATOR_CN.get(c, c)[:10] for c in missing_data[:5]])}")
        
        # 2025å¹´ç‰¹åˆ«è¯´æ˜
        if 2025 not in [int(y) for y in df['year'].unique()]:
            self.issues.append("ğŸš¨ AIäººæ‰æ•°æ®é›†æ— 2025å¹´æ•°æ®ï¼Œéœ€è¦å¤–æ¨é¢„æµ‹")
            print(f"\nâš ï¸ 2025å¹´æ•°æ®å®Œå…¨ç¼ºå¤±ï¼Œå»ºè®®ä½¿ç”¨æ—¶é—´åºåˆ—å¤–æ¨")
        
        self.analysis_results['recent_availability'] = availability
    
    def analyze_indicator_correlations(self):
        """åˆ†ææŒ‡æ ‡ç›¸å…³æ€§"""
        print("\n" + "=" * 80)
        print("ğŸ”— 8. æ ¸å¿ƒæŒ‡æ ‡ç›¸å…³æ€§åˆ†æ")
        print("=" * 80)
        
        df = self.load_main_data()
        
        # é€‰æ‹©æ ¸å¿ƒæŒ‡æ ‡è¿›è¡Œç›¸å…³æ€§åˆ†æ
        core_indicators = [
            'researchers_per_million',
            'tertiary_gross_enrollment_pct',
            'education_expenditure_pct_gdp',
            'rd_expenditure_pct_gdp',
            'pop_15_64_pct'
        ]
        
        available_core = [c for c in core_indicators if c in df.columns]
        
        if len(available_core) >= 3:
            corr_matrix = df[available_core].corr()
            
            print(f"\næ ¸å¿ƒæŒ‡æ ‡ç›¸å…³ç³»æ•°çŸ©é˜µ:")
            print("-" * 70)
            
            # æ‰“å°ç®€åŒ–çš„ç›¸å…³æ€§
            for i, col1 in enumerate(available_core):
                for j, col2 in enumerate(available_core):
                    if i < j:
                        corr = corr_matrix.loc[col1, col2]
                        cn1 = INDICATOR_CN.get(col1, col1)[:15]
                        cn2 = INDICATOR_CN.get(col2, col2)[:15]
                        strength = "å¼º" if abs(corr) > 0.7 else "ä¸­" if abs(corr) > 0.4 else "å¼±"
                        print(f"   {cn1} â†” {cn2}: {corr:.3f} ({strength})")
            
            self.analysis_results['correlations'] = corr_matrix.to_dict()
        else:
            print("   æ ¸å¿ƒæŒ‡æ ‡æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œç›¸å…³æ€§åˆ†æ")
    
    def analyze_alignment_with_ai_data(self):
        """åˆ†æä¸ä¸»AIæ•°æ®é›†çš„æ—¶é—´å¯¹é½æƒ…å†µ"""
        print("\n" + "=" * 80)
        print("ğŸ”„ 9. ä¸ä¸»AIæ•°æ®é›†æ—¶é—´å¯¹é½åˆ†æ")
        print("=" * 80)
        
        df = self.load_main_data()
        
        print(f"\nä¸»AIæ•°æ®é›†ç›®æ ‡èŒƒå›´: 2016-2025")
        print(f"AIäººæ‰æ•°æ®é›†å®é™…èŒƒå›´: {df['year'].min()}-{df['year'].max()}")
        
        # æ£€æŸ¥2016-2025è¦†ç›–
        meta_cols = ['country_code', 'country_cn', 'country_en', 'year']
        indicator_cols = [c for c in df.columns if c not in meta_cols]
        
        alignment_issues = []
        
        for col in indicator_cols:
            col_data = df[df[col].notna()]
            covered_years = [int(y) for y in col_data['year'].unique()]
            missing_focus = [y for y in FOCUS_YEARS if y not in covered_years]
            
            if missing_focus:
                alignment_issues.append({
                    'indicator': col,
                    'indicator_cn': INDICATOR_CN.get(col, col),
                    'missing_years': missing_focus
                })
        
        if alignment_issues:
            print(f"\nâš ï¸ {len(alignment_issues)} ä¸ªæŒ‡æ ‡åœ¨2016-2025æœŸé—´æœ‰ç¼ºå¤±:")
            for issue in alignment_issues[:8]:
                missing_str = ', '.join(map(str, issue['missing_years']))
                print(f"   â€¢ {issue['indicator_cn'][:25]}: ç¼ºå¤± [{missing_str}]")
            
            # ç»Ÿè®¡
            missing_2025 = sum(1 for i in alignment_issues if 2025 in i['missing_years'])
            missing_2024 = sum(1 for i in alignment_issues if 2024 in i['missing_years'])
            missing_2023 = sum(1 for i in alignment_issues if 2023 in i['missing_years'])
            
            print(f"\n   å¹´ä»½ç¼ºå¤±ç»Ÿè®¡:")
            print(f"      ç¼ºå¤±2025å¹´: {missing_2025} ä¸ªæŒ‡æ ‡")
            print(f"      ç¼ºå¤±2024å¹´: {missing_2024} ä¸ªæŒ‡æ ‡")
            print(f"      ç¼ºå¤±2023å¹´: {missing_2023} ä¸ªæŒ‡æ ‡")
            
            self.recommendations.append(f"ğŸ’¡ éœ€è¦å¤–æ¨{missing_2025}ä¸ªæŒ‡æ ‡çš„2025å¹´æ•°æ®")
        else:
            print(f"\nâœ… æ‰€æœ‰æŒ‡æ ‡åœ¨2016-2025æœŸé—´æ•°æ®å®Œæ•´")
        
        self.analysis_results['alignment_issues'] = alignment_issues
    
    def generate_report(self):
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        print("\n" + "=" * 100)
        print("ğŸ“‹ ç»¼åˆåˆ†ææŠ¥å‘Šä¸å»ºè®®")
        print("=" * 100)
        
        # é—®é¢˜æ±‡æ€»
        print("\nğŸš¨ å…³é”®é—®é¢˜:")
        if self.issues:
            for issue in self.issues:
                print(f"   {issue}")
        else:
            print("   âœ… æ— å…³é”®é—®é¢˜")
        
        # è­¦å‘Šæ±‡æ€»
        print(f"\nâš ï¸ è­¦å‘Š ({len(self.warnings)} æ¡):")
        for w in list(set(self.warnings))[:10]:
            print(f"   {w}")
        
        # å»ºè®®æ±‡æ€»
        print(f"\nğŸ’¡ é¢„å¤„ç†å»ºè®®:")
        for rec in list(set(self.recommendations)):
            print(f"   {rec}")
        
        # AIäººæ‰æ•°æ®ç‰¹ç‚¹è¯´æ˜
        print(f"\nğŸ“ AIäººæ‰æ•°æ®ç‰¹ç‚¹åˆ†æ:")
        print("   1. æ•°æ®ç±»å‹: ä»¥å¯†åº¦æŒ‡æ ‡å’Œæ¯”ä¾‹æŒ‡æ ‡ä¸ºä¸»ï¼Œå¤šæ•°ä¸éœ€è¦å¯¹æ•°å˜æ¢")
        print("   2. æ—¶é—´èŒƒå›´: 2015-2024ï¼Œéœ€è¦è¡¥å……2025å¹´æ•°æ®")
        print("   3. ç¼ºå¤±æ¨¡å¼: 2023-2024å¹´å¤šæŒ‡æ ‡ç¼ºå¤±ï¼Œéœ€å¤–æ¨ï¼›éƒ¨åˆ†æŒ‡æ ‡ç¨€ç–")
        print("   4. å…³é”®æŒ‡æ ‡: researchers_per_million, tertiary_gross_enrollment_pct")
        print("   5. ç¨€ç–æŒ‡æ ‡: stem_graduates_pct, tertiary_completion_25_34_pct è¦†ç›–ç‡ä½")
        
        # ä¸R&Dæ•°æ®å¯¹æ¯”
        print(f"\nğŸ“Š ä¸R&Dåˆ›æ–°æ•°æ®çš„å…³ç³»:")
        print("   â€¢ é‡å æŒ‡æ ‡: rd_expenditure_pct_gdp, researchers_per_million")
        print("   â€¢ äº’è¡¥æŒ‡æ ‡: äººæ‰æ•°æ®ä¾§é‡æ•™è‚²åŸ¹å…»ï¼ŒR&Dæ•°æ®ä¾§é‡åˆ›æ–°æŠ•å…¥äº§å‡º")
        print("   â€¢ åˆå¹¶å»ºè®®: æŒ‰country_code+yearåˆå¹¶ï¼Œä¿ç•™å„è‡ªç‰¹æœ‰æŒ‡æ ‡")
        
        # ä¿å­˜æŠ¥å‘Š
        report_content = f"""# AIäººæ‰æ•°æ®è´¨é‡åˆ†ææŠ¥å‘Š

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## æ•°æ®æ¦‚è§ˆ

- æ•°æ®æ¥æº: World Bank, UNESCO UIS
- æ—¶é—´èŒƒå›´: 2015-2024
- ç›®æ ‡å›½å®¶: {list(TARGET_COUNTRIES.keys())}
- é‡ç‚¹å¹´ä»½: 2016-2025ï¼ˆä¸AIä¸»æ•°æ®å¯¹é½ï¼‰
- æ ¸å¿ƒæŒ‡æ ‡: ç ”ç©¶äººå‘˜å¯†åº¦ã€é«˜ç­‰æ•™è‚²å…¥å­¦ç‡ã€æ•™è‚²æ”¯å‡ºã€R&Dæ”¯å‡ºç­‰

## å…³é”®é—®é¢˜

{chr(10).join(['- ' + i for i in self.issues]) if self.issues else 'æ— å…³é”®é—®é¢˜'}

## è­¦å‘Š

{chr(10).join(['- ' + w for w in list(set(self.warnings))])}

## é¢„å¤„ç†å»ºè®®

{chr(10).join(['- ' + r for r in list(set(self.recommendations))])}

## æŒ‡æ ‡è¦†ç›–ç‡

| æŒ‡æ ‡ | éç©ºç‡ | å»ºè®®å¤„ç† |
|------|--------|---------|
| pop_15_64_pct | 100% | ç›´æ¥ä½¿ç”¨ |
| population_total | 100% | å¯¹æ•°å˜æ¢ |
| tertiary_gross_enrollment_pct | 89% | æ’å€¼è¡¥å…¨ |
| rd_expenditure_pct_gdp | 76% | æ’å€¼+å¤–æ¨ |
| researchers_per_million | 66% | æ’å€¼+å¤–æ¨ |
| education_expenditure_pct_gdp | 63% | æ’å€¼+å¤–æ¨ |
| stem_graduates_pct | 9% | è€ƒè™‘å‰”é™¤æˆ–ç‰¹æ®Šå¤„ç† |
| tertiary_completion_25_34_pct | 7% | è€ƒè™‘å‰”é™¤æˆ–ç‰¹æ®Šå¤„ç† |

## é¢„å¤„ç†æ¸…å•

### 1. æ—¶é—´ç»´åº¦å¤„ç†
- [ ] 2023-2025å¹´ç¼ºå¤±æ•°æ®å¤–æ¨ï¼ˆHolt-WintersæŒ‡æ•°å¹³æ»‘ï¼‰
- [ ] ä¸­é—´å¹´ä»½ç¼ºå¤±æ’å€¼ï¼ˆä¸‰æ¬¡æ ·æ¡ï¼‰
- [ ] ä¸AIä¸»æ•°æ®2016-2025å¯¹é½

### 2. ç¼ºå¤±å€¼å¤„ç†
- [ ] å°¾éƒ¨ç¼ºå¤±ï¼šæ—¶é—´åºåˆ—å¤–æ¨
- [ ] ä¸­é—´ç¼ºå¤±ï¼šä¸‰æ¬¡æ ·æ¡æ’å€¼
- [ ] ç¨€ç–æŒ‡æ ‡ï¼šæ ‡è®°å¹¶è€ƒè™‘é™æƒæˆ–å‰”é™¤

### 3. ç‰¹å¾å·¥ç¨‹
- [ ] å¹´å¢é•¿ç‡ç‰¹å¾
- [ ] 3å¹´ç§»åŠ¨å¹³å‡ï¼ˆå¹³æ»‘æ³¢åŠ¨ï¼‰
- [ ] äººæ‰ç»¼åˆæŒ‡æ•°ï¼ˆæ ‡å‡†åŒ–åŠ æƒï¼‰
- [ ] ä¸AIäº§å‡ºçš„æ»åå…³è”ç‰¹å¾

### 4. æ•°æ®æ ‡å‡†åŒ–
- [ ] ç»å¯¹æ•°é‡æŒ‡æ ‡å¯¹æ•°å˜æ¢ï¼ˆpopulation_total, tertiary_enrollment_totalï¼‰
- [ ] å›½å®¶ä»£ç æ ‡å‡†åŒ–ï¼ˆISO 3166-1 alpha-3ï¼‰
- [ ] è¾“å‡ºæ ¼å¼ä¸ä¸»è¡¨ä¸€è‡´
"""
        
        report_path = OUTPUT_DIR / "ai_talent_quality_report.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"\nâœ… è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
        
        # ä¿å­˜åˆ†æç»“æœJSON
        results_path = OUTPUT_DIR / "ai_talent_analysis_results.json"
        
        def convert_types(obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {k: convert_types(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_types(i) for i in obj]
            return obj
        
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(convert_types(self.analysis_results), f, ensure_ascii=False, indent=2)
        
        print(f"âœ… åˆ†æç»“æœJSONå·²ä¿å­˜è‡³: {results_path}")


def main():
    """ä¸»å‡½æ•°"""
    analyzer = AITalentQualityAnalyzer()
    analyzer.run_full_analysis()


if __name__ == "__main__":
    main()
